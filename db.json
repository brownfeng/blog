{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/images/avatar.png","path":"images/avatar.png","modified":0,"renderable":0},{"_id":"themes/next/source/favicon.ico","path":"favicon.ico","modified":0,"renderable":1},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/hook-duoshuo.js","path":"js/src/hook-duoshuo.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fastclick/LICENSE","path":"vendors/fastclick/LICENSE","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fastclick/bower.json","path":"vendors/fastclick/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fastclick/README.md","path":"vendors/fastclick/README.md","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/font-awesome/HELP-US-OUT.txt","path":"vendors/font-awesome/HELP-US-OUT.txt","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/font-awesome/bower.json","path":"vendors/font-awesome/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/jquery_lazyload/CONTRIBUTING.md","path":"vendors/jquery_lazyload/CONTRIBUTING.md","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/jquery_lazyload/README.md","path":"vendors/jquery_lazyload/README.md","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/jquery_lazyload/bower.json","path":"vendors/jquery_lazyload/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/jquery_lazyload/jquery.lazyload.js","path":"vendors/jquery_lazyload/jquery.lazyload.js","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/jquery_lazyload/jquery.scrollstop.js","path":"vendors/jquery_lazyload/jquery.scrollstop.js","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/velocity/bower.json","path":"vendors/velocity/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/velocity/velocity.min.js","path":"vendors/velocity/velocity.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/velocity/velocity.ui.min.js","path":"vendors/velocity/velocity.ui.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/velocity/velocity.ui.js","path":"vendors/velocity/velocity.ui.js","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/jquery/index.js","path":"vendors/jquery/index.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fancybox/source/fancybox_loading.gif","path":"vendors/fancybox/source/fancybox_loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fancybox/source/blank.gif","path":"vendors/fancybox/source/blank.gif","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fancybox/source/fancybox_loading@2x.gif","path":"vendors/fancybox/source/fancybox_loading@2x.gif","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fancybox/source/fancybox_sprite.png","path":"vendors/fancybox/source/fancybox_sprite.png","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fancybox/source/fancybox_overlay.png","path":"vendors/fancybox/source/fancybox_overlay.png","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fancybox/source/fancybox_sprite@2x.png","path":"vendors/fancybox/source/fancybox_sprite@2x.png","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fancybox/source/jquery.fancybox.css","path":"vendors/fancybox/source/jquery.fancybox.css","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fancybox/source/jquery.fancybox.pack.js","path":"vendors/fancybox/source/jquery.fancybox.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fancybox/source/jquery.fancybox.js","path":"vendors/fancybox/source/jquery.fancybox.js","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fastclick/lib/fastclick.min.js","path":"vendors/fastclick/lib/fastclick.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fastclick/lib/fastclick.js","path":"vendors/fastclick/lib/fastclick.js","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/font-awesome/css/font-awesome.css","path":"vendors/font-awesome/css/font-awesome.css","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/font-awesome/css/font-awesome.min.css","path":"vendors/font-awesome/css/font-awesome.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/font-awesome/css/font-awesome.css.map","path":"vendors/font-awesome/css/font-awesome.css.map","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/font-awesome/fonts/fontawesome-webfont.woff2","path":"vendors/font-awesome/fonts/fontawesome-webfont.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/ua-parser-js/dist/ua-parser.min.js","path":"vendors/ua-parser-js/dist/ua-parser.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/ua-parser-js/dist/ua-parser.pack.js","path":"vendors/ua-parser-js/dist/ua-parser.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/font-awesome/fonts/FontAwesome.otf","path":"vendors/font-awesome/fonts/FontAwesome.otf","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/font-awesome/fonts/fontawesome-webfont.eot","path":"vendors/font-awesome/fonts/fontawesome-webfont.eot","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/font-awesome/fonts/fontawesome-webfont.woff","path":"vendors/font-awesome/fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/velocity/velocity.js","path":"vendors/velocity/velocity.js","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fancybox/source/helpers/fancybox_buttons.png","path":"vendors/fancybox/source/helpers/fancybox_buttons.png","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"vendors/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"vendors/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fancybox/source/helpers/jquery.fancybox-media.js","path":"vendors/fancybox/source/helpers/jquery.fancybox-media.js","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"vendors/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"vendors/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/font-awesome/fonts/fontawesome-webfont.ttf","path":"vendors/font-awesome/fonts/fontawesome-webfont.ttf","modified":0,"renderable":1},{"_id":"themes/next/source/vendors/font-awesome/fonts/fontawesome-webfont.svg","path":"vendors/font-awesome/fonts/fontawesome-webfont.svg","modified":0,"renderable":1}],"Cache":[{"_id":"themes/next/.bowerrc","hash":"80e096fdc1cf912ee85dd9f7e6e77fd40cf60f10","modified":1472228352000},{"_id":"themes/next/.editorconfig","hash":"792fd2bd8174ece1a75d5fd24ab16594886f3a7f","modified":1472228352000},{"_id":"themes/next/.hound.yml","hash":"b76daa84c9ca3ad292c78412603370a367cc2bc3","modified":1472228352000},{"_id":"themes/next/.javascript_ignore","hash":"d619ee13031908cd72666e4ff652d2ea3483b1c3","modified":1472228352000},{"_id":"themes/next/_config.yml","hash":"4e5dde5d40d8710da436f512437dcb348b6e4171","modified":1472228352000},{"_id":"themes/next/.jshintrc","hash":"9928f81bd822f6a8d67fdbc909b517178533bca9","modified":1472228352000},{"_id":"themes/next/README.en.md","hash":"565ba52b3825b85a9f05b41183caca7f18b741d4","modified":1472228352000},{"_id":"themes/next/README.md","hash":"500b5606eb6a09c979d16128f8b00f4bf9bc95ac","modified":1472228352000},{"_id":"themes/next/bower.json","hash":"f89c6700a11d81e067cc97273ca6bf96cb88c8f9","modified":1472228352000},{"_id":"themes/next/gulpfile.coffee","hash":"26e5b1b945704c8bc78b928feede895c4c111c95","modified":1472228352000},{"_id":"themes/next/package.json","hash":"63e9c0f1dd9e5d7f51b4ae383981ef939a2ed45d","modified":1472228352000},{"_id":"source/_drafts/PhotoKit学习笔记一.md","hash":"c47f926de3f3a1d71245bd402fa39cd71c3fc922","modified":1472228352000},{"_id":"source/_drafts/音频开发资料汇总.md","hash":"bbedcdca18517c5be0aeac0228831f7cdc7a31bd","modified":1472228352000},{"_id":"source/about/index.md","hash":"44920cd8c3cd2cf01433cff473253ad0b21a75e3","modified":1472228352000},{"_id":"source/_posts/iOS处理图片大小总结.md","hash":"25eb1c9e8323dfb18bdb02e2a6e0018500df82ec","modified":1472228352000},{"_id":"source/_posts/RunLoop总结.md","hash":"3a8e22f43921e58530782b5863a3dc73887147ae","modified":1472228352000},{"_id":"source/_posts/iOS音频系列(一).md","hash":"d095b79ccc3ee9cddc737b07e6c26dcd4d9d3f1b","modified":1472228352000},{"_id":"source/_posts/iOS音频系列(三).md","hash":"d52dc676088af7d44863444d76e28e40d467dca7","modified":1472228352000},{"_id":"source/_posts/iOS音频系列(二).md","hash":"7b69581fa98e4d410eaf49100011f24638c2444a","modified":1472228352000},{"_id":"source/_posts/iOS音频系列(四).md","hash":"098a8b15cc9872945e52bb3c6321910d3a85286c","modified":1472228352000},{"_id":"source/_posts/popToViewController的坑.md","hash":"72f76a0518748412eb3db587cf506c0f68b62ba6","modified":1472228352000},{"_id":"source/archive/index.md","hash":"daa7bfbb5340d463842284875ae3df9bd5605ea4","modified":1472228352000},{"_id":"source/categories/index.md","hash":"ee77ccbc282bca7b56a15dbd761a3b4783ea0bf8","modified":1472228352000},{"_id":"source/tags/index.md","hash":"45fd57b1b7daec0d153e15e136828b8afd6884cb","modified":1472228352000},{"_id":"source/images/avatar.png","hash":"4b70d8424875ad83471967430991718465a6250a","modified":1472228352000},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"5ab257af816986cd0e53f9527a92d5934ac70ae9","modified":1472228352000},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"c2024ded82143807c28a299c5fe6b927ef3525ff","modified":1472228352000},{"_id":"themes/next/languages/de.yml","hash":"786afba25cfc98845a20d9901823ebeebcd1cbbf","modified":1472228352000},{"_id":"themes/next/languages/default.yml","hash":"9db835c0543ade5a89bc80ec5a898203227cf3d8","modified":1472228352000},{"_id":"themes/next/languages/en.yml","hash":"f03799cbdb5a33064ead080bcac4baca1f6bc5f9","modified":1472228352000},{"_id":"themes/next/languages/fr-FR.yml","hash":"1a084623c39de74301f3e92f9388a3a815a542ca","modified":1472228352000},{"_id":"themes/next/languages/id.yml","hash":"147c01e41b931085ad14250fa900c2249dcbbdd7","modified":1472228352000},{"_id":"themes/next/languages/ja.yml","hash":"a2c7b6301b5474aab798946fb700289df237c3cf","modified":1472228352000},{"_id":"themes/next/languages/pt-BR.yml","hash":"462aa865ca3d479bcf6b363cba61247b50f230ff","modified":1472228352000},{"_id":"themes/next/languages/pt.yml","hash":"ca239b39bf65c9462e59d51b12f0fe566d453197","modified":1472228352000},{"_id":"themes/next/languages/zh-Hans.yml","hash":"bea452bc49aed171a210d09bd6cddc4e846ea8ab","modified":1472228352000},{"_id":"themes/next/languages/ru.yml","hash":"cc7b964a46587aea0e57b0a5269d8fd25570858e","modified":1472228352000},{"_id":"themes/next/languages/zh-hk.yml","hash":"519ab3d817ec3bc5bfc91159c494b6b3c170bea7","modified":1472228352000},{"_id":"themes/next/languages/zh-tw.yml","hash":"6b1f345aaefc13e6723dc8a6741b59ac05c20dfd","modified":1472228352000},{"_id":"themes/next/layout/_layout.swig","hash":"74157f6cfd679ea11febec632542793f37c5e5d4","modified":1472228352000},{"_id":"themes/next/layout/archive.swig","hash":"b5b59d70fc1563f482fa07afd435752774ad5981","modified":1472228352000},{"_id":"themes/next/layout/category.swig","hash":"6422d196ceaff4220d54b8af770e7e957f3364ad","modified":1472228352000},{"_id":"themes/next/layout/index.swig","hash":"427d0b95b854e311ae363088ab39a393bf8fdc8b","modified":1472228352000},{"_id":"themes/next/layout/page.swig","hash":"8019d02232a6dd1a665b6a4d2daef8e5dd2f0049","modified":1472228352000},{"_id":"themes/next/layout/post.swig","hash":"e2e512142961ddfe77eba29eaa88f4a2ee43ae18","modified":1472228352000},{"_id":"themes/next/layout/tag.swig","hash":"07cf49c49c39a14dfbe9ce8e7d7eea3d4d0a4911","modified":1472228352000},{"_id":"themes/next/scripts/merge-configs.js","hash":"0c56be2e85c694247cfa327ea6d627b99ca265e8","modified":1472228352000},{"_id":"themes/next/source/favicon.ico","hash":"afcec07e0677d7a23cac10b1ebdc783939a0b10d","modified":1472228352000},{"_id":"themes/next/test/.jshintrc","hash":"19f93d13d1689fe033c82eb2d5f3ce30b6543cc0","modified":1472228352000},{"_id":"themes/next/test/intern.js","hash":"11fa8a4f5c3b4119a179ae0a2584c8187f907a73","modified":1472228352000},{"_id":"themes/next/test/helpers.js","hash":"a1f5de25154c3724ffc24a91ddc576cdbd60864f","modified":1472228352000},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1472228352000},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"43c3433155ccd9abcbe7dce2e6bfa1f3a66af18b","modified":1472228352000},{"_id":"themes/next/layout/_macro/post.swig","hash":"1ca03011bed92614832b1343b65be92183957dc5","modified":1472228352000},{"_id":"themes/next/layout/_macro/reward.swig","hash":"37e5b7c42ec17b9b6b786c5512bcc481a21c974e","modified":1472228352000},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"1a77843ce5eac62151dc3d38f0a36c43e19e1a74","modified":1472228352000},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"14e785adeb0e671ba0ff9a553e6f0d8def6c670c","modified":1472228352000},{"_id":"themes/next/layout/_scripts/baidu-push.swig","hash":"82d060fe055d6e423bbc9199f82dfe5c68e74779","modified":1472228352000},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"03aaebe9d50f6acb007ec38cc04acd1cfceb404d","modified":1472228352000},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"766b2bdda29523ed6cd8d7aa197f996022f8fd94","modified":1472228352000},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"0b91cadecead8e0b5211cc42b085998d94af503a","modified":1472228352000},{"_id":"themes/next/layout/_partials/comments.swig","hash":"b73f9443bee2d3ea383aad52e49ffca8aa97dcc2","modified":1472228352000},{"_id":"themes/next/layout/_partials/duoshuo-hot-articles.swig","hash":"5d4638c46aef65bf32a01681495b62416ccc98db","modified":1472228352000},{"_id":"themes/next/layout/_partials/footer.swig","hash":"0ce71d8322ea7dea82d9371fa2fe13949aa870e3","modified":1472228352000},{"_id":"themes/next/layout/_partials/head.swig","hash":"6b0c495b8154ef8b2d2cb0a554e164ff22cdc962","modified":1472228352000},{"_id":"themes/next/layout/_partials/header.swig","hash":"eb028685cb3c329537bbced06c063d23e6a33817","modified":1472228352000},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"9e8e21d194ef44d271b1cca0bc1448c14d7edf4f","modified":1472228352000},{"_id":"themes/next/layout/_partials/search.swig","hash":"011b9d6c9f0a2f4654908ea20b9391f9b7981271","modified":1472228352000},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"535fc542781021c4326dec24d8495cbb1387634a","modified":1472228352000},{"_id":"themes/next/scripts/tags/full-image.js","hash":"3acce36db0feb11a982c6c799aa6b6b47df2827c","modified":1472228352000},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"49252824cd53184dc9b97b2f2d87ff28e1b3ef27","modified":1472228352000},{"_id":"themes/next/source/css/main.styl","hash":"20702c48d6053c92c5bcdbc68e8d0ef1369848a0","modified":1472228352000},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1472228352000},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1472228352000},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1472228352000},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1472228352000},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1472228352000},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1472228352000},{"_id":"themes/next/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1472228352000},{"_id":"themes/next/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1472228352000},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1472228352000},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1472228352000},{"_id":"themes/next/source/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1472228352000},{"_id":"themes/next/source/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1472228352000},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1472228352000},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1472228352000},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1472228352000},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1472228352000},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1472228352000},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1472228352000},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1472228352000},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1472228352000},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"069d1357c717572256e5cdee09574ebce529cbae","modified":1472228352000},{"_id":"themes/next/layout/_scripts/third-party/analytics.swig","hash":"0a89c04055bade7baa5962f1d5aefe438d83a244","modified":1472228352000},{"_id":"themes/next/layout/_scripts/third-party/comments.swig","hash":"907b931d775d32405d02a25b3b0a3ac03bf804d0","modified":1472228352000},{"_id":"themes/next/layout/_scripts/third-party/lean-analytics.swig","hash":"92dc60821307fc9769bea9b2d60adaeb798342af","modified":1472228352000},{"_id":"themes/next/layout/_scripts/third-party/localsearch.swig","hash":"7f7148c8f52e4d3cfc070d964160362179fa8e91","modified":1472228352000},{"_id":"themes/next/layout/_scripts/third-party/mathjax.swig","hash":"5bafc33f57508d1d04a9930165240f6e9efa8d6d","modified":1472228352000},{"_id":"themes/next/layout/_scripts/third-party/tinysou.swig","hash":"cb3a5d36dbe1630bab84e03a52733a46df7c219b","modified":1472228352000},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1472228352000},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"7ce76358411184482bb0934e70037949dd0da8ca","modified":1472228352000},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"ff5523d5dacaa77a55a24e50e6e6530c3b98bfad","modified":1472228352000},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"959b7e04a96a5596056e4009b73b6489c117597e","modified":1472228352000},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"eefe2388ff3d424694045eda21346989b123977c","modified":1472228352000},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"c07f7b2f264e5215b8ed42d67e8cef2477558364","modified":1472228352000},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"7ca5cb4daa58b3504e17f3e02975e794bc634658","modified":1472228352000},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"89c5a5240ecb223acfe1d12377df5562a943fd5d","modified":1472228352000},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"63315fcf210799f894208c9f512737096df84962","modified":1472228352000},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"328d9a9696cc2ccf59c67d3c26000d569f46344c","modified":1472228352000},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"715d5b40dc52f319fe4bff0325beb874774d9bd9","modified":1472228352000},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"78a83c38f69a8747bb74e420e6c9eeef1ea76525","modified":1472228352000},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"c8d35a6b9e3bff6d8fdb66de853065af9d37562d","modified":1472228352000},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"3ead77befa064d6327dc7afd0a5af7be59a5f196","modified":1472228352000},{"_id":"themes/next/source/css/_variables/base.styl","hash":"17624186f7a1f28daddea258d044f8e03b2f4bea","modified":1472228352000},{"_id":"themes/next/source/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1472228352000},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"39bf93769d9080fa01a9a875183b43198f79bc19","modified":1472228352000},{"_id":"themes/next/source/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1472228352000},{"_id":"themes/next/source/js/src/motion.js","hash":"269414e84df544a4ccb88519f6abae4943db3c67","modified":1472228352000},{"_id":"themes/next/source/js/src/post-details.js","hash":"2038f54e289b6da5def09689e69f623187147be5","modified":1472228352000},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1472228352000},{"_id":"themes/next/source/js/src/utils.js","hash":"e5cb720894c4bc28ca8f10b33df127fb394018d9","modified":1472228352000},{"_id":"themes/next/source/vendors/fancybox/.bower.json","hash":"cc40a9b11e52348e554c84e4a5c058056f6b7aeb","modified":1472228352000},{"_id":"themes/next/source/vendors/fancybox/.gitattributes","hash":"2db21acfbd457452462f71cc4048a943ee61b8e0","modified":1472228352000},{"_id":"themes/next/source/vendors/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1472228352000},{"_id":"themes/next/source/vendors/fastclick/bower.json","hash":"13379463c7463b4b96d13556b46faa4cc38d81e6","modified":1472228352000},{"_id":"themes/next/source/vendors/fastclick/README.md","hash":"1decd8e1adad2cd6db0ab50cf56de6035156f4ea","modified":1472228352000},{"_id":"themes/next/source/vendors/fastclick/.bower.json","hash":"93ebd5b35e632f714dcf1753e1f6db77ec74449b","modified":1472228352000},{"_id":"themes/next/source/vendors/font-awesome/.gitignore","hash":"69d152fa46b517141ec3b1114dd6134724494d83","modified":1472228352000},{"_id":"themes/next/source/vendors/font-awesome/HELP-US-OUT.txt","hash":"69a4c537d167b68a0ccf1c6febd138aeffca60d6","modified":1472228352000},{"_id":"themes/next/source/vendors/font-awesome/bower.json","hash":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1472228352000},{"_id":"themes/next/source/vendors/font-awesome/.bower.json","hash":"7da985a99674e54f514d4fd9fcd3bcea6e7e41d5","modified":1472228352000},{"_id":"themes/next/source/vendors/font-awesome/.npmignore","hash":"dcf470ab3a358103bb896a539cc03caeda10fa8b","modified":1472228352000},{"_id":"themes/next/source/vendors/jquery_lazyload/CONTRIBUTING.md","hash":"4891864c24c28efecd81a6a8d3f261145190f901","modified":1472228352000},{"_id":"themes/next/source/vendors/jquery_lazyload/README.md","hash":"895d50fa29759af7835256522e9dd7dac597765c","modified":1472228352000},{"_id":"themes/next/source/vendors/jquery_lazyload/bower.json","hash":"65bc85d12197e71c40a55c0cd7f6823995a05222","modified":1472228352000},{"_id":"themes/next/source/vendors/jquery/.bower.json","hash":"91745c2cc6c946c7275f952b2b0760b880cea69e","modified":1472228352000},{"_id":"themes/next/source/vendors/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1472228352000},{"_id":"themes/next/source/vendors/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1472228352000},{"_id":"themes/next/source/vendors/jquery_lazyload/.bower.json","hash":"b7638afc93e9cd350d0783565ee9a7da6805ad8e","modified":1472228352000},{"_id":"themes/next/source/vendors/velocity/bower.json","hash":"2ec99573e84c7117368beccb9e94b6bf35d2db03","modified":1472228352000},{"_id":"themes/next/source/vendors/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1472228352000},{"_id":"themes/next/source/vendors/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1472228352000},{"_id":"themes/next/source/vendors/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1472228352000},{"_id":"themes/next/source/vendors/velocity/.bower.json","hash":"05f960846f1c7a93dab1d3f9a1121e86812e8c88","modified":1472228352000},{"_id":"themes/next/source/vendors/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1472228352000},{"_id":"themes/next/layout/_scripts/third-party/analytics/baidu-analytics.swig","hash":"7c43d66da93cde65b473a7d6db2a86f9a42647d6","modified":1472228352000},{"_id":"themes/next/layout/_scripts/third-party/analytics/busuanzi-counter.swig","hash":"4fcbf57c4918528ab51d3d042cff92cf5aefb599","modified":1472228352000},{"_id":"themes/next/layout/_scripts/third-party/analytics/cnzz-analytics.swig","hash":"f9c6ee91c2a615edd8ca26edcc8a66b71883c238","modified":1472228352000},{"_id":"themes/next/layout/_scripts/third-party/analytics/facebook-sdk.swig","hash":"394d008e5e94575280407ad8a1607a028026cbc3","modified":1472228352000},{"_id":"themes/next/layout/_scripts/third-party/analytics/google-analytics.swig","hash":"30a23fa7e816496fdec0e932aa42e2d13098a9c2","modified":1472228352000},{"_id":"themes/next/layout/_scripts/third-party/analytics/tencent-analytics.swig","hash":"3658414379e0e8a34c45c40feadc3edc8dc55f88","modified":1472228352000},{"_id":"themes/next/layout/_scripts/third-party/comments/disqus.swig","hash":"bff3b18f56175c53f3bc6d733166c4d998e08732","modified":1472228352000},{"_id":"themes/next/layout/_scripts/third-party/comments/duoshuo.swig","hash":"696666141cdd204fd8818ac2ad18f05e320f8587","modified":1472228352000},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"2186be20e317505cd31886f1291429cc21f76703","modified":1472228352000},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"b49efc66bd055a2d0be7deabfcb02ee72a9a28c8","modified":1472228352000},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"0dfb4b3ba3180d7285e66f270e1d3fa0f132c3d2","modified":1472228352000},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"471f1627891aca5c0e1973e09fbcb01e1510d193","modified":1472228352000},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"10994990d6e0b4d965a728a22cf7f6ee29cae9f6","modified":1472228352000},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"711c8830886619d4f4a0598b0cde5499dce50c62","modified":1472228352000},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"dd8a3b22fc2f222ac6e6c05bd8a773fb039169c0","modified":1472228352000},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"5304f99581da3a31de3ecec959b7adf9002fde83","modified":1472228352000},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"54c90cf7bdbf5c596179d8dae6e671bad1292662","modified":1472228352000},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"ece571f38180febaf02ace8187ead8318a300ea7","modified":1472228352000},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"013619c472c7e4b08311c464fcbe9fcf5edde603","modified":1472228352000},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"64f5d56c08d74a338813df1265580ca0cbf0190b","modified":1472228352000},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"5ae7906dc7c1d9468c7f4b4a6feddddc555797a1","modified":1472228352000},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"c2d079788d6fc2e9a191ccdae94e50d55bf849dc","modified":1472228352000},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"b0dcca862cd0cc6e732e33d975b476d744911742","modified":1472228352000},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"38e5df90c8689a71c978fd83ba74af3d4e4e5386","modified":1472228352000},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"4303776991ef28f5742ca51c7dffe6f12f0acf34","modified":1472228352000},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1472228352000},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"9a5581a770af8964064fef7afd3e16963e45547f","modified":1472228352000},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"6ed60cc621bac096c0ed7534fa25b1a52dc571d4","modified":1472228352000},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"c2c6c4f6434b4f94aac2af5861cd769427f0ee10","modified":1472228352000},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"8829bc556ca38bfec4add4f15a2f028092ac6d46","modified":1472228352000},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1472228352000},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"a0e2030a606c934fb2c5c7373aaae04a1caac4c5","modified":1472228352000},{"_id":"themes/next/source/css/_schemes/Pisces/_full-image.styl","hash":"938d39eedc6e3d33918c1145a5bf1e79991d3fcf","modified":1472228352000},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"8d7cecde4933900c7df2db9d0a98f5f82f88dc93","modified":1472228352000},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"d09280e5b79f3b573edb30f30c7a5f03ac640986","modified":1472228352000},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"be22ad34f546a07f6d56b424338cdd898683eea4","modified":1472228352000},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"d4b7bd610ca03dbb2f5b66631c0e84a79fb4660b","modified":1472228352000},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"2f878213cb24c5ddc18877f6d15ec5c5f57745ac","modified":1472228352000},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"1b10ba2d3ad0c063c418dc94a0b7e0db4b342c53","modified":1472228352000},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"7506e7490c69a200831393c38d25e91c156bd471","modified":1472228352000},{"_id":"themes/next/source/vendors/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1472228352000},{"_id":"themes/next/source/vendors/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1472228352000},{"_id":"themes/next/source/vendors/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1472228352000},{"_id":"themes/next/source/vendors/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1472228352000},{"_id":"themes/next/source/vendors/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1472228352000},{"_id":"themes/next/source/vendors/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1472228352000},{"_id":"themes/next/source/vendors/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1472228352000},{"_id":"themes/next/source/vendors/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1472228352000},{"_id":"themes/next/source/vendors/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1472228352000},{"_id":"themes/next/source/vendors/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1472228352000},{"_id":"themes/next/source/vendors/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1472228352000},{"_id":"themes/next/source/vendors/font-awesome/css/font-awesome.css","hash":"3b87c2560832748cd06f9bfd2fd6ea8edbdae8c7","modified":1472228352000},{"_id":"themes/next/source/vendors/font-awesome/css/font-awesome.min.css","hash":"05ea25bc9b3ac48993e1fee322d3bc94b49a6e22","modified":1472228352000},{"_id":"themes/next/source/vendors/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1472228352000},{"_id":"themes/next/source/vendors/font-awesome/fonts/fontawesome-webfont.woff2","hash":"574ea2698c03ae9477db2ea3baf460ee32f1a7ea","modified":1472228352000},{"_id":"themes/next/source/vendors/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1472228352000},{"_id":"themes/next/source/vendors/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1472228352000},{"_id":"themes/next/source/vendors/font-awesome/fonts/FontAwesome.otf","hash":"0112e96f327d413938d37c1693806f468ffdbace","modified":1472228352000},{"_id":"themes/next/source/vendors/font-awesome/fonts/fontawesome-webfont.eot","hash":"b3c2f08e73320135b69c23a3908b87a12053a2f6","modified":1472228352000},{"_id":"themes/next/source/vendors/font-awesome/fonts/fontawesome-webfont.woff","hash":"507970402e328b2baeb05bde73bf9ded4e2c3a2d","modified":1472228352000},{"_id":"themes/next/source/vendors/velocity/velocity.js","hash":"e63dc7cea055ca60a95d286f32349d88b10c5a4d","modified":1472228352000},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"8994ffcce84deac0471532f270f97c44fea54dc0","modified":1472228352000},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"4da051c7f3924fa2db1e73c55b2baf1c2c150255","modified":1472228352000},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"90f8f9706cd7fe829cf06e9959a65fd3f8b994fa","modified":1472228352000},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"7778920dd105fa4de3a7ab206eeba30b1a7bac45","modified":1472228352000},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"4eff5b252d7b614e500fc7d52c97ce325e57d3ab","modified":1472228352000},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"3c46efd6601e268093ce6d7b1471d18501878f0d","modified":1472228352000},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"9bf4362a4d0ae151ada84b219d39fbe5bb8c790e","modified":1472228352000},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"ae1ca14e51de67b07dba8f61ec79ee0e2e344574","modified":1472228352000},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d27448f199fc2f9980b601bc22b87f08b5d64dd1","modified":1472228352000},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"c890ce7fe933abad7baf39764a01894924854e92","modified":1472228352000},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"6c00f6e0978f4d8f9a846a15579963728aaa6a17","modified":1472228352000},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"49c2b2c14a1e7fcc810c6be4b632975d0204c281","modified":1472228352000},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"b25132fe6a7ad67059a2c3afc60feabb479bdd75","modified":1472228352000},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"8fae54591877a73dff0b29b2be2e8935e3c63575","modified":1472228352000},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"2cdc094ecf907a02fce25ad4a607cd5c40da0f2b","modified":1472228352000},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"387ce23bba52b22a586b2dfb4ec618fe1ffd3926","modified":1472228352000},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"d543d1377c1f61b70e3adb6da0eb12797552e5f2","modified":1472228352000},{"_id":"themes/next/source/css/_common/components/post/post-more-link.styl","hash":"15063d79b5befc21820baf05d6f20cc1c1787477","modified":1472228352000},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"cbca4842a54950e2934b3b8f3cd940f122111aef","modified":1472228352000},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"a352ae5b1f8857393bf770d2e638bf15f0c9585d","modified":1472228352000},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"963105a531403d7aad6d9e5e23e3bfabb8ec065a","modified":1472228352000},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"10251257aceecb117233c9554dcf8ecfef8e2104","modified":1472228352000},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"4eb18b12fa0ea6c35925d9a64f64e2a7dae8c7fd","modified":1472228352000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"2e7ec9aaa3293941106b1bdd09055246aa3c3dc6","modified":1472228352000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"920343e41c124221a17f050bbb989494d44f7a24","modified":1472228352000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"9486ddd2cb255227db102d09a7df4cae0fabad72","modified":1472228352000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"c44f6a553ec7ea5508f2054a13be33a62a15d3a9","modified":1472228352000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"45fa7193435a8eae9960267438750b4c9fa9587f","modified":1472228352000},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"e792c8dc41561c96d128e9b421187f1c3dc978a0","modified":1472228352000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"7690b9596ec3a49befbe529a5a2649abec0faf76","modified":1472228352000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"2d3abbc85b979a648e0e579e45f16a6eba49d1e7","modified":1472228352000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"234facd038f144bd0fe09a31ed1357c5d74c517f","modified":1472228352000},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"3623e7fa4324ec1307370f33d8f287a9e20a5578","modified":1472228352000},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"618f73450cf541f88a4fddc3d22898aee49d105d","modified":1472228352000},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"c2abe4d87148e23e15d49ee225bc650de60baf46","modified":1472228352000},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"4851b981020c5cbc354a1af9b831a2dcb3cf9d39","modified":1472228352000},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"8e66c2635d48e11de616bb29c4b1323698eebc0a","modified":1472228352000},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"93b08815c4d17e2b96fef8530ec1f1064dede6ef","modified":1472228352000},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"b03f891883446f3a5548b7cc90d29c77e62f1053","modified":1472228352000},{"_id":"themes/next/source/css/_common/components/third-party/duoshuo.styl","hash":"2340dd9b3202c61d73cc708b790fac5adddbfc7f","modified":1472228352000},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"327b5f63d55ec26f7663185c1a778440588d9803","modified":1472228352000},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"795d94561888d31cb7a6ff4a125596809ea69b7d","modified":1472228352000},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"3afc459442c132c480d1d832f1a872f1070bb048","modified":1472228352000},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"5dc4859c66305f871e56cba78f64bfe3bf1b5f01","modified":1472228352000},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"8b8e8cbce98a9296c8fd77f512ae85d945f65d40","modified":1472228352000},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"8b8e8cbce98a9296c8fd77f512ae85d945f65d40","modified":1472228352000},{"_id":"themes/next/source/vendors/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1472228352000},{"_id":"themes/next/source/vendors/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1472228352000},{"_id":"themes/next/source/vendors/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1472228352000},{"_id":"themes/next/source/vendors/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1472228352000},{"_id":"themes/next/source/vendors/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1472228352000},{"_id":"themes/next/source/vendors/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1472228352000},{"_id":"themes/next/source/vendors/font-awesome/fonts/fontawesome-webfont.ttf","hash":"27cf1f2ec59aece6938c7bb2feb0e287ea778ff9","modified":1472228352000},{"_id":"themes/next/source/vendors/font-awesome/fonts/fontawesome-webfont.svg","hash":"2b3c8ba7008cc014d8fb37abc6f9f49aeda83824","modified":1472228352000}],"Category":[],"Data":[],"Page":[{"title":"about","type":"about","comments":0,"_content":"\nHi 大家好!\n\n我在深圳工作,是一个初学iOS的小菜鸟,在这里分享一些学习路上的的一些总结与心得~~~~","source":"about/index.md","raw":"---\ntitle: about\ntype: \"about\"\ncomments: false\n---\n\nHi 大家好!\n\n我在深圳工作,是一个初学iOS的小菜鸟,在这里分享一些学习路上的的一些总结与心得~~~~","date":"2016-08-26T16:19:12.000Z","updated":"2016-08-26T16:19:12.000Z","path":"about/index.html","layout":"page","_id":"cisbyxx7u00016squdoqkqxyb","content":"<p>Hi 大家好!</p>\n<p>我在深圳工作,是一个初学iOS的小菜鸟,在这里分享一些学习路上的的一些总结与心得~~~~</p>\n","excerpt":"","more":"<p>Hi 大家好!</p>\n<p>我在深圳工作,是一个初学iOS的小菜鸟,在这里分享一些学习路上的的一些总结与心得~~~~</p>\n"},{"title":"archive","layout":"page-archive","_content":"","source":"archive/index.md","raw":"---\ntitle: archive\nlayout: page-archive\n---\n","date":"2016-08-26T16:19:12.000Z","updated":"2016-08-26T16:19:12.000Z","path":"archive/index.html","comments":1,"_id":"cisbyxx7y00036squk6vpjybn","content":"","excerpt":"","more":""},{"title":"categories","date":"2016-07-27T02:45:04.000Z","type":"categories","comments":0,"_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2016-07-27 10:45:04\ntype: \"categories\"\ncomments: false\n---\n","updated":"2016-08-26T16:19:12.000Z","path":"categories/index.html","layout":"page","_id":"cisbyxx8300066squnl6pmte7","content":"","excerpt":"","more":""},{"title":"All tags","date":"2016-08-03T07:00:37.000Z","type":"tags","comments":0,"_content":"","source":"tags/index.md","raw":"---\ntitle: All tags\ndate: 2016-08-03 15:00:37\ntype: \"tags\"\ncomments: false\n---\n","updated":"2016-08-26T16:19:12.000Z","path":"tags/index.html","layout":"page","_id":"cisbyxxd500106squ66rp2hs1","content":"","excerpt":"","more":""}],"Post":[{"title":"PhotoKit学习笔记一","_content":"\nApple在iOS8中推出PhotoKit用来取代AssetsLibrary.本篇文章是自己做文章时候的几个概念:\n\n* PHAsset 代表照片库中的一个资源,与ALAsset类似,通过PHAsset可以获取和保存资源\n* PHFetchOptions 获取资源时候的参数,可以传nil,系统默认\n* PHFetchResult 表示一些列的资源集合,也可以是相册集合\n* PHAssetCollection 表示一个相册或者一个智能相册\n* PHImageManager 用于处理资源的加载,加载图片过程中带有缓存处理,可以通过传入一个PHImageRequestOptions 控制资源的输出尺寸等规格\n* PHImageRequestOptions 控制加载图片时的一系列参数","source":"_drafts/PhotoKit学习笔记一.md","raw":"---\ntitle: PhotoKit学习笔记一\ntags:\n- iOS\n- PhotoKit\n- AssetsLibrary\n---\n\nApple在iOS8中推出PhotoKit用来取代AssetsLibrary.本篇文章是自己做文章时候的几个概念:\n\n* PHAsset 代表照片库中的一个资源,与ALAsset类似,通过PHAsset可以获取和保存资源\n* PHFetchOptions 获取资源时候的参数,可以传nil,系统默认\n* PHFetchResult 表示一些列的资源集合,也可以是相册集合\n* PHAssetCollection 表示一个相册或者一个智能相册\n* PHImageManager 用于处理资源的加载,加载图片过程中带有缓存处理,可以通过传入一个PHImageRequestOptions 控制资源的输出尺寸等规格\n* PHImageRequestOptions 控制加载图片时的一系列参数","slug":"PhotoKit学习笔记一","published":0,"date":"2016-08-26T16:19:12.000Z","updated":"2016-08-26T16:19:12.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cisbyxx7p00006squppn3h5k6","content":"<p>Apple在iOS8中推出PhotoKit用来取代AssetsLibrary.本篇文章是自己做文章时候的几个概念:</p>\n<ul>\n<li>PHAsset 代表照片库中的一个资源,与ALAsset类似,通过PHAsset可以获取和保存资源</li>\n<li>PHFetchOptions 获取资源时候的参数,可以传nil,系统默认</li>\n<li>PHFetchResult 表示一些列的资源集合,也可以是相册集合</li>\n<li>PHAssetCollection 表示一个相册或者一个智能相册</li>\n<li>PHImageManager 用于处理资源的加载,加载图片过程中带有缓存处理,可以通过传入一个PHImageRequestOptions 控制资源的输出尺寸等规格</li>\n<li>PHImageRequestOptions 控制加载图片时的一系列参数</li>\n</ul>\n","excerpt":"","more":"<p>Apple在iOS8中推出PhotoKit用来取代AssetsLibrary.本篇文章是自己做文章时候的几个概念:</p>\n<ul>\n<li>PHAsset 代表照片库中的一个资源,与ALAsset类似,通过PHAsset可以获取和保存资源</li>\n<li>PHFetchOptions 获取资源时候的参数,可以传nil,系统默认</li>\n<li>PHFetchResult 表示一些列的资源集合,也可以是相册集合</li>\n<li>PHAssetCollection 表示一个相册或者一个智能相册</li>\n<li>PHImageManager 用于处理资源的加载,加载图片过程中带有缓存处理,可以通过传入一个PHImageRequestOptions 控制资源的输出尺寸等规格</li>\n<li>PHImageRequestOptions 控制加载图片时的一系列参数</li>\n</ul>\n"},{"title":"音频开发资料汇总","_content":"\n\n一.音乐播放类概念\niOS 下能支持歌曲和声音播放的的类有几个：\nSystemSound \nAVFoundtion库中的AVAudioPlayer #重要\nMediMPMusicPlayerController\n常用音频控件 \n3. MPMediaPickerController 本地音乐库选择器 \n5. MPVolumeView 播放进度条\n这里有一个PPT在解释几种概念：\nhttps://ccrma.stanford.edu/~jsanchez/NSSpain.pdf \n这教程中同时用不同机制播放例子： \nhttps://github.com/jsanchezsierra/AudioLab\n声音可视化的设计\n如果想要程序中输出声音，波形,频谱以及其它特效， \n一定要看一下这一篇教程：\niPodVisualizer\nhttp://www.raywenderlich.com/36475/how-to-make-a-music-visualizer-in-ios\n它是种用AVAudioPlayer 的averagePowerForChannel 这样接口来输出波形文件。 \nMPMusicPlayerController没有发现支持这一功能 \n这里写图片描述\naurioTouch\n另外Apple官方给出一个输出例子：aurioTouch 录音数据的波形，其中带普通波形文件，以及经过FFT运算得到频谱数据。可以参考。\n源码在此：https://developer.apple.com/library/prerelease/ios/samplecode/aurioTouch/Introduction/Intro.html\n以及更新版（苹果已经移走这个版本） \nhttps://github.com/caseytcaprice/aurioTouch2\n根据 \nhttps://github.com/irtemed88/PitchDetector\nPitchDetector\n画得更加完美的波形文件： \nhttps://github.com/irtemed88/PitchDetector\nSpeakHere\nApple官方给的例子,显示录音实时波开： \nhttps://developer.apple.com/library/ios/samplecode/SpeakHere/Introduction/Intro.html\nAvTouch\n更简单的声音转波形的例子 \nhttps://developer.apple.com/library/ios/samplecode/avTouch/Introduction/Intro.html","source":"_drafts/音频开发资料汇总.md","raw":"---\ntitle: 音频开发资料汇总\ntags:\n---\n\n\n一.音乐播放类概念\niOS 下能支持歌曲和声音播放的的类有几个：\nSystemSound \nAVFoundtion库中的AVAudioPlayer #重要\nMediMPMusicPlayerController\n常用音频控件 \n3. MPMediaPickerController 本地音乐库选择器 \n5. MPVolumeView 播放进度条\n这里有一个PPT在解释几种概念：\nhttps://ccrma.stanford.edu/~jsanchez/NSSpain.pdf \n这教程中同时用不同机制播放例子： \nhttps://github.com/jsanchezsierra/AudioLab\n声音可视化的设计\n如果想要程序中输出声音，波形,频谱以及其它特效， \n一定要看一下这一篇教程：\niPodVisualizer\nhttp://www.raywenderlich.com/36475/how-to-make-a-music-visualizer-in-ios\n它是种用AVAudioPlayer 的averagePowerForChannel 这样接口来输出波形文件。 \nMPMusicPlayerController没有发现支持这一功能 \n这里写图片描述\naurioTouch\n另外Apple官方给出一个输出例子：aurioTouch 录音数据的波形，其中带普通波形文件，以及经过FFT运算得到频谱数据。可以参考。\n源码在此：https://developer.apple.com/library/prerelease/ios/samplecode/aurioTouch/Introduction/Intro.html\n以及更新版（苹果已经移走这个版本） \nhttps://github.com/caseytcaprice/aurioTouch2\n根据 \nhttps://github.com/irtemed88/PitchDetector\nPitchDetector\n画得更加完美的波形文件： \nhttps://github.com/irtemed88/PitchDetector\nSpeakHere\nApple官方给的例子,显示录音实时波开： \nhttps://developer.apple.com/library/ios/samplecode/SpeakHere/Introduction/Intro.html\nAvTouch\n更简单的声音转波形的例子 \nhttps://developer.apple.com/library/ios/samplecode/avTouch/Introduction/Intro.html","slug":"音频开发资料汇总","published":0,"date":"2016-08-26T16:19:12.000Z","updated":"2016-08-26T16:19:12.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cisbyxx7v00026squ1ota5ahy","content":"<p>一.音乐播放类概念<br>iOS 下能支持歌曲和声音播放的的类有几个：<br>SystemSound<br>AVFoundtion库中的AVAudioPlayer #重要<br>MediMPMusicPlayerController<br>常用音频控件 </p>\n<ol>\n<li>MPMediaPickerController 本地音乐库选择器 </li>\n<li>MPVolumeView 播放进度条<br>这里有一个PPT在解释几种概念：<br><a href=\"https://ccrma.stanford.edu/~jsanchez/NSSpain.pdf\" target=\"_blank\" rel=\"external\">https://ccrma.stanford.edu/~jsanchez/NSSpain.pdf</a><br>这教程中同时用不同机制播放例子：<br><a href=\"https://github.com/jsanchezsierra/AudioLab\" target=\"_blank\" rel=\"external\">https://github.com/jsanchezsierra/AudioLab</a><br>声音可视化的设计<br>如果想要程序中输出声音，波形,频谱以及其它特效，<br>一定要看一下这一篇教程：<br>iPodVisualizer<br><a href=\"http://www.raywenderlich.com/36475/how-to-make-a-music-visualizer-in-ios\" target=\"_blank\" rel=\"external\">http://www.raywenderlich.com/36475/how-to-make-a-music-visualizer-in-ios</a><br>它是种用AVAudioPlayer 的averagePowerForChannel 这样接口来输出波形文件。<br>MPMusicPlayerController没有发现支持这一功能<br>这里写图片描述<br>aurioTouch<br>另外Apple官方给出一个输出例子：aurioTouch 录音数据的波形，其中带普通波形文件，以及经过FFT运算得到频谱数据。可以参考。<br>源码在此：<a href=\"https://developer.apple.com/library/prerelease/ios/samplecode/aurioTouch/Introduction/Intro.html\" target=\"_blank\" rel=\"external\">https://developer.apple.com/library/prerelease/ios/samplecode/aurioTouch/Introduction/Intro.html</a><br>以及更新版（苹果已经移走这个版本）<br><a href=\"https://github.com/caseytcaprice/aurioTouch2\" target=\"_blank\" rel=\"external\">https://github.com/caseytcaprice/aurioTouch2</a><br>根据<br><a href=\"https://github.com/irtemed88/PitchDetector\" target=\"_blank\" rel=\"external\">https://github.com/irtemed88/PitchDetector</a><br>PitchDetector<br>画得更加完美的波形文件：<br><a href=\"https://github.com/irtemed88/PitchDetector\" target=\"_blank\" rel=\"external\">https://github.com/irtemed88/PitchDetector</a><br>SpeakHere<br>Apple官方给的例子,显示录音实时波开：<br><a href=\"https://developer.apple.com/library/ios/samplecode/SpeakHere/Introduction/Intro.html\" target=\"_blank\" rel=\"external\">https://developer.apple.com/library/ios/samplecode/SpeakHere/Introduction/Intro.html</a><br>AvTouch<br>更简单的声音转波形的例子<br><a href=\"https://developer.apple.com/library/ios/samplecode/avTouch/Introduction/Intro.html\" target=\"_blank\" rel=\"external\">https://developer.apple.com/library/ios/samplecode/avTouch/Introduction/Intro.html</a></li>\n</ol>\n","excerpt":"","more":"<p>一.音乐播放类概念<br>iOS 下能支持歌曲和声音播放的的类有几个：<br>SystemSound<br>AVFoundtion库中的AVAudioPlayer #重要<br>MediMPMusicPlayerController<br>常用音频控件 </p>\n<ol>\n<li>MPMediaPickerController 本地音乐库选择器 </li>\n<li>MPVolumeView 播放进度条<br>这里有一个PPT在解释几种概念：<br><a href=\"https://ccrma.stanford.edu/~jsanchez/NSSpain.pdf\">https://ccrma.stanford.edu/~jsanchez/NSSpain.pdf</a><br>这教程中同时用不同机制播放例子：<br><a href=\"https://github.com/jsanchezsierra/AudioLab\">https://github.com/jsanchezsierra/AudioLab</a><br>声音可视化的设计<br>如果想要程序中输出声音，波形,频谱以及其它特效，<br>一定要看一下这一篇教程：<br>iPodVisualizer<br><a href=\"http://www.raywenderlich.com/36475/how-to-make-a-music-visualizer-in-ios\">http://www.raywenderlich.com/36475/how-to-make-a-music-visualizer-in-ios</a><br>它是种用AVAudioPlayer 的averagePowerForChannel 这样接口来输出波形文件。<br>MPMusicPlayerController没有发现支持这一功能<br>这里写图片描述<br>aurioTouch<br>另外Apple官方给出一个输出例子：aurioTouch 录音数据的波形，其中带普通波形文件，以及经过FFT运算得到频谱数据。可以参考。<br>源码在此：<a href=\"https://developer.apple.com/library/prerelease/ios/samplecode/aurioTouch/Introduction/Intro.html\">https://developer.apple.com/library/prerelease/ios/samplecode/aurioTouch/Introduction/Intro.html</a><br>以及更新版（苹果已经移走这个版本）<br><a href=\"https://github.com/caseytcaprice/aurioTouch2\">https://github.com/caseytcaprice/aurioTouch2</a><br>根据<br><a href=\"https://github.com/irtemed88/PitchDetector\">https://github.com/irtemed88/PitchDetector</a><br>PitchDetector<br>画得更加完美的波形文件：<br><a href=\"https://github.com/irtemed88/PitchDetector\">https://github.com/irtemed88/PitchDetector</a><br>SpeakHere<br>Apple官方给的例子,显示录音实时波开：<br><a href=\"https://developer.apple.com/library/ios/samplecode/SpeakHere/Introduction/Intro.html\">https://developer.apple.com/library/ios/samplecode/SpeakHere/Introduction/Intro.html</a><br>AvTouch<br>更简单的声音转波形的例子<br><a href=\"https://developer.apple.com/library/ios/samplecode/avTouch/Introduction/Intro.html\">https://developer.apple.com/library/ios/samplecode/avTouch/Introduction/Intro.html</a></li>\n</ol>\n"},{"title":"iOS处理图片大小总结","_content":"\niOS中图片相关的内容非常多,iOS中的图片相关的API也非常多,从UIKit中的UIImage,到CoreGraphic中CGImage,CoreImage中的CIImage.\n\n通常情况下使用UIImage来缩放图片大小,在UIImage有`contentMode`属性,会有多个枚举属性`.ScaleAspectFit`,`.ScaleAspectFill`,通过赖进行\n\n### 使图片缩放改变大小\n\n在进行图片的缩放的大小时,首先需要了解到缩放的目标大小.\n\n#### 直接通过缩放因子改变大小\n\n最简单的方式就是使用常量factor,去乘以图像的原来大小.\n\n```objc\nlet size = CGSize(width: image.size.width/2, height: image.size.height/2)\n```\n\n或者直接使用`CGAffineTransform`改变transform\n\n```objc\nlet size = CGSizeApplyAffineTransform(image.size, CGAffineTransformScale(0.5,0.5))\n```\n\n#### 不改变Aspect Ratio的缩放\n\n如果要将一个图片放到一个rect中,而不改变它原始的aspect ratio.可以使用`AVFoundation`的`AVMakeRectWithAspectRatioInsideRect`方法.\n\n```objc\nimport AVFoundation\nlet rect = AVMakeRectWithAspectRatioInsideRect(image.size, imageView.bounds)\n```\n\n****\n\n### 改变图片大小的方法汇总\n\n有多种方法可以改变图片的大小,每种方法的性能不同.\n\n#### `UIGraphicsBeginImageContextWithOptions`和`UIImage -drawInRect:`\n\n通常最高阶的API是UIKit,使用UIImage然后使用Graphic Context实时绘出一个大小不同的图片.可以使用`UIGraphicsBeginImageContextWithOptions`和`UIGraphicsGetImageFromCurrentImageContext`获取缩略图.\n\n```objc\nlet image = UIImage(contentsOfFile: self.URL.absoluteString!)\n\nlet size = CGSizeApplyAffineTransform(image.size, CGAffineTransformMakeScale(0.5, 0.5))\nlet hasAlpha = false\nlet scale: CGFloat = 0.0 // Automatically use scale factor of main screen\n\nUIGraphicsBeginImageContextWithOptions(size, !hasAlpha, scale)\nimage.drawInRect(CGRect(origin: CGPointZero, size: size))\n\nlet scaledImage = UIGraphicsGetImageFromCurrentImageContext()\nUIGraphicsEndImageContext()\n```\n\n`UIGraphicsBeginImageContextWithOptions()`会创建一个渲染context,然后使用原来的图片draw.\n\n* 第一个参数`size`就是缩小以后的图片的大小.\n* 第二个参数`isOpaque`,用来描述图片的alpha通道是否渲染.设置为`false`表示完全不透明\n* 第三个参数`scale`,就是dispaly scale factor.如果设置成`0.0`那么就是main screen使用的(retina是2.0,iphone6p是3.0)\n\n\n#### `CGBitmapContextCreate`和`CGContextDrawImage`\n\n使用的Core Graphic/Quartz 2D使用的是低阶API,可以使用CGImage,使用`CGBitmapContextCreate()`和`CGBitmapContextCreateImage()`获取缩略图.\n\n```objc\nlet cgImage = UIImage(contentsOfFile: self.URL.absoluteString!).CGImage\n\nlet width = CGImageGetWidth(cgImage) / 2\nlet height = CGImageGetHeight(cgImage) / 2\nlet bitsPerComponent = CGImageGetBitsPerComponent(cgImage)\nlet bytesPerRow = CGImageGetBytesPerRow(cgImage)\nlet colorSpace = CGImageGetColorSpace(cgImage)\nlet bitmapInfo = CGImageGetBitmapInfo(cgImage)\nlet context = CGBitmapContextCreate(nil, width, height, bitsPerComponent, bytesPerRow, colorSpace, bitmapInfo.rawValue)\n\nCGContextSetInterpolationQuality(context, kCGInterpolationHigh)\nCGContextDrawImage(context, CGRect(origin: CGPointZero, size: CGSize(width: CGFloat(width), height: CGFloat(height))), cgImage)\nlet scaledImage = CGBitmapContextCreateImage(context).flatMap { UIImage(CGImage: $0) }\n\n```\n\n#### CGImageSourceCreateThumbnailAtIndex\n\n也可以使用Image I/O的framework也可以用来缩放图片大小.\n\n```objc\nimport ImageIO\n\nif let imageSource = CGImageSourceCreateWithURL(self.URL, nil) {\n    let options: [NSString: NSObject] = [\n        kCGImageSourceThumbnailMaxPixelSize: max(size.width, size.height) / 2.0,\n        kCGImageSourceCreateThumbnailFromImageAlways: true\n    ]\n\n    let scaledImage = CGImageSourceCreateThumbnailAtIndex(imageSource, 0, options).flatMap { UIImage(CGImage: $0) }\n}\n```\n\n#### 使用CoreImage进行Lanczos重新采样\n\nCoreImage中内置Lanczos Resampling,具体的函数是`CILanczosScaleTransform` filter.\n\n```objc\nlet image = CIImage(contentsOfURL: self.URL)\n\nlet filter = CIFilter(name: \"CILanczosScaleTransform\")!\nfilter.setValue(image, forKey: \"inputImage\")\nfilter.setValue(0.5, forKey: \"inputScale\")\nfilter.setValue(1.0, forKey: \"inputAspectRatio\")\nlet outputImage = filter.valueForKey(\"outputImage\") as! CIImage\n\nlet context = CIContext(options: [kCIContextUseSoftwareRenderer: false])\nlet scaledImage = UIImage(CGImage: self.context.createCGImage(outputImage, fromRect: outputImage.extent()))\n```\n\n#### 在Accelerate中的vImage\n\n使用Accelerat framework包括`vImage`的图像处理函数.\n\n```objc\nlet cgImage = UIImage(contentsOfFile: self.URL.absoluteString!).CGImage\n\n// create a source buffer\nvar format = vImage_CGImageFormat(bitsPerComponent: 8, bitsPerPixel: 32, colorSpace: nil, \n    bitmapInfo: CGBitmapInfo(rawValue: CGImageAlphaInfo.First.rawValue), \n    version: 0, decode: nil, renderingIntent: CGColorRenderingIntent.RenderingIntentDefault)\nvar sourceBuffer = vImage_Buffer()\ndefer {\n    sourceBuffer.data.dealloc(Int(sourceBuffer.height) * Int(sourceBuffer.height) * 4)\n}\n\nvar error = vImageBuffer_InitWithCGImage(&sourceBuffer, &format, nil, cgImage, numericCast(kvImageNoFlags))\nguard error == kvImageNoError else { return nil }\n\n// create a destination buffer\nlet scale = UIScreen.mainScreen().scale\nlet destWidth = Int(image.size.width * 0.5 * scale)\nlet destHeight = Int(image.size.height * 0.5 * scale)\nlet bytesPerPixel = CGImageGetBitsPerPixel(image.CGImage) / 8\nlet destBytesPerRow = destWidth * bytesPerPixel\nlet destData = UnsafeMutablePointer<UInt8>.alloc(destHeight * destBytesPerRow)\ndefer {\n    destData.dealloc(destHeight * destBytesPerRow)\n}\nvar destBuffer = vImage_Buffer(data: destData, height: vImagePixelCount(destHeight), width: vImagePixelCount(destWidth), rowBytes: destBytesPerRow)\n\n// scale the image\nerror = vImageScale_ARGB8888(&sourceBuffer, &destBuffer, nil, numericCast(kvImageHighQualityResampling))\nguard error == kvImageNoError else { return nil }\n\n// create a CGImage from vImage_Buffer\nlet destCGImage = vImageCreateCGImageFromBuffer(&destBuffer, &format, nil, nil, numericCast(kvImageNoFlags), &error)?.takeRetainedValue()\nguard error == kvImageNoError else { return nil }\n\n// create a UIImage\nlet scaledImage = destCGImage.flatMap { UIImage(CGImage: $0, scale: 0.0, orientation: image.imageOrientation) }\n```\n\n****\n\n### 各种方式的选择\n\n* UIKit,CoreGraphics以及Image I/O的性能优秀.如果仅仅是缩放的话最好使用CoreImage.\n* 日常的图像缩放以后不进行其他操作的话,使用`UIGraphicsBeginImageContextWithOptions`是最好的选择.\n* 如果对图像质量有更高的要求,最好使用`CGBitmapContextCreate`和`CGContextSetInterpolationQuality`.\n* 如果缩放的目的是显示缩略图,那么最好使用`CGImageSourceCreateThumbnailAtIndex`\n* 缩放不要用`vImage`\n\n> 参考文档: [Image Resizing Techniques](http://nshipster.com/image-resizing/)","source":"_posts/iOS处理图片大小总结.md","raw":"---\ntitle: iOS处理图片大小总结\ntags:\n---\n\niOS中图片相关的内容非常多,iOS中的图片相关的API也非常多,从UIKit中的UIImage,到CoreGraphic中CGImage,CoreImage中的CIImage.\n\n通常情况下使用UIImage来缩放图片大小,在UIImage有`contentMode`属性,会有多个枚举属性`.ScaleAspectFit`,`.ScaleAspectFill`,通过赖进行\n\n### 使图片缩放改变大小\n\n在进行图片的缩放的大小时,首先需要了解到缩放的目标大小.\n\n#### 直接通过缩放因子改变大小\n\n最简单的方式就是使用常量factor,去乘以图像的原来大小.\n\n```objc\nlet size = CGSize(width: image.size.width/2, height: image.size.height/2)\n```\n\n或者直接使用`CGAffineTransform`改变transform\n\n```objc\nlet size = CGSizeApplyAffineTransform(image.size, CGAffineTransformScale(0.5,0.5))\n```\n\n#### 不改变Aspect Ratio的缩放\n\n如果要将一个图片放到一个rect中,而不改变它原始的aspect ratio.可以使用`AVFoundation`的`AVMakeRectWithAspectRatioInsideRect`方法.\n\n```objc\nimport AVFoundation\nlet rect = AVMakeRectWithAspectRatioInsideRect(image.size, imageView.bounds)\n```\n\n****\n\n### 改变图片大小的方法汇总\n\n有多种方法可以改变图片的大小,每种方法的性能不同.\n\n#### `UIGraphicsBeginImageContextWithOptions`和`UIImage -drawInRect:`\n\n通常最高阶的API是UIKit,使用UIImage然后使用Graphic Context实时绘出一个大小不同的图片.可以使用`UIGraphicsBeginImageContextWithOptions`和`UIGraphicsGetImageFromCurrentImageContext`获取缩略图.\n\n```objc\nlet image = UIImage(contentsOfFile: self.URL.absoluteString!)\n\nlet size = CGSizeApplyAffineTransform(image.size, CGAffineTransformMakeScale(0.5, 0.5))\nlet hasAlpha = false\nlet scale: CGFloat = 0.0 // Automatically use scale factor of main screen\n\nUIGraphicsBeginImageContextWithOptions(size, !hasAlpha, scale)\nimage.drawInRect(CGRect(origin: CGPointZero, size: size))\n\nlet scaledImage = UIGraphicsGetImageFromCurrentImageContext()\nUIGraphicsEndImageContext()\n```\n\n`UIGraphicsBeginImageContextWithOptions()`会创建一个渲染context,然后使用原来的图片draw.\n\n* 第一个参数`size`就是缩小以后的图片的大小.\n* 第二个参数`isOpaque`,用来描述图片的alpha通道是否渲染.设置为`false`表示完全不透明\n* 第三个参数`scale`,就是dispaly scale factor.如果设置成`0.0`那么就是main screen使用的(retina是2.0,iphone6p是3.0)\n\n\n#### `CGBitmapContextCreate`和`CGContextDrawImage`\n\n使用的Core Graphic/Quartz 2D使用的是低阶API,可以使用CGImage,使用`CGBitmapContextCreate()`和`CGBitmapContextCreateImage()`获取缩略图.\n\n```objc\nlet cgImage = UIImage(contentsOfFile: self.URL.absoluteString!).CGImage\n\nlet width = CGImageGetWidth(cgImage) / 2\nlet height = CGImageGetHeight(cgImage) / 2\nlet bitsPerComponent = CGImageGetBitsPerComponent(cgImage)\nlet bytesPerRow = CGImageGetBytesPerRow(cgImage)\nlet colorSpace = CGImageGetColorSpace(cgImage)\nlet bitmapInfo = CGImageGetBitmapInfo(cgImage)\nlet context = CGBitmapContextCreate(nil, width, height, bitsPerComponent, bytesPerRow, colorSpace, bitmapInfo.rawValue)\n\nCGContextSetInterpolationQuality(context, kCGInterpolationHigh)\nCGContextDrawImage(context, CGRect(origin: CGPointZero, size: CGSize(width: CGFloat(width), height: CGFloat(height))), cgImage)\nlet scaledImage = CGBitmapContextCreateImage(context).flatMap { UIImage(CGImage: $0) }\n\n```\n\n#### CGImageSourceCreateThumbnailAtIndex\n\n也可以使用Image I/O的framework也可以用来缩放图片大小.\n\n```objc\nimport ImageIO\n\nif let imageSource = CGImageSourceCreateWithURL(self.URL, nil) {\n    let options: [NSString: NSObject] = [\n        kCGImageSourceThumbnailMaxPixelSize: max(size.width, size.height) / 2.0,\n        kCGImageSourceCreateThumbnailFromImageAlways: true\n    ]\n\n    let scaledImage = CGImageSourceCreateThumbnailAtIndex(imageSource, 0, options).flatMap { UIImage(CGImage: $0) }\n}\n```\n\n#### 使用CoreImage进行Lanczos重新采样\n\nCoreImage中内置Lanczos Resampling,具体的函数是`CILanczosScaleTransform` filter.\n\n```objc\nlet image = CIImage(contentsOfURL: self.URL)\n\nlet filter = CIFilter(name: \"CILanczosScaleTransform\")!\nfilter.setValue(image, forKey: \"inputImage\")\nfilter.setValue(0.5, forKey: \"inputScale\")\nfilter.setValue(1.0, forKey: \"inputAspectRatio\")\nlet outputImage = filter.valueForKey(\"outputImage\") as! CIImage\n\nlet context = CIContext(options: [kCIContextUseSoftwareRenderer: false])\nlet scaledImage = UIImage(CGImage: self.context.createCGImage(outputImage, fromRect: outputImage.extent()))\n```\n\n#### 在Accelerate中的vImage\n\n使用Accelerat framework包括`vImage`的图像处理函数.\n\n```objc\nlet cgImage = UIImage(contentsOfFile: self.URL.absoluteString!).CGImage\n\n// create a source buffer\nvar format = vImage_CGImageFormat(bitsPerComponent: 8, bitsPerPixel: 32, colorSpace: nil, \n    bitmapInfo: CGBitmapInfo(rawValue: CGImageAlphaInfo.First.rawValue), \n    version: 0, decode: nil, renderingIntent: CGColorRenderingIntent.RenderingIntentDefault)\nvar sourceBuffer = vImage_Buffer()\ndefer {\n    sourceBuffer.data.dealloc(Int(sourceBuffer.height) * Int(sourceBuffer.height) * 4)\n}\n\nvar error = vImageBuffer_InitWithCGImage(&sourceBuffer, &format, nil, cgImage, numericCast(kvImageNoFlags))\nguard error == kvImageNoError else { return nil }\n\n// create a destination buffer\nlet scale = UIScreen.mainScreen().scale\nlet destWidth = Int(image.size.width * 0.5 * scale)\nlet destHeight = Int(image.size.height * 0.5 * scale)\nlet bytesPerPixel = CGImageGetBitsPerPixel(image.CGImage) / 8\nlet destBytesPerRow = destWidth * bytesPerPixel\nlet destData = UnsafeMutablePointer<UInt8>.alloc(destHeight * destBytesPerRow)\ndefer {\n    destData.dealloc(destHeight * destBytesPerRow)\n}\nvar destBuffer = vImage_Buffer(data: destData, height: vImagePixelCount(destHeight), width: vImagePixelCount(destWidth), rowBytes: destBytesPerRow)\n\n// scale the image\nerror = vImageScale_ARGB8888(&sourceBuffer, &destBuffer, nil, numericCast(kvImageHighQualityResampling))\nguard error == kvImageNoError else { return nil }\n\n// create a CGImage from vImage_Buffer\nlet destCGImage = vImageCreateCGImageFromBuffer(&destBuffer, &format, nil, nil, numericCast(kvImageNoFlags), &error)?.takeRetainedValue()\nguard error == kvImageNoError else { return nil }\n\n// create a UIImage\nlet scaledImage = destCGImage.flatMap { UIImage(CGImage: $0, scale: 0.0, orientation: image.imageOrientation) }\n```\n\n****\n\n### 各种方式的选择\n\n* UIKit,CoreGraphics以及Image I/O的性能优秀.如果仅仅是缩放的话最好使用CoreImage.\n* 日常的图像缩放以后不进行其他操作的话,使用`UIGraphicsBeginImageContextWithOptions`是最好的选择.\n* 如果对图像质量有更高的要求,最好使用`CGBitmapContextCreate`和`CGContextSetInterpolationQuality`.\n* 如果缩放的目的是显示缩略图,那么最好使用`CGImageSourceCreateThumbnailAtIndex`\n* 缩放不要用`vImage`\n\n> 参考文档: [Image Resizing Techniques](http://nshipster.com/image-resizing/)","slug":"iOS处理图片大小总结","published":1,"date":"2016-08-26T16:19:12.000Z","updated":"2016-08-26T16:19:12.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cisbyxx8000056squjs3a2zwp","content":"<p>iOS中图片相关的内容非常多,iOS中的图片相关的API也非常多,从UIKit中的UIImage,到CoreGraphic中CGImage,CoreImage中的CIImage.</p>\n<p>通常情况下使用UIImage来缩放图片大小,在UIImage有<code>contentMode</code>属性,会有多个枚举属性<code>.ScaleAspectFit</code>,<code>.ScaleAspectFill</code>,通过赖进行</p>\n<h3 id=\"使图片缩放改变大小\"><a href=\"#使图片缩放改变大小\" class=\"headerlink\" title=\"使图片缩放改变大小\"></a>使图片缩放改变大小</h3><p>在进行图片的缩放的大小时,首先需要了解到缩放的目标大小.</p>\n<h4 id=\"直接通过缩放因子改变大小\"><a href=\"#直接通过缩放因子改变大小\" class=\"headerlink\" title=\"直接通过缩放因子改变大小\"></a>直接通过缩放因子改变大小</h4><p>最简单的方式就是使用常量factor,去乘以图像的原来大小.</p>\n<figure class=\"highlight objc\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">let size = <span class=\"built_in\">CGSize</span>(width: image.size.width/<span class=\"number\">2</span>, height: image.size.height/<span class=\"number\">2</span>)</div></pre></td></tr></table></figure>\n<p>或者直接使用<code>CGAffineTransform</code>改变transform</p>\n<figure class=\"highlight objc\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">let size = <span class=\"built_in\">CGSizeApplyAffineTransform</span>(image.size, <span class=\"built_in\">CGAffineTransformScale</span>(<span class=\"number\">0.5</span>,<span class=\"number\">0.5</span>))</div></pre></td></tr></table></figure>\n<h4 id=\"不改变Aspect-Ratio的缩放\"><a href=\"#不改变Aspect-Ratio的缩放\" class=\"headerlink\" title=\"不改变Aspect Ratio的缩放\"></a>不改变Aspect Ratio的缩放</h4><p>如果要将一个图片放到一个rect中,而不改变它原始的aspect ratio.可以使用<code>AVFoundation</code>的<code>AVMakeRectWithAspectRatioInsideRect</code>方法.</p>\n<figure class=\"highlight objc\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">import <span class=\"built_in\">AVFoundation</span></div><div class=\"line\">let rect = <span class=\"built_in\">AVMakeRectWithAspectRatioInsideRect</span>(image.size, imageView.bounds)</div></pre></td></tr></table></figure>\n<hr>\n<h3 id=\"改变图片大小的方法汇总\"><a href=\"#改变图片大小的方法汇总\" class=\"headerlink\" title=\"改变图片大小的方法汇总\"></a>改变图片大小的方法汇总</h3><p>有多种方法可以改变图片的大小,每种方法的性能不同.</p>\n<h4 id=\"UIGraphicsBeginImageContextWithOptions和UIImage-drawInRect\"><a href=\"#UIGraphicsBeginImageContextWithOptions和UIImage-drawInRect\" class=\"headerlink\" title=\"UIGraphicsBeginImageContextWithOptions和UIImage -drawInRect:\"></a><code>UIGraphicsBeginImageContextWithOptions</code>和<code>UIImage -drawInRect:</code></h4><p>通常最高阶的API是UIKit,使用UIImage然后使用Graphic Context实时绘出一个大小不同的图片.可以使用<code>UIGraphicsBeginImageContextWithOptions</code>和<code>UIGraphicsGetImageFromCurrentImageContext</code>获取缩略图.</p>\n<figure class=\"highlight objc\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">let image = <span class=\"built_in\">UIImage</span>(contentsOfFile: <span class=\"keyword\">self</span>.URL.absoluteString!)</div><div class=\"line\"></div><div class=\"line\">let size = <span class=\"built_in\">CGSizeApplyAffineTransform</span>(image.size, <span class=\"built_in\">CGAffineTransformMakeScale</span>(<span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>))</div><div class=\"line\">let hasAlpha = <span class=\"literal\">false</span></div><div class=\"line\">let scale: <span class=\"built_in\">CGFloat</span> = <span class=\"number\">0.0</span> <span class=\"comment\">// Automatically use scale factor of main screen</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"built_in\">UIGraphicsBeginImageContextWithOptions</span>(size, !hasAlpha, scale)</div><div class=\"line\">image.drawInRect(<span class=\"built_in\">CGRect</span>(origin: <span class=\"built_in\">CGPointZero</span>, size: size))</div><div class=\"line\"></div><div class=\"line\">let scaledImage = <span class=\"built_in\">UIGraphicsGetImageFromCurrentImageContext</span>()</div><div class=\"line\"><span class=\"built_in\">UIGraphicsEndImageContext</span>()</div></pre></td></tr></table></figure>\n<p><code>UIGraphicsBeginImageContextWithOptions()</code>会创建一个渲染context,然后使用原来的图片draw.</p>\n<ul>\n<li>第一个参数<code>size</code>就是缩小以后的图片的大小.</li>\n<li>第二个参数<code>isOpaque</code>,用来描述图片的alpha通道是否渲染.设置为<code>false</code>表示完全不透明</li>\n<li>第三个参数<code>scale</code>,就是dispaly scale factor.如果设置成<code>0.0</code>那么就是main screen使用的(retina是2.0,iphone6p是3.0)</li>\n</ul>\n<h4 id=\"CGBitmapContextCreate和CGContextDrawImage\"><a href=\"#CGBitmapContextCreate和CGContextDrawImage\" class=\"headerlink\" title=\"CGBitmapContextCreate和CGContextDrawImage\"></a><code>CGBitmapContextCreate</code>和<code>CGContextDrawImage</code></h4><p>使用的Core Graphic/Quartz 2D使用的是低阶API,可以使用CGImage,使用<code>CGBitmapContextCreate()</code>和<code>CGBitmapContextCreateImage()</code>获取缩略图.</p>\n<figure class=\"highlight objc\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\">let cgImage = <span class=\"built_in\">UIImage</span>(contentsOfFile: <span class=\"keyword\">self</span>.URL.absoluteString!).CGImage</div><div class=\"line\"></div><div class=\"line\">let width = <span class=\"built_in\">CGImageGetWidth</span>(cgImage) / <span class=\"number\">2</span></div><div class=\"line\">let height = <span class=\"built_in\">CGImageGetHeight</span>(cgImage) / <span class=\"number\">2</span></div><div class=\"line\">let bitsPerComponent = <span class=\"built_in\">CGImageGetBitsPerComponent</span>(cgImage)</div><div class=\"line\">let bytesPerRow = <span class=\"built_in\">CGImageGetBytesPerRow</span>(cgImage)</div><div class=\"line\">let colorSpace = <span class=\"built_in\">CGImageGetColorSpace</span>(cgImage)</div><div class=\"line\">let bitmapInfo = <span class=\"built_in\">CGImageGetBitmapInfo</span>(cgImage)</div><div class=\"line\">let context = <span class=\"built_in\">CGBitmapContextCreate</span>(<span class=\"literal\">nil</span>, width, height, bitsPerComponent, bytesPerRow, colorSpace, bitmapInfo.rawValue)</div><div class=\"line\"></div><div class=\"line\"><span class=\"built_in\">CGContextSetInterpolationQuality</span>(context, kCGInterpolationHigh)</div><div class=\"line\"><span class=\"built_in\">CGContextDrawImage</span>(context, <span class=\"built_in\">CGRect</span>(origin: <span class=\"built_in\">CGPointZero</span>, size: <span class=\"built_in\">CGSize</span>(width: <span class=\"built_in\">CGFloat</span>(width), height: <span class=\"built_in\">CGFloat</span>(height))), cgImage)</div><div class=\"line\">let scaledImage = <span class=\"built_in\">CGBitmapContextCreateImage</span>(context).flatMap &#123; <span class=\"built_in\">UIImage</span>(<span class=\"built_in\">CGImage</span>: $<span class=\"number\">0</span>) &#125;</div></pre></td></tr></table></figure>\n<h4 id=\"CGImageSourceCreateThumbnailAtIndex\"><a href=\"#CGImageSourceCreateThumbnailAtIndex\" class=\"headerlink\" title=\"CGImageSourceCreateThumbnailAtIndex\"></a>CGImageSourceCreateThumbnailAtIndex</h4><p>也可以使用Image I/O的framework也可以用来缩放图片大小.</p>\n<figure class=\"highlight objc\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">import ImageIO</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">if</span> let imageSource = <span class=\"built_in\">CGImageSourceCreateWithURL</span>(<span class=\"keyword\">self</span>.URL, <span class=\"literal\">nil</span>) &#123;</div><div class=\"line\">    let options: [<span class=\"built_in\">NSString</span>: <span class=\"built_in\">NSObject</span>] = [</div><div class=\"line\">        kCGImageSourceThumbnailMaxPixelSize: max(size.width, size.height) / <span class=\"number\">2.0</span>,</div><div class=\"line\">        kCGImageSourceCreateThumbnailFromImageAlways: <span class=\"literal\">true</span></div><div class=\"line\">    ]</div><div class=\"line\"></div><div class=\"line\">    let scaledImage = <span class=\"built_in\">CGImageSourceCreateThumbnailAtIndex</span>(imageSource, <span class=\"number\">0</span>, options).flatMap &#123; <span class=\"built_in\">UIImage</span>(<span class=\"built_in\">CGImage</span>: $<span class=\"number\">0</span>) &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h4 id=\"使用CoreImage进行Lanczos重新采样\"><a href=\"#使用CoreImage进行Lanczos重新采样\" class=\"headerlink\" title=\"使用CoreImage进行Lanczos重新采样\"></a>使用CoreImage进行Lanczos重新采样</h4><p>CoreImage中内置Lanczos Resampling,具体的函数是<code>CILanczosScaleTransform</code> filter.</p>\n<figure class=\"highlight objc\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">let image = <span class=\"built_in\">CIImage</span>(contentsOfURL: <span class=\"keyword\">self</span>.URL)</div><div class=\"line\"></div><div class=\"line\">let filter = <span class=\"built_in\">CIFilter</span>(name: <span class=\"string\">\"CILanczosScaleTransform\"</span>)!</div><div class=\"line\">filter.setValue(image, forKey: <span class=\"string\">\"inputImage\"</span>)</div><div class=\"line\">filter.setValue(<span class=\"number\">0.5</span>, forKey: <span class=\"string\">\"inputScale\"</span>)</div><div class=\"line\">filter.setValue(<span class=\"number\">1.0</span>, forKey: <span class=\"string\">\"inputAspectRatio\"</span>)</div><div class=\"line\">let outputImage = filter.valueForKey(<span class=\"string\">\"outputImage\"</span>) as! <span class=\"built_in\">CIImage</span></div><div class=\"line\"></div><div class=\"line\">let context = <span class=\"built_in\">CIContext</span>(options: [kCIContextUseSoftwareRenderer: <span class=\"literal\">false</span>])</div><div class=\"line\">let scaledImage = <span class=\"built_in\">UIImage</span>(<span class=\"built_in\">CGImage</span>: <span class=\"keyword\">self</span>.context.createCGImage(outputImage, fromRect: outputImage.extent()))</div></pre></td></tr></table></figure>\n<h4 id=\"在Accelerate中的vImage\"><a href=\"#在Accelerate中的vImage\" class=\"headerlink\" title=\"在Accelerate中的vImage\"></a>在Accelerate中的vImage</h4><p>使用Accelerat framework包括<code>vImage</code>的图像处理函数.</p>\n<figure class=\"highlight objc\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div></pre></td><td class=\"code\"><pre><div class=\"line\">let cgImage = <span class=\"built_in\">UIImage</span>(contentsOfFile: <span class=\"keyword\">self</span>.URL.absoluteString!).CGImage</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// create a source buffer</span></div><div class=\"line\">var format = vImage_CGImageFormat(bitsPerComponent: <span class=\"number\">8</span>, bitsPerPixel: <span class=\"number\">32</span>, colorSpace: <span class=\"literal\">nil</span>, </div><div class=\"line\">    bitmapInfo: <span class=\"built_in\">CGBitmapInfo</span>(rawValue: <span class=\"built_in\">CGImageAlphaInfo</span>.First.rawValue), </div><div class=\"line\">    version: <span class=\"number\">0</span>, decode: <span class=\"literal\">nil</span>, renderingIntent: <span class=\"built_in\">CGColorRenderingIntent</span>.RenderingIntentDefault)</div><div class=\"line\">var sourceBuffer = vImage_Buffer()</div><div class=\"line\">defer &#123;</div><div class=\"line\">    sourceBuffer.data.dealloc(Int(sourceBuffer.height) * Int(sourceBuffer.height) * <span class=\"number\">4</span>)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">var error = vImageBuffer_InitWithCGImage(&amp;sourceBuffer, &amp;format, <span class=\"literal\">nil</span>, cgImage, numericCast(kvImageNoFlags))</div><div class=\"line\">guard error == kvImageNoError <span class=\"keyword\">else</span> &#123; <span class=\"keyword\">return</span> <span class=\"literal\">nil</span> &#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// create a destination buffer</span></div><div class=\"line\">let scale = <span class=\"built_in\">UIScreen</span>.mainScreen().scale</div><div class=\"line\">let destWidth = Int(image.size.width * <span class=\"number\">0.5</span> * scale)</div><div class=\"line\">let destHeight = Int(image.size.height * <span class=\"number\">0.5</span> * scale)</div><div class=\"line\">let bytesPerPixel = <span class=\"built_in\">CGImageGetBitsPerPixel</span>(image.CGImage) / <span class=\"number\">8</span></div><div class=\"line\">let destBytesPerRow = destWidth * bytesPerPixel</div><div class=\"line\">let destData = UnsafeMutablePointer&lt;<span class=\"built_in\">UInt8</span>&gt;.alloc(destHeight * destBytesPerRow)</div><div class=\"line\">defer &#123;</div><div class=\"line\">    destData.dealloc(destHeight * destBytesPerRow)</div><div class=\"line\">&#125;</div><div class=\"line\">var destBuffer = vImage_Buffer(data: destData, height: vImagePixelCount(destHeight), width: vImagePixelCount(destWidth), rowBytes: destBytesPerRow)</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// scale the image</span></div><div class=\"line\">error = vImageScale_ARGB8888(&amp;sourceBuffer, &amp;destBuffer, <span class=\"literal\">nil</span>, numericCast(kvImageHighQualityResampling))</div><div class=\"line\">guard error == kvImageNoError <span class=\"keyword\">else</span> &#123; <span class=\"keyword\">return</span> <span class=\"literal\">nil</span> &#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// create a CGImage from vImage_Buffer</span></div><div class=\"line\">let destCGImage = vImageCreateCGImageFromBuffer(&amp;destBuffer, &amp;format, <span class=\"literal\">nil</span>, <span class=\"literal\">nil</span>, numericCast(kvImageNoFlags), &amp;error)?.takeRetainedValue()</div><div class=\"line\">guard error == kvImageNoError <span class=\"keyword\">else</span> &#123; <span class=\"keyword\">return</span> <span class=\"literal\">nil</span> &#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// create a UIImage</span></div><div class=\"line\">let scaledImage = destCGImage.flatMap &#123; <span class=\"built_in\">UIImage</span>(<span class=\"built_in\">CGImage</span>: $<span class=\"number\">0</span>, scale: <span class=\"number\">0.0</span>, orientation: image.imageOrientation) &#125;</div></pre></td></tr></table></figure>\n<hr>\n<h3 id=\"各种方式的选择\"><a href=\"#各种方式的选择\" class=\"headerlink\" title=\"各种方式的选择\"></a>各种方式的选择</h3><ul>\n<li>UIKit,CoreGraphics以及Image I/O的性能优秀.如果仅仅是缩放的话最好使用CoreImage.</li>\n<li>日常的图像缩放以后不进行其他操作的话,使用<code>UIGraphicsBeginImageContextWithOptions</code>是最好的选择.</li>\n<li>如果对图像质量有更高的要求,最好使用<code>CGBitmapContextCreate</code>和<code>CGContextSetInterpolationQuality</code>.</li>\n<li>如果缩放的目的是显示缩略图,那么最好使用<code>CGImageSourceCreateThumbnailAtIndex</code></li>\n<li>缩放不要用<code>vImage</code></li>\n</ul>\n<blockquote>\n<p>参考文档: <a href=\"http://nshipster.com/image-resizing/\" target=\"_blank\" rel=\"external\">Image Resizing Techniques</a></p>\n</blockquote>\n","excerpt":"","more":"<p>iOS中图片相关的内容非常多,iOS中的图片相关的API也非常多,从UIKit中的UIImage,到CoreGraphic中CGImage,CoreImage中的CIImage.</p>\n<p>通常情况下使用UIImage来缩放图片大小,在UIImage有<code>contentMode</code>属性,会有多个枚举属性<code>.ScaleAspectFit</code>,<code>.ScaleAspectFill</code>,通过赖进行</p>\n<h3 id=\"使图片缩放改变大小\"><a href=\"#使图片缩放改变大小\" class=\"headerlink\" title=\"使图片缩放改变大小\"></a>使图片缩放改变大小</h3><p>在进行图片的缩放的大小时,首先需要了解到缩放的目标大小.</p>\n<h4 id=\"直接通过缩放因子改变大小\"><a href=\"#直接通过缩放因子改变大小\" class=\"headerlink\" title=\"直接通过缩放因子改变大小\"></a>直接通过缩放因子改变大小</h4><p>最简单的方式就是使用常量factor,去乘以图像的原来大小.</p>\n<figure class=\"highlight objc\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">let size = <span class=\"built_in\">CGSize</span>(width: image.size.width/<span class=\"number\">2</span>, height: image.size.height/<span class=\"number\">2</span>)</div></pre></td></tr></table></figure>\n<p>或者直接使用<code>CGAffineTransform</code>改变transform</p>\n<figure class=\"highlight objc\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">let size = <span class=\"built_in\">CGSizeApplyAffineTransform</span>(image.size, <span class=\"built_in\">CGAffineTransformScale</span>(<span class=\"number\">0.5</span>,<span class=\"number\">0.5</span>))</div></pre></td></tr></table></figure>\n<h4 id=\"不改变Aspect-Ratio的缩放\"><a href=\"#不改变Aspect-Ratio的缩放\" class=\"headerlink\" title=\"不改变Aspect Ratio的缩放\"></a>不改变Aspect Ratio的缩放</h4><p>如果要将一个图片放到一个rect中,而不改变它原始的aspect ratio.可以使用<code>AVFoundation</code>的<code>AVMakeRectWithAspectRatioInsideRect</code>方法.</p>\n<figure class=\"highlight objc\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">import <span class=\"built_in\">AVFoundation</span></div><div class=\"line\">let rect = <span class=\"built_in\">AVMakeRectWithAspectRatioInsideRect</span>(image.size, imageView.bounds)</div></pre></td></tr></table></figure>\n<hr>\n<h3 id=\"改变图片大小的方法汇总\"><a href=\"#改变图片大小的方法汇总\" class=\"headerlink\" title=\"改变图片大小的方法汇总\"></a>改变图片大小的方法汇总</h3><p>有多种方法可以改变图片的大小,每种方法的性能不同.</p>\n<h4 id=\"UIGraphicsBeginImageContextWithOptions和UIImage-drawInRect\"><a href=\"#UIGraphicsBeginImageContextWithOptions和UIImage-drawInRect\" class=\"headerlink\" title=\"UIGraphicsBeginImageContextWithOptions和UIImage -drawInRect:\"></a><code>UIGraphicsBeginImageContextWithOptions</code>和<code>UIImage -drawInRect:</code></h4><p>通常最高阶的API是UIKit,使用UIImage然后使用Graphic Context实时绘出一个大小不同的图片.可以使用<code>UIGraphicsBeginImageContextWithOptions</code>和<code>UIGraphicsGetImageFromCurrentImageContext</code>获取缩略图.</p>\n<figure class=\"highlight objc\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">let image = <span class=\"built_in\">UIImage</span>(contentsOfFile: <span class=\"keyword\">self</span>.URL.absoluteString!)</div><div class=\"line\"></div><div class=\"line\">let size = <span class=\"built_in\">CGSizeApplyAffineTransform</span>(image.size, <span class=\"built_in\">CGAffineTransformMakeScale</span>(<span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>))</div><div class=\"line\">let hasAlpha = <span class=\"literal\">false</span></div><div class=\"line\">let scale: <span class=\"built_in\">CGFloat</span> = <span class=\"number\">0.0</span> <span class=\"comment\">// Automatically use scale factor of main screen</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"built_in\">UIGraphicsBeginImageContextWithOptions</span>(size, !hasAlpha, scale)</div><div class=\"line\">image.drawInRect(<span class=\"built_in\">CGRect</span>(origin: <span class=\"built_in\">CGPointZero</span>, size: size))</div><div class=\"line\"></div><div class=\"line\">let scaledImage = <span class=\"built_in\">UIGraphicsGetImageFromCurrentImageContext</span>()</div><div class=\"line\"><span class=\"built_in\">UIGraphicsEndImageContext</span>()</div></pre></td></tr></table></figure>\n<p><code>UIGraphicsBeginImageContextWithOptions()</code>会创建一个渲染context,然后使用原来的图片draw.</p>\n<ul>\n<li>第一个参数<code>size</code>就是缩小以后的图片的大小.</li>\n<li>第二个参数<code>isOpaque</code>,用来描述图片的alpha通道是否渲染.设置为<code>false</code>表示完全不透明</li>\n<li>第三个参数<code>scale</code>,就是dispaly scale factor.如果设置成<code>0.0</code>那么就是main screen使用的(retina是2.0,iphone6p是3.0)</li>\n</ul>\n<h4 id=\"CGBitmapContextCreate和CGContextDrawImage\"><a href=\"#CGBitmapContextCreate和CGContextDrawImage\" class=\"headerlink\" title=\"CGBitmapContextCreate和CGContextDrawImage\"></a><code>CGBitmapContextCreate</code>和<code>CGContextDrawImage</code></h4><p>使用的Core Graphic/Quartz 2D使用的是低阶API,可以使用CGImage,使用<code>CGBitmapContextCreate()</code>和<code>CGBitmapContextCreateImage()</code>获取缩略图.</p>\n<figure class=\"highlight objc\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\">let cgImage = <span class=\"built_in\">UIImage</span>(contentsOfFile: <span class=\"keyword\">self</span>.URL.absoluteString!).CGImage</div><div class=\"line\"></div><div class=\"line\">let width = <span class=\"built_in\">CGImageGetWidth</span>(cgImage) / <span class=\"number\">2</span></div><div class=\"line\">let height = <span class=\"built_in\">CGImageGetHeight</span>(cgImage) / <span class=\"number\">2</span></div><div class=\"line\">let bitsPerComponent = <span class=\"built_in\">CGImageGetBitsPerComponent</span>(cgImage)</div><div class=\"line\">let bytesPerRow = <span class=\"built_in\">CGImageGetBytesPerRow</span>(cgImage)</div><div class=\"line\">let colorSpace = <span class=\"built_in\">CGImageGetColorSpace</span>(cgImage)</div><div class=\"line\">let bitmapInfo = <span class=\"built_in\">CGImageGetBitmapInfo</span>(cgImage)</div><div class=\"line\">let context = <span class=\"built_in\">CGBitmapContextCreate</span>(<span class=\"literal\">nil</span>, width, height, bitsPerComponent, bytesPerRow, colorSpace, bitmapInfo.rawValue)</div><div class=\"line\"></div><div class=\"line\"><span class=\"built_in\">CGContextSetInterpolationQuality</span>(context, kCGInterpolationHigh)</div><div class=\"line\"><span class=\"built_in\">CGContextDrawImage</span>(context, <span class=\"built_in\">CGRect</span>(origin: <span class=\"built_in\">CGPointZero</span>, size: <span class=\"built_in\">CGSize</span>(width: <span class=\"built_in\">CGFloat</span>(width), height: <span class=\"built_in\">CGFloat</span>(height))), cgImage)</div><div class=\"line\">let scaledImage = <span class=\"built_in\">CGBitmapContextCreateImage</span>(context).flatMap &#123; <span class=\"built_in\">UIImage</span>(<span class=\"built_in\">CGImage</span>: $<span class=\"number\">0</span>) &#125;</div></pre></td></tr></table></figure>\n<h4 id=\"CGImageSourceCreateThumbnailAtIndex\"><a href=\"#CGImageSourceCreateThumbnailAtIndex\" class=\"headerlink\" title=\"CGImageSourceCreateThumbnailAtIndex\"></a>CGImageSourceCreateThumbnailAtIndex</h4><p>也可以使用Image I/O的framework也可以用来缩放图片大小.</p>\n<figure class=\"highlight objc\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">import ImageIO</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">if</span> let imageSource = <span class=\"built_in\">CGImageSourceCreateWithURL</span>(<span class=\"keyword\">self</span>.URL, <span class=\"literal\">nil</span>) &#123;</div><div class=\"line\">    let options: [<span class=\"built_in\">NSString</span>: <span class=\"built_in\">NSObject</span>] = [</div><div class=\"line\">        kCGImageSourceThumbnailMaxPixelSize: max(size.width, size.height) / <span class=\"number\">2.0</span>,</div><div class=\"line\">        kCGImageSourceCreateThumbnailFromImageAlways: <span class=\"literal\">true</span></div><div class=\"line\">    ]</div><div class=\"line\"></div><div class=\"line\">    let scaledImage = <span class=\"built_in\">CGImageSourceCreateThumbnailAtIndex</span>(imageSource, <span class=\"number\">0</span>, options).flatMap &#123; <span class=\"built_in\">UIImage</span>(<span class=\"built_in\">CGImage</span>: $<span class=\"number\">0</span>) &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h4 id=\"使用CoreImage进行Lanczos重新采样\"><a href=\"#使用CoreImage进行Lanczos重新采样\" class=\"headerlink\" title=\"使用CoreImage进行Lanczos重新采样\"></a>使用CoreImage进行Lanczos重新采样</h4><p>CoreImage中内置Lanczos Resampling,具体的函数是<code>CILanczosScaleTransform</code> filter.</p>\n<figure class=\"highlight objc\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">let image = <span class=\"built_in\">CIImage</span>(contentsOfURL: <span class=\"keyword\">self</span>.URL)</div><div class=\"line\"></div><div class=\"line\">let filter = <span class=\"built_in\">CIFilter</span>(name: <span class=\"string\">\"CILanczosScaleTransform\"</span>)!</div><div class=\"line\">filter.setValue(image, forKey: <span class=\"string\">\"inputImage\"</span>)</div><div class=\"line\">filter.setValue(<span class=\"number\">0.5</span>, forKey: <span class=\"string\">\"inputScale\"</span>)</div><div class=\"line\">filter.setValue(<span class=\"number\">1.0</span>, forKey: <span class=\"string\">\"inputAspectRatio\"</span>)</div><div class=\"line\">let outputImage = filter.valueForKey(<span class=\"string\">\"outputImage\"</span>) as! <span class=\"built_in\">CIImage</span></div><div class=\"line\"></div><div class=\"line\">let context = <span class=\"built_in\">CIContext</span>(options: [kCIContextUseSoftwareRenderer: <span class=\"literal\">false</span>])</div><div class=\"line\">let scaledImage = <span class=\"built_in\">UIImage</span>(<span class=\"built_in\">CGImage</span>: <span class=\"keyword\">self</span>.context.createCGImage(outputImage, fromRect: outputImage.extent()))</div></pre></td></tr></table></figure>\n<h4 id=\"在Accelerate中的vImage\"><a href=\"#在Accelerate中的vImage\" class=\"headerlink\" title=\"在Accelerate中的vImage\"></a>在Accelerate中的vImage</h4><p>使用Accelerat framework包括<code>vImage</code>的图像处理函数.</p>\n<figure class=\"highlight objc\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div></pre></td><td class=\"code\"><pre><div class=\"line\">let cgImage = <span class=\"built_in\">UIImage</span>(contentsOfFile: <span class=\"keyword\">self</span>.URL.absoluteString!).CGImage</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// create a source buffer</span></div><div class=\"line\">var format = vImage_CGImageFormat(bitsPerComponent: <span class=\"number\">8</span>, bitsPerPixel: <span class=\"number\">32</span>, colorSpace: <span class=\"literal\">nil</span>, </div><div class=\"line\">    bitmapInfo: <span class=\"built_in\">CGBitmapInfo</span>(rawValue: <span class=\"built_in\">CGImageAlphaInfo</span>.First.rawValue), </div><div class=\"line\">    version: <span class=\"number\">0</span>, decode: <span class=\"literal\">nil</span>, renderingIntent: <span class=\"built_in\">CGColorRenderingIntent</span>.RenderingIntentDefault)</div><div class=\"line\">var sourceBuffer = vImage_Buffer()</div><div class=\"line\">defer &#123;</div><div class=\"line\">    sourceBuffer.data.dealloc(Int(sourceBuffer.height) * Int(sourceBuffer.height) * <span class=\"number\">4</span>)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">var error = vImageBuffer_InitWithCGImage(&amp;sourceBuffer, &amp;format, <span class=\"literal\">nil</span>, cgImage, numericCast(kvImageNoFlags))</div><div class=\"line\">guard error == kvImageNoError <span class=\"keyword\">else</span> &#123; <span class=\"keyword\">return</span> <span class=\"literal\">nil</span> &#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// create a destination buffer</span></div><div class=\"line\">let scale = <span class=\"built_in\">UIScreen</span>.mainScreen().scale</div><div class=\"line\">let destWidth = Int(image.size.width * <span class=\"number\">0.5</span> * scale)</div><div class=\"line\">let destHeight = Int(image.size.height * <span class=\"number\">0.5</span> * scale)</div><div class=\"line\">let bytesPerPixel = <span class=\"built_in\">CGImageGetBitsPerPixel</span>(image.CGImage) / <span class=\"number\">8</span></div><div class=\"line\">let destBytesPerRow = destWidth * bytesPerPixel</div><div class=\"line\">let destData = UnsafeMutablePointer&lt;<span class=\"built_in\">UInt8</span>&gt;.alloc(destHeight * destBytesPerRow)</div><div class=\"line\">defer &#123;</div><div class=\"line\">    destData.dealloc(destHeight * destBytesPerRow)</div><div class=\"line\">&#125;</div><div class=\"line\">var destBuffer = vImage_Buffer(data: destData, height: vImagePixelCount(destHeight), width: vImagePixelCount(destWidth), rowBytes: destBytesPerRow)</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// scale the image</span></div><div class=\"line\">error = vImageScale_ARGB8888(&amp;sourceBuffer, &amp;destBuffer, <span class=\"literal\">nil</span>, numericCast(kvImageHighQualityResampling))</div><div class=\"line\">guard error == kvImageNoError <span class=\"keyword\">else</span> &#123; <span class=\"keyword\">return</span> <span class=\"literal\">nil</span> &#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// create a CGImage from vImage_Buffer</span></div><div class=\"line\">let destCGImage = vImageCreateCGImageFromBuffer(&amp;destBuffer, &amp;format, <span class=\"literal\">nil</span>, <span class=\"literal\">nil</span>, numericCast(kvImageNoFlags), &amp;error)?.takeRetainedValue()</div><div class=\"line\">guard error == kvImageNoError <span class=\"keyword\">else</span> &#123; <span class=\"keyword\">return</span> <span class=\"literal\">nil</span> &#125;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// create a UIImage</span></div><div class=\"line\">let scaledImage = destCGImage.flatMap &#123; <span class=\"built_in\">UIImage</span>(<span class=\"built_in\">CGImage</span>: $<span class=\"number\">0</span>, scale: <span class=\"number\">0.0</span>, orientation: image.imageOrientation) &#125;</div></pre></td></tr></table></figure>\n<hr>\n<h3 id=\"各种方式的选择\"><a href=\"#各种方式的选择\" class=\"headerlink\" title=\"各种方式的选择\"></a>各种方式的选择</h3><ul>\n<li>UIKit,CoreGraphics以及Image I/O的性能优秀.如果仅仅是缩放的话最好使用CoreImage.</li>\n<li>日常的图像缩放以后不进行其他操作的话,使用<code>UIGraphicsBeginImageContextWithOptions</code>是最好的选择.</li>\n<li>如果对图像质量有更高的要求,最好使用<code>CGBitmapContextCreate</code>和<code>CGContextSetInterpolationQuality</code>.</li>\n<li>如果缩放的目的是显示缩略图,那么最好使用<code>CGImageSourceCreateThumbnailAtIndex</code></li>\n<li>缩放不要用<code>vImage</code></li>\n</ul>\n<blockquote>\n<p>参考文档: <a href=\"http://nshipster.com/image-resizing/\">Image Resizing Techniques</a></p>\n</blockquote>\n"},{"title":"RunLoop学习总结","date":"2016-07-26T13:17:16.000Z","_content":"\n通过前面几篇文章可以知道RunLoop实际上是一个事件处理的循环.只要一个线程启动了RunLoop,在它没有收到事件时候,它就会使得线程休眠,如果有事件,就调用相应的事件处理函数.\n\n### RunLoop与线程的关系\nRunLoop与线程是一一对应的.每个新建的线程(MovedainThread除外,主线程会默认启动)在默认状态下,它的RunLoop是没有开启的,必须手动调用`CurrentRunLoop`等方法才会开启RunLoop.通常iOS开发中,接触到的API分别是Cocoa的NSRunLoop和CoreFoundation的CFRunLoop.\n\n### RunLoop的组成\n\nRunLoop每次只能运行在一个mode中,它与mode是一对多的关系.RunLoopMode中包括不同类型的事件源,主要分成两大类:TimerSource, 各种异步输入InputSource.与此同时,RunLoop会在不同的声明周期给外界发送通知,告知观察者自己的状态.因此可以注册RunLoopObserver来观察RunLoop的状态.他们之间的关系如下:\n\n* 一个runLoop包含多个Mode,每个Mode内有多个个Source/Timer/Observer(底层会在RunLoopMode中用Array存储)\n* 每次启动RunLoop,需要指定一个Mode(`CFRunLoopRunSpecific`函数),如果需要切换Mode,只能退出RunLoop，再重新调用`CFRunLoopRunSpecific`,重新指定Mode\n* Source/Timer/Observer统称为mode item,每个item可以同时加入多个mode,但一个item被重复加入同一个mode无效\n* 如果一个mode中一个item都没有,RunLoop就会立即退出(AFNetworking2.x)\n\n#### 1. RunLoopMode\n\nRunLoop会运行在某个特定的mode下,称为RunLoopMode,具体mode可以使用系统的NSDefaultMode或者NSTrackingMode,也可以自己定义mode.如果想让事件在任何一个mode中处理,可以把事件源加入到CommonMode中.\n\n#### 2. TimerSource\n\nTimerSource在Cocoa框架中,一般是NSTimer.在使用它时,它会向RunLoop发送消息,RunLoop会根据Timer的类型去单次或者多次执行回调函数.在开始启动Timer时候,需要手动设定具体运行在哪一个RunLoopMode,默认情况下会运行在NSDefaultMode.\n\n通过RunLoop的源码发现,TimerSource的触发实际是通过Source1(MachPort)唤醒RunLoop,然后调用回调方法的.\n\n#### 3. 异步输入源InputSource\n\n异步事件其实会包括的类型:\n* source0: 非port事件,可以手动触发(内部只有一个callback函数指针)\n* source1: 基于MachPort的Source事件,大多是内核发出的,需要调用`mach_msg`方法\n\n#### 4. NSObject的selector源\n由NSObject 的 performSelector:wait:...等产生,下文会细讲.\n\n#### 5. RunLoopObserver\nRunLoop的事件源中,Timer是同步事件,另外的是异步事件Source0,Source1,selector. 而RunLoopObserver可以让你在某个特定的时期处理一些事情,比如在RunLoop马上进入休眠状态时,或者在刚刚被唤醒时等等.\n\n具体的RunLoop可以被观察的状态有:\n\n* RunLoop进入时候(kCFRunLoopEntry)\n* RunLoop将要处理Timer时(kCFRunLoopBeforeTimers)\n* RunLoop将要处理Source0时(kCFRunLoopBeforeSources)\n* RunLoop将要进入睡眠的时候(kCFRunLoopBeforeWaiting)\n* RunLoop将要被唤醒的时(kCFRunLoopAfterWaiting)\n* RunLoop停止的时候(kCFRunLoopExit)\n\n\n你可以在RunLoop中观察一个或者多个状态.观察者也可以设置是否只观察一次或者重复多次观察,如果只观察一次,那么调用回调函数以后,观察者就自动被移除了.\n\n### RunLoop具体循环逻辑\nRunLoop源码中关键的函数有以下几个:\n\n```\nCFRunLoopRunSpecific(xxxx)//入口&出口\nCFRunLoopRun(xxxx)//循环\n```\n\n具体的执行过程如下:\n\n```\n1 通知观察者runLoop已经启动(DoObserver)\n2 记录RunLoop启动事件,并设定RunLoop运行的超时条件\n\n**** RunLoop正式进入循环 *****\n\n3 通知观察者任何即将要开始的定时器(DoObserver)\n4 通知观察着任何即将启动的非基于端口的源(DoObserver)\n5 调用RunLoopMode的blocks链中的block方法(DoBlocks)\n6 启动任何准备好的非基于端口的source0源(DoSource0)\n7 调用RunLoopMode的blocks链中的block方法(DoBlocks)\n8 检查MachPort端口是否有消息要处理，如果有立即进入步骤12\n9 如果没有消息处理,通知观察者线程进入休眠(DoObserver)\n\n**** RunLoop休眠: zzz...****\n\n10 将线程至于休眠直到任意下面的事件发生(调用`mach_msg`使RunLoop休眠)\n\tA.某一时间到达基于端口的源\n\tB.定时器启动\n\tC.runLoop设置的时间已经超过\n\tD.runLoop被显示唤醒\n11 通知观察者线程将被唤醒(DoObserver)\n12 处理未处理的事件(有dispatchPort显示是哪个port的事件需要处理)\n\tA.如果livePort为NULL,啥都不做(会跳出循环)\n\tB.如果livePort是wakeUpPort,说明RunLoop运行超时(会跳出循环)\n\tC.如果livePort是timerPort,说明timerSource启动(DoTimers),进入步骤3\n\tD.如果livePort是dispatchPort,说明系统的libDispatch向主线程送消息,\n\t  会调dispatch_async(dispatch_get_main_queue(),block)\n\tE.如果livePort是其他port,通过mach_msg取出回调,处理事件(DoSource1),\n\t  进入步骤3\n\t\n**** RunLoop退出循环 ****\n\n13 通知观察者runLoop结束\n```\n\n从RunLoop的底层源码可以看出,RunLoop中的各种DoXXXX函数与最终的callout系统调用关系如下:\n\n* DoObserver\n`__CFRUNLOOP_IS_CALLING_OUT_TO_AN_OBSERVER_CALLBACK_FUNCTION__()`\n* DoBlocks \n`__CFRUNLOOP_IS_CALLING_OUT_TO_A_BLOCK__()`\n* DoSource0 `__CFRUNLOOP_IS_CALLING_OUT_TO_A_SOURCE0_PERFORM_FUNCTION__()`\n* DoTimers `__CFRUNLOOP_IS_CALLING_OUT_TO_A_TIMER_CALLBACK_FUNCTION__()`\n* dispatchPort `__CFRUNLOOP_IS_SERVICING_THE_MAIN_DISPATCH_QUEUE__()`\n* DoSource1 `__CFRUNLOOP_IS_CALLING_OUT_TO_A_SOURCE1_PERFORM_FUNCTION__()`\n\n>注意,以上callout调用,对于传入RunLoopMode中的Timers,Source0s,Blocks都是数组,因此对数组内的每个满足条件的成员都会进行callout调用,而source1和dispatchPort由于唤醒时每次只能执行一个.\n\n>repeat Timers执行的时间不一定是完全准确的,如果错过某次执行,只能等到下次RunLoop循环时候再执行了\n\n\n#### 何时使用RunLoop\n\n自己配置并启动它，你不需要再任何情况下都去启动一个线程的runLoop。runLoop在你\n要和线程有更多的交互时才需要，比如以下情况：\n\n* 使用端口或者自定义输入源来和其他线程通信；\n* 使用线程的NSTimer (可以用dispatch_timer代替)\n* Cocoa中使用任何performSelector\n* 让线程周期性干活(AFNetworking2.x)\n\n\n### CFRunLoop相关内容\n#### CoreFoundation中RunLoop的接口\n\n一般使用的是CoreFoundation中的CFRunLoop,因此这里主要总结CFRunLoop的对外接口.RunLoop中对外主要有以下几个接口类:\n\n* CFRunLoopRef\n* CFRunLoopModeRef (并未对外暴露)\n* CFRunLoopSourceRef: 异步的source0和source1事件源\n\t* source0: 结构体中只有perform回调函数,它并不能主动触发事件,使用时,需要手动触发(`CFRunLoopSourceSignal()`).CFRunLoopSourceSignal将这个source标记为待处理，后调用CFRunLoopWakeUp来唤醒runLoop，让其处理这个事件(也就是source0无法唤醒RunLoop)\n    * source1: 结构体中有mach_port和invoke回调函数,通常是内核和其他进程给该RunLoop发送的消息,从源码可以看出source1,是可以自动唤醒RunLoop的,因为它是通过port方式发送事件的.\n* CFRunLoopTimerRef: 同步的TimerSource, 在Cocoa中是NSTimer.它的底层结构体中包含触发时间和回调，当其加入到runLoop中是，runLoop会注册对应的时间点，当时\n间点到时,RunLoop会收到wakeUpPort(RunLoop的一个属性)source1类型的消息,将唤醒以执行Timer的回调；\n* CFRunLoopObserverRef: 注册观察者时候可以添加一个回调函数,当RunLoop的状态变化时会通知注册的观察者,注册的回调函数在此时调用\n\n> 具体的demo可以参考:\n> https://github.com/brownfeng/RunLoopDemo\n#### RunLoop底层结构体\n\nCFRunLoop的结构如下\n\n```\nstruct __CFRunLoop {\n    CFMutableSetRef _commonModes;     \n    CFMutableSetRef _commonModeItems; \n    CFRunLoopModeRef _currentMode;    \n    CFMutableSetRef _modes;           \n    ...\n};\n```\n\nCFRunLoopMode的结构如下：\n\n```\nstruct __CFRunLoopMode {\n    CFStringRef _name;            \n    CFMutableSetRef _sources0;    \n    CFMutableSetRef _sources1;    \n    CFMutableArrayRef _observers; \n    CFMutableArrayRef _timers;    \n    ...\n};\n```\n\n其中，CFRunLoop对外暴露的管理Mode的接口有两个:\n\n```\nCFRunLoopAddCommonMode\nCFRunLoopRunInMode\n```\n\nMode暴露的管理mode item的接口有下面几个：\n\n```\nCFRunLoopAddSource\nCFRunLoopAddObserver\nCFRunLoopAddTimer\nCFRunLoopRemoveSource\nCFRunLoopRemoveObserver\nCFRunLoopRemoveTimer\n```\n\n### Cocoa框架中RunLoop相关的内容\n\n#### 1 AutoReleasePool\n\nAutoReleasePool是Apple中清理临时变量,释放内容的机制,在app的main函数中,所有的内容都是包裹在一个AutoReleasePool中的.这个过程中,Apple在RunLoop中注册三个RunLoopObserver:\n\n* 第一个RunLoopObserver关注kCFRunLoopEntry状态,callback函数中会自动创建autoReleasePool,并且observer优先级最高(RunLoop第一个调用它的回调),保证应用启动后所有的操作都在autoreleasePool中运行.\n* 第二个RunLoopObserver关注kCFRunLoopBeforeWaiting状态,在此时autoreleasePool会释放旧的池,并创建一个新的autoreleasePool.\n* 第三个RunLoopObserver关注kCFRunLoopExit状态,此时释放autoreleasePool,并且保证observer的优先级最低,即在所有其他的observers都执行完以后才执行autorelease相关的observer的callback函数.\n\n> 深入学习可以参考:\n> http://blog.sunnyxx.com/2014/10/15/behind-autorelease/\n\n#### 2 iOS事件响应\n\nRunLoop解释了为何iOS应用能够接受到屏幕触摸等事件.Apple在iOS app启动时候在main RunLoop中注册一个Source1事件,是基于Mach Port(进程间通信)的系统层面的进程.\n\n当一个触摸事件发生以后,IOKit会生成一个IOHIDEvent事件,并由系统层面的SpringBoard接受按键(锁屏/静音),触摸,加速,接近传感器等几种Event,然后通过进程间通信的mach port发送给App的main RunLoop.然后在DoSource1中,source1的回调会触发,内部会调用系统`_UIApplicationHandleEventQueue()`进行事件分发.\n\n`_UIApplicationHandleEventQueue()`方法会把IOHIDEvent处理,并包装成常见的UIEvent事件进行处理或分发,其中包括识别UIGesture/处理屏幕旋转/发送给UIWindow等.通常事件比如UIButton点击,touchesBegin/Move/End/Cancel等事件都是在这个Source1回调中完成.\n\n#### 3 手势识别GestureRecognizer\n在上面的事件响应中,回调方法`_UIApplicationHandleEventQueue()`识别了一个手势以后,首先会调用Cancel将当前的touchesBegin/Move/End系列的回调打断.随后系统会将UIGestureRecognizer标记为等待处理.\n\n然后苹果会在RunLoop中注册一个Observer观察kCFRunLoopBeforeWaiting(睡眠前).这个观察者的回调函数是`_UIGestureRecognizerUpdateObserver()`,它会获取刚才所有被标记为等待处理等的GestureRecognizer,并且执行GestureRecognizer的回调.\n\n#### 4 界面更新(UI update)\n在App改变UI时,例如修改view的Frame,更新UIView/CALayer的层次,或者手动调用UIView/CALayer的setNeedsLayout/setNeedsDisplay方法以后,这个UIView/CALayer就被标记为等待处理,并且提交到一个全局容器.\n\n然后苹果在RunLoop中注册一个Observer观察kCFRunLoopBeforeWaiting和kCFRunLoopExit状态.在状态触发DoObserver的callback中会调用函数`_ZN2CA11Transaction17observer_callbackEP19__CFRunLoopObservermPv()`.这个函数中会遍历所有的等待处理的UIView/CALayer,执行绘制和调整,更新UI界面.\n\n这个函数的调用栈如下:\n\n```\n_ZN2CA11Transaction17observer_callbackEP19__CFRunLoopObservermPv()\n    QuartzCore:CA::Transaction::observer_callback://DoObserver\n        CA::Transaction::commit();//提交到全局容器\n            CA::Context::commit_transaction();\n                CA::Layer::layout_and_display_if_needed();\n                    CA::Layer::layout_if_needed();\n                        [CALayer layoutSublayers];\n                            [UIView layoutSubviews];\n                    CA::Layer::display_if_needed();\n                        [CALayer display];\n                            [UIView drawRect];\n\n```\n\n#### 5 NSTimer定时器\n\nNSTimer的CF层是CFRunLoopTimerRef,它们可以toll-free bridged.当使用`CFRunLoopAddTimer()`将NSTimer&CFRunLoopTimerRef注册到RunLoop中以后,RunLoop会为它重复的时间点注册好事件(有一定的时间容忍度,触发的时间误差).如果中间某次触发被错过,那么这次触发时间点的回调也会被跳过去. 实际中NSTimer的触发是通过source1触发的,如果timer被触发会将RunLoop的有一个Mach Port-`timePort`-发消息,然后会唤醒RunLoop,调用DoTimers.可以参考上面的RunLoop运行流程.\n\n> CADisplayLink 是一个和屏幕刷新一致的定时器,如果两次屏幕刷新时候在执行一个长时间任务,那其中就会有一帧被跳过,造成界面卡顿.Facebook的AsyncDisplayLink使用RunLoop来解决丢帧的问题.\n> \n> 还有一个GCD的定时器`dispatch_timer`,RunLoop的超时机制也是使用`dispatch_timer`.\n\n#### 6 部分PerformSelector事件源\n\nNSObject的部分与时间相关的PerformSelector方法与RunLoop密切相关.当调用NSObject的performSelector:afterDelay以后,实际内部会创建一个Timer并添加RunLoopMode中,等待Timer触发,然后调用DoTimers,其中包含的回调方法就是selector.因此如果当前线程没有开启RunLoop,这个方法无效.类似的NSObject的performSelector:onThread:也是类似,需要线程开启RunLoop.具体涉及到的perform方法包括:\n\n```\n- performSelector:afterDelay:\n– performSelector:withObject:afterDelay:\n– performSelectorOnMainThread:withObject:waitUntilDone:\n– performSelectorOnMainThread:withObject:waitUntilDone:modes:\n– performSelector:onThread:withObject:waitUntilDone:modes:\n- performSelector:onThread:withObject:waitUntilDone:\n```\n\n#### 7 GCD与RunLoop\n\nGCD的中部分接口使用了RunLoop,提交到mainQueue的blocks.当调用`dispatch_async(dispatch_get_main_queue(), block)`时,libDispatch(系统的某个进程)会向app的进程的main runloop的dispatchPort(是一个mach port)发送消息.此时RunLoop如果在休眠状态,会被唤醒,并从dispatch port中取出消息,并在会在回调方法`__CFRUNLOOP_IS_SERVICING_THE_MAIN_DISPATCH_QUEUE__()`中执行这个block.\n\n> libDispatch分发到main queue的block才会与RunLoop有关,分发到其他的子线程的内容,还是又libDispatch处理.\n> \n> RunLoop的运行超时是由GCD的dispatch_timer控制的.\n\n#### 8 关于网络请求\n\n关于网络请求的接口，主要有以下几层:\n\n* CFSocket:是最底层的接口，只负责socket的通信\n* CFNetwork:是基于CFSocket等接口的上层封装,ASIHttpRequest工作在这层\n* NSURLConnection:是基于CFNetwork的更高层的封装，提供面向对象的接口，\nAFNetworking2.x工作于这一层\n* NSURLSession:是ios7中新增的接口,表面上和NSURLConnection并列,但底层\n仍然用到NSURLConnection的部分功能,AFNetworking3.x和Alamofire在这层\n\n因此,现在使用的大多网络库都与CFSocket,CFNetwork层相关.\n\n通常使用NSURLConnection时,你会传入一个Delegate,当调用[connection start]后，这个Delegate就会不停收到事件回调。实际上,start 这个函数的内部会\n会获取 CurrentRunLoop，然后在其中的 DefaultMode 添加了4个 Source0 (即需要手动\n触发的Source)。CFMultiplexerSource 是负责各种 Delegate 回调的，CFHTTPCookie\nStorage 是处理各种 Cookie 的。\n\n>NSURLConnection的工作过程可以参考:\nhttp://blog.ibireme.com/2015/05/18/runloop/#base\n\n### RunLoop的应用举例\n#### 1 AFNetworking2.x\n\n使用NSURLConnection时候,需要在后台自定义线程中接受Delegate回调.AFNetworking创建一个自定义线程,然后在其中添加一个Mach Port,然后启动RunLoop(前面提到过,如果RunLoop中没有source/timer/observer会立即退出).在以后线程以后通过NSObject的`performSelector:onThread:withObject:waitUnitlDone:modes:`方法,将`[self.connection scheduleInRunLoop:runLoop forMode:runLoopMode]`.\n\n>可以参考:\n>https://github.com/brownfeng/SourceSet/tree/master/AFNetworking2.x\n\n#### 2 AsyncDisplayKit\nAsyncDisplayKit是Facebook推出的用于保持界面流畅性的框架,其原理大致如下：\n\nUI线程中一旦出现繁重的任务就会导致界面卡顿,这类任务通常分为3类:排版,绘制,UI对象操作。通过各种方法将前两种任务丢到后台运行,最后一类操作只能在主线程中执行,因此,ASDK仿照QuartzCore/UIKit框架的模式,实现了一套类似的界面更新的机制:即在主线程的RunLoop中添加一个 Observer,监听kCFRunLoopBeforeWaiting和 kCFRunLoopExit事件,在收到回调时,遍历所有之前放入队列的待处理的任务,然后一一执行。\n\n#### 3 NSTimer在TrackingMode运行\n#### 4 TableView延迟加载图片\n#### 5 RunLoop解决大图加载的问题\n#### 6 RunLoop监控App卡顿\n\n>可以参考文章:\n>\n>http://www.jianshu.com/p/929d855c5a5a\n>http://www.jianshu.com/p/924cb2b218f5\n\n### 参考内容\nhttp://blog.ibireme.com/2015/05/18/runloop/\n\nhttp://yun.baidu.com/share/link?shareid=2268593032&uk=2885973690","source":"_posts/RunLoop总结.md","raw":"---\ntitle: RunLoop学习总结\ndate: 2016-07-26 21:17:16\ntags:\n - runloop\n - iOS\n---\n\n通过前面几篇文章可以知道RunLoop实际上是一个事件处理的循环.只要一个线程启动了RunLoop,在它没有收到事件时候,它就会使得线程休眠,如果有事件,就调用相应的事件处理函数.\n\n### RunLoop与线程的关系\nRunLoop与线程是一一对应的.每个新建的线程(MovedainThread除外,主线程会默认启动)在默认状态下,它的RunLoop是没有开启的,必须手动调用`CurrentRunLoop`等方法才会开启RunLoop.通常iOS开发中,接触到的API分别是Cocoa的NSRunLoop和CoreFoundation的CFRunLoop.\n\n### RunLoop的组成\n\nRunLoop每次只能运行在一个mode中,它与mode是一对多的关系.RunLoopMode中包括不同类型的事件源,主要分成两大类:TimerSource, 各种异步输入InputSource.与此同时,RunLoop会在不同的声明周期给外界发送通知,告知观察者自己的状态.因此可以注册RunLoopObserver来观察RunLoop的状态.他们之间的关系如下:\n\n* 一个runLoop包含多个Mode,每个Mode内有多个个Source/Timer/Observer(底层会在RunLoopMode中用Array存储)\n* 每次启动RunLoop,需要指定一个Mode(`CFRunLoopRunSpecific`函数),如果需要切换Mode,只能退出RunLoop，再重新调用`CFRunLoopRunSpecific`,重新指定Mode\n* Source/Timer/Observer统称为mode item,每个item可以同时加入多个mode,但一个item被重复加入同一个mode无效\n* 如果一个mode中一个item都没有,RunLoop就会立即退出(AFNetworking2.x)\n\n#### 1. RunLoopMode\n\nRunLoop会运行在某个特定的mode下,称为RunLoopMode,具体mode可以使用系统的NSDefaultMode或者NSTrackingMode,也可以自己定义mode.如果想让事件在任何一个mode中处理,可以把事件源加入到CommonMode中.\n\n#### 2. TimerSource\n\nTimerSource在Cocoa框架中,一般是NSTimer.在使用它时,它会向RunLoop发送消息,RunLoop会根据Timer的类型去单次或者多次执行回调函数.在开始启动Timer时候,需要手动设定具体运行在哪一个RunLoopMode,默认情况下会运行在NSDefaultMode.\n\n通过RunLoop的源码发现,TimerSource的触发实际是通过Source1(MachPort)唤醒RunLoop,然后调用回调方法的.\n\n#### 3. 异步输入源InputSource\n\n异步事件其实会包括的类型:\n* source0: 非port事件,可以手动触发(内部只有一个callback函数指针)\n* source1: 基于MachPort的Source事件,大多是内核发出的,需要调用`mach_msg`方法\n\n#### 4. NSObject的selector源\n由NSObject 的 performSelector:wait:...等产生,下文会细讲.\n\n#### 5. RunLoopObserver\nRunLoop的事件源中,Timer是同步事件,另外的是异步事件Source0,Source1,selector. 而RunLoopObserver可以让你在某个特定的时期处理一些事情,比如在RunLoop马上进入休眠状态时,或者在刚刚被唤醒时等等.\n\n具体的RunLoop可以被观察的状态有:\n\n* RunLoop进入时候(kCFRunLoopEntry)\n* RunLoop将要处理Timer时(kCFRunLoopBeforeTimers)\n* RunLoop将要处理Source0时(kCFRunLoopBeforeSources)\n* RunLoop将要进入睡眠的时候(kCFRunLoopBeforeWaiting)\n* RunLoop将要被唤醒的时(kCFRunLoopAfterWaiting)\n* RunLoop停止的时候(kCFRunLoopExit)\n\n\n你可以在RunLoop中观察一个或者多个状态.观察者也可以设置是否只观察一次或者重复多次观察,如果只观察一次,那么调用回调函数以后,观察者就自动被移除了.\n\n### RunLoop具体循环逻辑\nRunLoop源码中关键的函数有以下几个:\n\n```\nCFRunLoopRunSpecific(xxxx)//入口&出口\nCFRunLoopRun(xxxx)//循环\n```\n\n具体的执行过程如下:\n\n```\n1 通知观察者runLoop已经启动(DoObserver)\n2 记录RunLoop启动事件,并设定RunLoop运行的超时条件\n\n**** RunLoop正式进入循环 *****\n\n3 通知观察者任何即将要开始的定时器(DoObserver)\n4 通知观察着任何即将启动的非基于端口的源(DoObserver)\n5 调用RunLoopMode的blocks链中的block方法(DoBlocks)\n6 启动任何准备好的非基于端口的source0源(DoSource0)\n7 调用RunLoopMode的blocks链中的block方法(DoBlocks)\n8 检查MachPort端口是否有消息要处理，如果有立即进入步骤12\n9 如果没有消息处理,通知观察者线程进入休眠(DoObserver)\n\n**** RunLoop休眠: zzz...****\n\n10 将线程至于休眠直到任意下面的事件发生(调用`mach_msg`使RunLoop休眠)\n\tA.某一时间到达基于端口的源\n\tB.定时器启动\n\tC.runLoop设置的时间已经超过\n\tD.runLoop被显示唤醒\n11 通知观察者线程将被唤醒(DoObserver)\n12 处理未处理的事件(有dispatchPort显示是哪个port的事件需要处理)\n\tA.如果livePort为NULL,啥都不做(会跳出循环)\n\tB.如果livePort是wakeUpPort,说明RunLoop运行超时(会跳出循环)\n\tC.如果livePort是timerPort,说明timerSource启动(DoTimers),进入步骤3\n\tD.如果livePort是dispatchPort,说明系统的libDispatch向主线程送消息,\n\t  会调dispatch_async(dispatch_get_main_queue(),block)\n\tE.如果livePort是其他port,通过mach_msg取出回调,处理事件(DoSource1),\n\t  进入步骤3\n\t\n**** RunLoop退出循环 ****\n\n13 通知观察者runLoop结束\n```\n\n从RunLoop的底层源码可以看出,RunLoop中的各种DoXXXX函数与最终的callout系统调用关系如下:\n\n* DoObserver\n`__CFRUNLOOP_IS_CALLING_OUT_TO_AN_OBSERVER_CALLBACK_FUNCTION__()`\n* DoBlocks \n`__CFRUNLOOP_IS_CALLING_OUT_TO_A_BLOCK__()`\n* DoSource0 `__CFRUNLOOP_IS_CALLING_OUT_TO_A_SOURCE0_PERFORM_FUNCTION__()`\n* DoTimers `__CFRUNLOOP_IS_CALLING_OUT_TO_A_TIMER_CALLBACK_FUNCTION__()`\n* dispatchPort `__CFRUNLOOP_IS_SERVICING_THE_MAIN_DISPATCH_QUEUE__()`\n* DoSource1 `__CFRUNLOOP_IS_CALLING_OUT_TO_A_SOURCE1_PERFORM_FUNCTION__()`\n\n>注意,以上callout调用,对于传入RunLoopMode中的Timers,Source0s,Blocks都是数组,因此对数组内的每个满足条件的成员都会进行callout调用,而source1和dispatchPort由于唤醒时每次只能执行一个.\n\n>repeat Timers执行的时间不一定是完全准确的,如果错过某次执行,只能等到下次RunLoop循环时候再执行了\n\n\n#### 何时使用RunLoop\n\n自己配置并启动它，你不需要再任何情况下都去启动一个线程的runLoop。runLoop在你\n要和线程有更多的交互时才需要，比如以下情况：\n\n* 使用端口或者自定义输入源来和其他线程通信；\n* 使用线程的NSTimer (可以用dispatch_timer代替)\n* Cocoa中使用任何performSelector\n* 让线程周期性干活(AFNetworking2.x)\n\n\n### CFRunLoop相关内容\n#### CoreFoundation中RunLoop的接口\n\n一般使用的是CoreFoundation中的CFRunLoop,因此这里主要总结CFRunLoop的对外接口.RunLoop中对外主要有以下几个接口类:\n\n* CFRunLoopRef\n* CFRunLoopModeRef (并未对外暴露)\n* CFRunLoopSourceRef: 异步的source0和source1事件源\n\t* source0: 结构体中只有perform回调函数,它并不能主动触发事件,使用时,需要手动触发(`CFRunLoopSourceSignal()`).CFRunLoopSourceSignal将这个source标记为待处理，后调用CFRunLoopWakeUp来唤醒runLoop，让其处理这个事件(也就是source0无法唤醒RunLoop)\n    * source1: 结构体中有mach_port和invoke回调函数,通常是内核和其他进程给该RunLoop发送的消息,从源码可以看出source1,是可以自动唤醒RunLoop的,因为它是通过port方式发送事件的.\n* CFRunLoopTimerRef: 同步的TimerSource, 在Cocoa中是NSTimer.它的底层结构体中包含触发时间和回调，当其加入到runLoop中是，runLoop会注册对应的时间点，当时\n间点到时,RunLoop会收到wakeUpPort(RunLoop的一个属性)source1类型的消息,将唤醒以执行Timer的回调；\n* CFRunLoopObserverRef: 注册观察者时候可以添加一个回调函数,当RunLoop的状态变化时会通知注册的观察者,注册的回调函数在此时调用\n\n> 具体的demo可以参考:\n> https://github.com/brownfeng/RunLoopDemo\n#### RunLoop底层结构体\n\nCFRunLoop的结构如下\n\n```\nstruct __CFRunLoop {\n    CFMutableSetRef _commonModes;     \n    CFMutableSetRef _commonModeItems; \n    CFRunLoopModeRef _currentMode;    \n    CFMutableSetRef _modes;           \n    ...\n};\n```\n\nCFRunLoopMode的结构如下：\n\n```\nstruct __CFRunLoopMode {\n    CFStringRef _name;            \n    CFMutableSetRef _sources0;    \n    CFMutableSetRef _sources1;    \n    CFMutableArrayRef _observers; \n    CFMutableArrayRef _timers;    \n    ...\n};\n```\n\n其中，CFRunLoop对外暴露的管理Mode的接口有两个:\n\n```\nCFRunLoopAddCommonMode\nCFRunLoopRunInMode\n```\n\nMode暴露的管理mode item的接口有下面几个：\n\n```\nCFRunLoopAddSource\nCFRunLoopAddObserver\nCFRunLoopAddTimer\nCFRunLoopRemoveSource\nCFRunLoopRemoveObserver\nCFRunLoopRemoveTimer\n```\n\n### Cocoa框架中RunLoop相关的内容\n\n#### 1 AutoReleasePool\n\nAutoReleasePool是Apple中清理临时变量,释放内容的机制,在app的main函数中,所有的内容都是包裹在一个AutoReleasePool中的.这个过程中,Apple在RunLoop中注册三个RunLoopObserver:\n\n* 第一个RunLoopObserver关注kCFRunLoopEntry状态,callback函数中会自动创建autoReleasePool,并且observer优先级最高(RunLoop第一个调用它的回调),保证应用启动后所有的操作都在autoreleasePool中运行.\n* 第二个RunLoopObserver关注kCFRunLoopBeforeWaiting状态,在此时autoreleasePool会释放旧的池,并创建一个新的autoreleasePool.\n* 第三个RunLoopObserver关注kCFRunLoopExit状态,此时释放autoreleasePool,并且保证observer的优先级最低,即在所有其他的observers都执行完以后才执行autorelease相关的observer的callback函数.\n\n> 深入学习可以参考:\n> http://blog.sunnyxx.com/2014/10/15/behind-autorelease/\n\n#### 2 iOS事件响应\n\nRunLoop解释了为何iOS应用能够接受到屏幕触摸等事件.Apple在iOS app启动时候在main RunLoop中注册一个Source1事件,是基于Mach Port(进程间通信)的系统层面的进程.\n\n当一个触摸事件发生以后,IOKit会生成一个IOHIDEvent事件,并由系统层面的SpringBoard接受按键(锁屏/静音),触摸,加速,接近传感器等几种Event,然后通过进程间通信的mach port发送给App的main RunLoop.然后在DoSource1中,source1的回调会触发,内部会调用系统`_UIApplicationHandleEventQueue()`进行事件分发.\n\n`_UIApplicationHandleEventQueue()`方法会把IOHIDEvent处理,并包装成常见的UIEvent事件进行处理或分发,其中包括识别UIGesture/处理屏幕旋转/发送给UIWindow等.通常事件比如UIButton点击,touchesBegin/Move/End/Cancel等事件都是在这个Source1回调中完成.\n\n#### 3 手势识别GestureRecognizer\n在上面的事件响应中,回调方法`_UIApplicationHandleEventQueue()`识别了一个手势以后,首先会调用Cancel将当前的touchesBegin/Move/End系列的回调打断.随后系统会将UIGestureRecognizer标记为等待处理.\n\n然后苹果会在RunLoop中注册一个Observer观察kCFRunLoopBeforeWaiting(睡眠前).这个观察者的回调函数是`_UIGestureRecognizerUpdateObserver()`,它会获取刚才所有被标记为等待处理等的GestureRecognizer,并且执行GestureRecognizer的回调.\n\n#### 4 界面更新(UI update)\n在App改变UI时,例如修改view的Frame,更新UIView/CALayer的层次,或者手动调用UIView/CALayer的setNeedsLayout/setNeedsDisplay方法以后,这个UIView/CALayer就被标记为等待处理,并且提交到一个全局容器.\n\n然后苹果在RunLoop中注册一个Observer观察kCFRunLoopBeforeWaiting和kCFRunLoopExit状态.在状态触发DoObserver的callback中会调用函数`_ZN2CA11Transaction17observer_callbackEP19__CFRunLoopObservermPv()`.这个函数中会遍历所有的等待处理的UIView/CALayer,执行绘制和调整,更新UI界面.\n\n这个函数的调用栈如下:\n\n```\n_ZN2CA11Transaction17observer_callbackEP19__CFRunLoopObservermPv()\n    QuartzCore:CA::Transaction::observer_callback://DoObserver\n        CA::Transaction::commit();//提交到全局容器\n            CA::Context::commit_transaction();\n                CA::Layer::layout_and_display_if_needed();\n                    CA::Layer::layout_if_needed();\n                        [CALayer layoutSublayers];\n                            [UIView layoutSubviews];\n                    CA::Layer::display_if_needed();\n                        [CALayer display];\n                            [UIView drawRect];\n\n```\n\n#### 5 NSTimer定时器\n\nNSTimer的CF层是CFRunLoopTimerRef,它们可以toll-free bridged.当使用`CFRunLoopAddTimer()`将NSTimer&CFRunLoopTimerRef注册到RunLoop中以后,RunLoop会为它重复的时间点注册好事件(有一定的时间容忍度,触发的时间误差).如果中间某次触发被错过,那么这次触发时间点的回调也会被跳过去. 实际中NSTimer的触发是通过source1触发的,如果timer被触发会将RunLoop的有一个Mach Port-`timePort`-发消息,然后会唤醒RunLoop,调用DoTimers.可以参考上面的RunLoop运行流程.\n\n> CADisplayLink 是一个和屏幕刷新一致的定时器,如果两次屏幕刷新时候在执行一个长时间任务,那其中就会有一帧被跳过,造成界面卡顿.Facebook的AsyncDisplayLink使用RunLoop来解决丢帧的问题.\n> \n> 还有一个GCD的定时器`dispatch_timer`,RunLoop的超时机制也是使用`dispatch_timer`.\n\n#### 6 部分PerformSelector事件源\n\nNSObject的部分与时间相关的PerformSelector方法与RunLoop密切相关.当调用NSObject的performSelector:afterDelay以后,实际内部会创建一个Timer并添加RunLoopMode中,等待Timer触发,然后调用DoTimers,其中包含的回调方法就是selector.因此如果当前线程没有开启RunLoop,这个方法无效.类似的NSObject的performSelector:onThread:也是类似,需要线程开启RunLoop.具体涉及到的perform方法包括:\n\n```\n- performSelector:afterDelay:\n– performSelector:withObject:afterDelay:\n– performSelectorOnMainThread:withObject:waitUntilDone:\n– performSelectorOnMainThread:withObject:waitUntilDone:modes:\n– performSelector:onThread:withObject:waitUntilDone:modes:\n- performSelector:onThread:withObject:waitUntilDone:\n```\n\n#### 7 GCD与RunLoop\n\nGCD的中部分接口使用了RunLoop,提交到mainQueue的blocks.当调用`dispatch_async(dispatch_get_main_queue(), block)`时,libDispatch(系统的某个进程)会向app的进程的main runloop的dispatchPort(是一个mach port)发送消息.此时RunLoop如果在休眠状态,会被唤醒,并从dispatch port中取出消息,并在会在回调方法`__CFRUNLOOP_IS_SERVICING_THE_MAIN_DISPATCH_QUEUE__()`中执行这个block.\n\n> libDispatch分发到main queue的block才会与RunLoop有关,分发到其他的子线程的内容,还是又libDispatch处理.\n> \n> RunLoop的运行超时是由GCD的dispatch_timer控制的.\n\n#### 8 关于网络请求\n\n关于网络请求的接口，主要有以下几层:\n\n* CFSocket:是最底层的接口，只负责socket的通信\n* CFNetwork:是基于CFSocket等接口的上层封装,ASIHttpRequest工作在这层\n* NSURLConnection:是基于CFNetwork的更高层的封装，提供面向对象的接口，\nAFNetworking2.x工作于这一层\n* NSURLSession:是ios7中新增的接口,表面上和NSURLConnection并列,但底层\n仍然用到NSURLConnection的部分功能,AFNetworking3.x和Alamofire在这层\n\n因此,现在使用的大多网络库都与CFSocket,CFNetwork层相关.\n\n通常使用NSURLConnection时,你会传入一个Delegate,当调用[connection start]后，这个Delegate就会不停收到事件回调。实际上,start 这个函数的内部会\n会获取 CurrentRunLoop，然后在其中的 DefaultMode 添加了4个 Source0 (即需要手动\n触发的Source)。CFMultiplexerSource 是负责各种 Delegate 回调的，CFHTTPCookie\nStorage 是处理各种 Cookie 的。\n\n>NSURLConnection的工作过程可以参考:\nhttp://blog.ibireme.com/2015/05/18/runloop/#base\n\n### RunLoop的应用举例\n#### 1 AFNetworking2.x\n\n使用NSURLConnection时候,需要在后台自定义线程中接受Delegate回调.AFNetworking创建一个自定义线程,然后在其中添加一个Mach Port,然后启动RunLoop(前面提到过,如果RunLoop中没有source/timer/observer会立即退出).在以后线程以后通过NSObject的`performSelector:onThread:withObject:waitUnitlDone:modes:`方法,将`[self.connection scheduleInRunLoop:runLoop forMode:runLoopMode]`.\n\n>可以参考:\n>https://github.com/brownfeng/SourceSet/tree/master/AFNetworking2.x\n\n#### 2 AsyncDisplayKit\nAsyncDisplayKit是Facebook推出的用于保持界面流畅性的框架,其原理大致如下：\n\nUI线程中一旦出现繁重的任务就会导致界面卡顿,这类任务通常分为3类:排版,绘制,UI对象操作。通过各种方法将前两种任务丢到后台运行,最后一类操作只能在主线程中执行,因此,ASDK仿照QuartzCore/UIKit框架的模式,实现了一套类似的界面更新的机制:即在主线程的RunLoop中添加一个 Observer,监听kCFRunLoopBeforeWaiting和 kCFRunLoopExit事件,在收到回调时,遍历所有之前放入队列的待处理的任务,然后一一执行。\n\n#### 3 NSTimer在TrackingMode运行\n#### 4 TableView延迟加载图片\n#### 5 RunLoop解决大图加载的问题\n#### 6 RunLoop监控App卡顿\n\n>可以参考文章:\n>\n>http://www.jianshu.com/p/929d855c5a5a\n>http://www.jianshu.com/p/924cb2b218f5\n\n### 参考内容\nhttp://blog.ibireme.com/2015/05/18/runloop/\n\nhttp://yun.baidu.com/share/link?shareid=2268593032&uk=2885973690","slug":"RunLoop总结","published":1,"updated":"2016-08-26T16:19:12.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cisbyxx8500076squa773be04","content":"<p>通过前面几篇文章可以知道RunLoop实际上是一个事件处理的循环.只要一个线程启动了RunLoop,在它没有收到事件时候,它就会使得线程休眠,如果有事件,就调用相应的事件处理函数.</p>\n<h3 id=\"RunLoop与线程的关系\"><a href=\"#RunLoop与线程的关系\" class=\"headerlink\" title=\"RunLoop与线程的关系\"></a>RunLoop与线程的关系</h3><p>RunLoop与线程是一一对应的.每个新建的线程(MovedainThread除外,主线程会默认启动)在默认状态下,它的RunLoop是没有开启的,必须手动调用<code>CurrentRunLoop</code>等方法才会开启RunLoop.通常iOS开发中,接触到的API分别是Cocoa的NSRunLoop和CoreFoundation的CFRunLoop.</p>\n<h3 id=\"RunLoop的组成\"><a href=\"#RunLoop的组成\" class=\"headerlink\" title=\"RunLoop的组成\"></a>RunLoop的组成</h3><p>RunLoop每次只能运行在一个mode中,它与mode是一对多的关系.RunLoopMode中包括不同类型的事件源,主要分成两大类:TimerSource, 各种异步输入InputSource.与此同时,RunLoop会在不同的声明周期给外界发送通知,告知观察者自己的状态.因此可以注册RunLoopObserver来观察RunLoop的状态.他们之间的关系如下:</p>\n<ul>\n<li>一个runLoop包含多个Mode,每个Mode内有多个个Source/Timer/Observer(底层会在RunLoopMode中用Array存储)</li>\n<li>每次启动RunLoop,需要指定一个Mode(<code>CFRunLoopRunSpecific</code>函数),如果需要切换Mode,只能退出RunLoop，再重新调用<code>CFRunLoopRunSpecific</code>,重新指定Mode</li>\n<li>Source/Timer/Observer统称为mode item,每个item可以同时加入多个mode,但一个item被重复加入同一个mode无效</li>\n<li>如果一个mode中一个item都没有,RunLoop就会立即退出(AFNetworking2.x)</li>\n</ul>\n<h4 id=\"1-RunLoopMode\"><a href=\"#1-RunLoopMode\" class=\"headerlink\" title=\"1. RunLoopMode\"></a>1. RunLoopMode</h4><p>RunLoop会运行在某个特定的mode下,称为RunLoopMode,具体mode可以使用系统的NSDefaultMode或者NSTrackingMode,也可以自己定义mode.如果想让事件在任何一个mode中处理,可以把事件源加入到CommonMode中.</p>\n<h4 id=\"2-TimerSource\"><a href=\"#2-TimerSource\" class=\"headerlink\" title=\"2. TimerSource\"></a>2. TimerSource</h4><p>TimerSource在Cocoa框架中,一般是NSTimer.在使用它时,它会向RunLoop发送消息,RunLoop会根据Timer的类型去单次或者多次执行回调函数.在开始启动Timer时候,需要手动设定具体运行在哪一个RunLoopMode,默认情况下会运行在NSDefaultMode.</p>\n<p>通过RunLoop的源码发现,TimerSource的触发实际是通过Source1(MachPort)唤醒RunLoop,然后调用回调方法的.</p>\n<h4 id=\"3-异步输入源InputSource\"><a href=\"#3-异步输入源InputSource\" class=\"headerlink\" title=\"3. 异步输入源InputSource\"></a>3. 异步输入源InputSource</h4><p>异步事件其实会包括的类型:</p>\n<ul>\n<li>source0: 非port事件,可以手动触发(内部只有一个callback函数指针)</li>\n<li>source1: 基于MachPort的Source事件,大多是内核发出的,需要调用<code>mach_msg</code>方法</li>\n</ul>\n<h4 id=\"4-NSObject的selector源\"><a href=\"#4-NSObject的selector源\" class=\"headerlink\" title=\"4. NSObject的selector源\"></a>4. NSObject的selector源</h4><p>由NSObject 的 performSelector:wait:…等产生,下文会细讲.</p>\n<h4 id=\"5-RunLoopObserver\"><a href=\"#5-RunLoopObserver\" class=\"headerlink\" title=\"5. RunLoopObserver\"></a>5. RunLoopObserver</h4><p>RunLoop的事件源中,Timer是同步事件,另外的是异步事件Source0,Source1,selector. 而RunLoopObserver可以让你在某个特定的时期处理一些事情,比如在RunLoop马上进入休眠状态时,或者在刚刚被唤醒时等等.</p>\n<p>具体的RunLoop可以被观察的状态有:</p>\n<ul>\n<li>RunLoop进入时候(kCFRunLoopEntry)</li>\n<li>RunLoop将要处理Timer时(kCFRunLoopBeforeTimers)</li>\n<li>RunLoop将要处理Source0时(kCFRunLoopBeforeSources)</li>\n<li>RunLoop将要进入睡眠的时候(kCFRunLoopBeforeWaiting)</li>\n<li>RunLoop将要被唤醒的时(kCFRunLoopAfterWaiting)</li>\n<li>RunLoop停止的时候(kCFRunLoopExit)</li>\n</ul>\n<p>你可以在RunLoop中观察一个或者多个状态.观察者也可以设置是否只观察一次或者重复多次观察,如果只观察一次,那么调用回调函数以后,观察者就自动被移除了.</p>\n<h3 id=\"RunLoop具体循环逻辑\"><a href=\"#RunLoop具体循环逻辑\" class=\"headerlink\" title=\"RunLoop具体循环逻辑\"></a>RunLoop具体循环逻辑</h3><p>RunLoop源码中关键的函数有以下几个:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">CFRunLoopRunSpecific(xxxx)//入口&amp;出口</div><div class=\"line\">CFRunLoopRun(xxxx)//循环</div></pre></td></tr></table></figure>\n<p>具体的执行过程如下:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div></pre></td><td class=\"code\"><pre><div class=\"line\">1 通知观察者runLoop已经启动(DoObserver)</div><div class=\"line\">2 记录RunLoop启动事件,并设定RunLoop运行的超时条件</div><div class=\"line\"></div><div class=\"line\">**** RunLoop正式进入循环 *****</div><div class=\"line\"></div><div class=\"line\">3 通知观察者任何即将要开始的定时器(DoObserver)</div><div class=\"line\">4 通知观察着任何即将启动的非基于端口的源(DoObserver)</div><div class=\"line\">5 调用RunLoopMode的blocks链中的block方法(DoBlocks)</div><div class=\"line\">6 启动任何准备好的非基于端口的source0源(DoSource0)</div><div class=\"line\">7 调用RunLoopMode的blocks链中的block方法(DoBlocks)</div><div class=\"line\">8 检查MachPort端口是否有消息要处理，如果有立即进入步骤12</div><div class=\"line\">9 如果没有消息处理,通知观察者线程进入休眠(DoObserver)</div><div class=\"line\"></div><div class=\"line\">**** RunLoop休眠: zzz...****</div><div class=\"line\"></div><div class=\"line\">10 将线程至于休眠直到任意下面的事件发生(调用`mach_msg`使RunLoop休眠)</div><div class=\"line\">\tA.某一时间到达基于端口的源</div><div class=\"line\">\tB.定时器启动</div><div class=\"line\">\tC.runLoop设置的时间已经超过</div><div class=\"line\">\tD.runLoop被显示唤醒</div><div class=\"line\">11 通知观察者线程将被唤醒(DoObserver)</div><div class=\"line\">12 处理未处理的事件(有dispatchPort显示是哪个port的事件需要处理)</div><div class=\"line\">\tA.如果livePort为NULL,啥都不做(会跳出循环)</div><div class=\"line\">\tB.如果livePort是wakeUpPort,说明RunLoop运行超时(会跳出循环)</div><div class=\"line\">\tC.如果livePort是timerPort,说明timerSource启动(DoTimers),进入步骤3</div><div class=\"line\">\tD.如果livePort是dispatchPort,说明系统的libDispatch向主线程送消息,</div><div class=\"line\">\t  会调dispatch_async(dispatch_get_main_queue(),block)</div><div class=\"line\">\tE.如果livePort是其他port,通过mach_msg取出回调,处理事件(DoSource1),</div><div class=\"line\">\t  进入步骤3</div><div class=\"line\">\t</div><div class=\"line\">**** RunLoop退出循环 ****</div><div class=\"line\"></div><div class=\"line\">13 通知观察者runLoop结束</div></pre></td></tr></table></figure>\n<p>从RunLoop的底层源码可以看出,RunLoop中的各种DoXXXX函数与最终的callout系统调用关系如下:</p>\n<ul>\n<li>DoObserver<br><code>__CFRUNLOOP_IS_CALLING_OUT_TO_AN_OBSERVER_CALLBACK_FUNCTION__()</code></li>\n<li>DoBlocks<br><code>__CFRUNLOOP_IS_CALLING_OUT_TO_A_BLOCK__()</code></li>\n<li>DoSource0 <code>__CFRUNLOOP_IS_CALLING_OUT_TO_A_SOURCE0_PERFORM_FUNCTION__()</code></li>\n<li>DoTimers <code>__CFRUNLOOP_IS_CALLING_OUT_TO_A_TIMER_CALLBACK_FUNCTION__()</code></li>\n<li>dispatchPort <code>__CFRUNLOOP_IS_SERVICING_THE_MAIN_DISPATCH_QUEUE__()</code></li>\n<li>DoSource1 <code>__CFRUNLOOP_IS_CALLING_OUT_TO_A_SOURCE1_PERFORM_FUNCTION__()</code></li>\n</ul>\n<blockquote>\n<p>注意,以上callout调用,对于传入RunLoopMode中的Timers,Source0s,Blocks都是数组,因此对数组内的每个满足条件的成员都会进行callout调用,而source1和dispatchPort由于唤醒时每次只能执行一个.</p>\n<p>repeat Timers执行的时间不一定是完全准确的,如果错过某次执行,只能等到下次RunLoop循环时候再执行了</p>\n</blockquote>\n<h4 id=\"何时使用RunLoop\"><a href=\"#何时使用RunLoop\" class=\"headerlink\" title=\"何时使用RunLoop\"></a>何时使用RunLoop</h4><p>自己配置并启动它，你不需要再任何情况下都去启动一个线程的runLoop。runLoop在你<br>要和线程有更多的交互时才需要，比如以下情况：</p>\n<ul>\n<li>使用端口或者自定义输入源来和其他线程通信；</li>\n<li>使用线程的NSTimer (可以用dispatch_timer代替)</li>\n<li>Cocoa中使用任何performSelector</li>\n<li>让线程周期性干活(AFNetworking2.x)</li>\n</ul>\n<h3 id=\"CFRunLoop相关内容\"><a href=\"#CFRunLoop相关内容\" class=\"headerlink\" title=\"CFRunLoop相关内容\"></a>CFRunLoop相关内容</h3><h4 id=\"CoreFoundation中RunLoop的接口\"><a href=\"#CoreFoundation中RunLoop的接口\" class=\"headerlink\" title=\"CoreFoundation中RunLoop的接口\"></a>CoreFoundation中RunLoop的接口</h4><p>一般使用的是CoreFoundation中的CFRunLoop,因此这里主要总结CFRunLoop的对外接口.RunLoop中对外主要有以下几个接口类:</p>\n<ul>\n<li>CFRunLoopRef</li>\n<li>CFRunLoopModeRef (并未对外暴露)</li>\n<li>CFRunLoopSourceRef: 异步的source0和source1事件源<ul>\n<li>source0: 结构体中只有perform回调函数,它并不能主动触发事件,使用时,需要手动触发(<code>CFRunLoopSourceSignal()</code>).CFRunLoopSourceSignal将这个source标记为待处理，后调用CFRunLoopWakeUp来唤醒runLoop，让其处理这个事件(也就是source0无法唤醒RunLoop)</li>\n<li>source1: 结构体中有mach_port和invoke回调函数,通常是内核和其他进程给该RunLoop发送的消息,从源码可以看出source1,是可以自动唤醒RunLoop的,因为它是通过port方式发送事件的.</li>\n</ul>\n</li>\n<li>CFRunLoopTimerRef: 同步的TimerSource, 在Cocoa中是NSTimer.它的底层结构体中包含触发时间和回调，当其加入到runLoop中是，runLoop会注册对应的时间点，当时<br>间点到时,RunLoop会收到wakeUpPort(RunLoop的一个属性)source1类型的消息,将唤醒以执行Timer的回调；</li>\n<li>CFRunLoopObserverRef: 注册观察者时候可以添加一个回调函数,当RunLoop的状态变化时会通知注册的观察者,注册的回调函数在此时调用</li>\n</ul>\n<blockquote>\n<p>具体的demo可以参考:<br><a href=\"https://github.com/brownfeng/RunLoopDemo\" target=\"_blank\" rel=\"external\">https://github.com/brownfeng/RunLoopDemo</a></p>\n<h4 id=\"RunLoop底层结构体\"><a href=\"#RunLoop底层结构体\" class=\"headerlink\" title=\"RunLoop底层结构体\"></a>RunLoop底层结构体</h4></blockquote>\n<p>CFRunLoop的结构如下</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">struct __CFRunLoop &#123;</div><div class=\"line\">    CFMutableSetRef _commonModes;     </div><div class=\"line\">    CFMutableSetRef _commonModeItems; </div><div class=\"line\">    CFRunLoopModeRef _currentMode;    </div><div class=\"line\">    CFMutableSetRef _modes;           </div><div class=\"line\">    ...</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>CFRunLoopMode的结构如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">struct __CFRunLoopMode &#123;</div><div class=\"line\">    CFStringRef _name;            </div><div class=\"line\">    CFMutableSetRef _sources0;    </div><div class=\"line\">    CFMutableSetRef _sources1;    </div><div class=\"line\">    CFMutableArrayRef _observers; </div><div class=\"line\">    CFMutableArrayRef _timers;    </div><div class=\"line\">    ...</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>其中，CFRunLoop对外暴露的管理Mode的接口有两个:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">CFRunLoopAddCommonMode</div><div class=\"line\">CFRunLoopRunInMode</div></pre></td></tr></table></figure>\n<p>Mode暴露的管理mode item的接口有下面几个：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">CFRunLoopAddSource</div><div class=\"line\">CFRunLoopAddObserver</div><div class=\"line\">CFRunLoopAddTimer</div><div class=\"line\">CFRunLoopRemoveSource</div><div class=\"line\">CFRunLoopRemoveObserver</div><div class=\"line\">CFRunLoopRemoveTimer</div></pre></td></tr></table></figure>\n<h3 id=\"Cocoa框架中RunLoop相关的内容\"><a href=\"#Cocoa框架中RunLoop相关的内容\" class=\"headerlink\" title=\"Cocoa框架中RunLoop相关的内容\"></a>Cocoa框架中RunLoop相关的内容</h3><h4 id=\"1-AutoReleasePool\"><a href=\"#1-AutoReleasePool\" class=\"headerlink\" title=\"1 AutoReleasePool\"></a>1 AutoReleasePool</h4><p>AutoReleasePool是Apple中清理临时变量,释放内容的机制,在app的main函数中,所有的内容都是包裹在一个AutoReleasePool中的.这个过程中,Apple在RunLoop中注册三个RunLoopObserver:</p>\n<ul>\n<li>第一个RunLoopObserver关注kCFRunLoopEntry状态,callback函数中会自动创建autoReleasePool,并且observer优先级最高(RunLoop第一个调用它的回调),保证应用启动后所有的操作都在autoreleasePool中运行.</li>\n<li>第二个RunLoopObserver关注kCFRunLoopBeforeWaiting状态,在此时autoreleasePool会释放旧的池,并创建一个新的autoreleasePool.</li>\n<li>第三个RunLoopObserver关注kCFRunLoopExit状态,此时释放autoreleasePool,并且保证observer的优先级最低,即在所有其他的observers都执行完以后才执行autorelease相关的observer的callback函数.</li>\n</ul>\n<blockquote>\n<p>深入学习可以参考:<br><a href=\"http://blog.sunnyxx.com/2014/10/15/behind-autorelease/\" target=\"_blank\" rel=\"external\">http://blog.sunnyxx.com/2014/10/15/behind-autorelease/</a></p>\n</blockquote>\n<h4 id=\"2-iOS事件响应\"><a href=\"#2-iOS事件响应\" class=\"headerlink\" title=\"2 iOS事件响应\"></a>2 iOS事件响应</h4><p>RunLoop解释了为何iOS应用能够接受到屏幕触摸等事件.Apple在iOS app启动时候在main RunLoop中注册一个Source1事件,是基于Mach Port(进程间通信)的系统层面的进程.</p>\n<p>当一个触摸事件发生以后,IOKit会生成一个IOHIDEvent事件,并由系统层面的SpringBoard接受按键(锁屏/静音),触摸,加速,接近传感器等几种Event,然后通过进程间通信的mach port发送给App的main RunLoop.然后在DoSource1中,source1的回调会触发,内部会调用系统<code>_UIApplicationHandleEventQueue()</code>进行事件分发.</p>\n<p><code>_UIApplicationHandleEventQueue()</code>方法会把IOHIDEvent处理,并包装成常见的UIEvent事件进行处理或分发,其中包括识别UIGesture/处理屏幕旋转/发送给UIWindow等.通常事件比如UIButton点击,touchesBegin/Move/End/Cancel等事件都是在这个Source1回调中完成.</p>\n<h4 id=\"3-手势识别GestureRecognizer\"><a href=\"#3-手势识别GestureRecognizer\" class=\"headerlink\" title=\"3 手势识别GestureRecognizer\"></a>3 手势识别GestureRecognizer</h4><p>在上面的事件响应中,回调方法<code>_UIApplicationHandleEventQueue()</code>识别了一个手势以后,首先会调用Cancel将当前的touchesBegin/Move/End系列的回调打断.随后系统会将UIGestureRecognizer标记为等待处理.</p>\n<p>然后苹果会在RunLoop中注册一个Observer观察kCFRunLoopBeforeWaiting(睡眠前).这个观察者的回调函数是<code>_UIGestureRecognizerUpdateObserver()</code>,它会获取刚才所有被标记为等待处理等的GestureRecognizer,并且执行GestureRecognizer的回调.</p>\n<h4 id=\"4-界面更新-UI-update\"><a href=\"#4-界面更新-UI-update\" class=\"headerlink\" title=\"4 界面更新(UI update)\"></a>4 界面更新(UI update)</h4><p>在App改变UI时,例如修改view的Frame,更新UIView/CALayer的层次,或者手动调用UIView/CALayer的setNeedsLayout/setNeedsDisplay方法以后,这个UIView/CALayer就被标记为等待处理,并且提交到一个全局容器.</p>\n<p>然后苹果在RunLoop中注册一个Observer观察kCFRunLoopBeforeWaiting和kCFRunLoopExit状态.在状态触发DoObserver的callback中会调用函数<code>_ZN2CA11Transaction17observer_callbackEP19__CFRunLoopObservermPv()</code>.这个函数中会遍历所有的等待处理的UIView/CALayer,执行绘制和调整,更新UI界面.</p>\n<p>这个函数的调用栈如下:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">_ZN2CA11Transaction17observer_callbackEP19__CFRunLoopObservermPv()</div><div class=\"line\">    QuartzCore:CA::Transaction::observer_callback://DoObserver</div><div class=\"line\">        CA::Transaction::commit();//提交到全局容器</div><div class=\"line\">            CA::Context::commit_transaction();</div><div class=\"line\">                CA::Layer::layout_and_display_if_needed();</div><div class=\"line\">                    CA::Layer::layout_if_needed();</div><div class=\"line\">                        [CALayer layoutSublayers];</div><div class=\"line\">                            [UIView layoutSubviews];</div><div class=\"line\">                    CA::Layer::display_if_needed();</div><div class=\"line\">                        [CALayer display];</div><div class=\"line\">                            [UIView drawRect];</div></pre></td></tr></table></figure>\n<h4 id=\"5-NSTimer定时器\"><a href=\"#5-NSTimer定时器\" class=\"headerlink\" title=\"5 NSTimer定时器\"></a>5 NSTimer定时器</h4><p>NSTimer的CF层是CFRunLoopTimerRef,它们可以toll-free bridged.当使用<code>CFRunLoopAddTimer()</code>将NSTimer&amp;CFRunLoopTimerRef注册到RunLoop中以后,RunLoop会为它重复的时间点注册好事件(有一定的时间容忍度,触发的时间误差).如果中间某次触发被错过,那么这次触发时间点的回调也会被跳过去. 实际中NSTimer的触发是通过source1触发的,如果timer被触发会将RunLoop的有一个Mach Port-<code>timePort</code>-发消息,然后会唤醒RunLoop,调用DoTimers.可以参考上面的RunLoop运行流程.</p>\n<blockquote>\n<p>CADisplayLink 是一个和屏幕刷新一致的定时器,如果两次屏幕刷新时候在执行一个长时间任务,那其中就会有一帧被跳过,造成界面卡顿.Facebook的AsyncDisplayLink使用RunLoop来解决丢帧的问题.</p>\n<p>还有一个GCD的定时器<code>dispatch_timer</code>,RunLoop的超时机制也是使用<code>dispatch_timer</code>.</p>\n</blockquote>\n<h4 id=\"6-部分PerformSelector事件源\"><a href=\"#6-部分PerformSelector事件源\" class=\"headerlink\" title=\"6 部分PerformSelector事件源\"></a>6 部分PerformSelector事件源</h4><p>NSObject的部分与时间相关的PerformSelector方法与RunLoop密切相关.当调用NSObject的performSelector:afterDelay以后,实际内部会创建一个Timer并添加RunLoopMode中,等待Timer触发,然后调用DoTimers,其中包含的回调方法就是selector.因此如果当前线程没有开启RunLoop,这个方法无效.类似的NSObject的performSelector:onThread:也是类似,需要线程开启RunLoop.具体涉及到的perform方法包括:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">- performSelector:afterDelay:</div><div class=\"line\">– performSelector:withObject:afterDelay:</div><div class=\"line\">– performSelectorOnMainThread:withObject:waitUntilDone:</div><div class=\"line\">– performSelectorOnMainThread:withObject:waitUntilDone:modes:</div><div class=\"line\">– performSelector:onThread:withObject:waitUntilDone:modes:</div><div class=\"line\">- performSelector:onThread:withObject:waitUntilDone:</div></pre></td></tr></table></figure>\n<h4 id=\"7-GCD与RunLoop\"><a href=\"#7-GCD与RunLoop\" class=\"headerlink\" title=\"7 GCD与RunLoop\"></a>7 GCD与RunLoop</h4><p>GCD的中部分接口使用了RunLoop,提交到mainQueue的blocks.当调用<code>dispatch_async(dispatch_get_main_queue(), block)</code>时,libDispatch(系统的某个进程)会向app的进程的main runloop的dispatchPort(是一个mach port)发送消息.此时RunLoop如果在休眠状态,会被唤醒,并从dispatch port中取出消息,并在会在回调方法<code>__CFRUNLOOP_IS_SERVICING_THE_MAIN_DISPATCH_QUEUE__()</code>中执行这个block.</p>\n<blockquote>\n<p>libDispatch分发到main queue的block才会与RunLoop有关,分发到其他的子线程的内容,还是又libDispatch处理.</p>\n<p>RunLoop的运行超时是由GCD的dispatch_timer控制的.</p>\n</blockquote>\n<h4 id=\"8-关于网络请求\"><a href=\"#8-关于网络请求\" class=\"headerlink\" title=\"8 关于网络请求\"></a>8 关于网络请求</h4><p>关于网络请求的接口，主要有以下几层:</p>\n<ul>\n<li>CFSocket:是最底层的接口，只负责socket的通信</li>\n<li>CFNetwork:是基于CFSocket等接口的上层封装,ASIHttpRequest工作在这层</li>\n<li>NSURLConnection:是基于CFNetwork的更高层的封装，提供面向对象的接口，<br>AFNetworking2.x工作于这一层</li>\n<li>NSURLSession:是ios7中新增的接口,表面上和NSURLConnection并列,但底层<br>仍然用到NSURLConnection的部分功能,AFNetworking3.x和Alamofire在这层</li>\n</ul>\n<p>因此,现在使用的大多网络库都与CFSocket,CFNetwork层相关.</p>\n<p>通常使用NSURLConnection时,你会传入一个Delegate,当调用[connection start]后，这个Delegate就会不停收到事件回调。实际上,start 这个函数的内部会<br>会获取 CurrentRunLoop，然后在其中的 DefaultMode 添加了4个 Source0 (即需要手动<br>触发的Source)。CFMultiplexerSource 是负责各种 Delegate 回调的，CFHTTPCookie<br>Storage 是处理各种 Cookie 的。</p>\n<blockquote>\n<p>NSURLConnection的工作过程可以参考:<br><a href=\"http://blog.ibireme.com/2015/05/18/runloop/#base\" target=\"_blank\" rel=\"external\">http://blog.ibireme.com/2015/05/18/runloop/#base</a></p>\n</blockquote>\n<h3 id=\"RunLoop的应用举例\"><a href=\"#RunLoop的应用举例\" class=\"headerlink\" title=\"RunLoop的应用举例\"></a>RunLoop的应用举例</h3><h4 id=\"1-AFNetworking2-x\"><a href=\"#1-AFNetworking2-x\" class=\"headerlink\" title=\"1 AFNetworking2.x\"></a>1 AFNetworking2.x</h4><p>使用NSURLConnection时候,需要在后台自定义线程中接受Delegate回调.AFNetworking创建一个自定义线程,然后在其中添加一个Mach Port,然后启动RunLoop(前面提到过,如果RunLoop中没有source/timer/observer会立即退出).在以后线程以后通过NSObject的<code>performSelector:onThread:withObject:waitUnitlDone:modes:</code>方法,将<code>[self.connection scheduleInRunLoop:runLoop forMode:runLoopMode]</code>.</p>\n<blockquote>\n<p>可以参考:<br><a href=\"https://github.com/brownfeng/SourceSet/tree/master/AFNetworking2.x\" target=\"_blank\" rel=\"external\">https://github.com/brownfeng/SourceSet/tree/master/AFNetworking2.x</a></p>\n</blockquote>\n<h4 id=\"2-AsyncDisplayKit\"><a href=\"#2-AsyncDisplayKit\" class=\"headerlink\" title=\"2 AsyncDisplayKit\"></a>2 AsyncDisplayKit</h4><p>AsyncDisplayKit是Facebook推出的用于保持界面流畅性的框架,其原理大致如下：</p>\n<p>UI线程中一旦出现繁重的任务就会导致界面卡顿,这类任务通常分为3类:排版,绘制,UI对象操作。通过各种方法将前两种任务丢到后台运行,最后一类操作只能在主线程中执行,因此,ASDK仿照QuartzCore/UIKit框架的模式,实现了一套类似的界面更新的机制:即在主线程的RunLoop中添加一个 Observer,监听kCFRunLoopBeforeWaiting和 kCFRunLoopExit事件,在收到回调时,遍历所有之前放入队列的待处理的任务,然后一一执行。</p>\n<h4 id=\"3-NSTimer在TrackingMode运行\"><a href=\"#3-NSTimer在TrackingMode运行\" class=\"headerlink\" title=\"3 NSTimer在TrackingMode运行\"></a>3 NSTimer在TrackingMode运行</h4><h4 id=\"4-TableView延迟加载图片\"><a href=\"#4-TableView延迟加载图片\" class=\"headerlink\" title=\"4 TableView延迟加载图片\"></a>4 TableView延迟加载图片</h4><h4 id=\"5-RunLoop解决大图加载的问题\"><a href=\"#5-RunLoop解决大图加载的问题\" class=\"headerlink\" title=\"5 RunLoop解决大图加载的问题\"></a>5 RunLoop解决大图加载的问题</h4><h4 id=\"6-RunLoop监控App卡顿\"><a href=\"#6-RunLoop监控App卡顿\" class=\"headerlink\" title=\"6 RunLoop监控App卡顿\"></a>6 RunLoop监控App卡顿</h4><blockquote>\n<p>可以参考文章:</p>\n<p><a href=\"http://www.jianshu.com/p/929d855c5a5a\" target=\"_blank\" rel=\"external\">http://www.jianshu.com/p/929d855c5a5a</a><br><a href=\"http://www.jianshu.com/p/924cb2b218f5\" target=\"_blank\" rel=\"external\">http://www.jianshu.com/p/924cb2b218f5</a></p>\n</blockquote>\n<h3 id=\"参考内容\"><a href=\"#参考内容\" class=\"headerlink\" title=\"参考内容\"></a>参考内容</h3><p><a href=\"http://blog.ibireme.com/2015/05/18/runloop/\" target=\"_blank\" rel=\"external\">http://blog.ibireme.com/2015/05/18/runloop/</a></p>\n<p><a href=\"http://yun.baidu.com/share/link?shareid=2268593032&amp;uk=2885973690\" target=\"_blank\" rel=\"external\">http://yun.baidu.com/share/link?shareid=2268593032&amp;uk=2885973690</a></p>\n","excerpt":"","more":"<p>通过前面几篇文章可以知道RunLoop实际上是一个事件处理的循环.只要一个线程启动了RunLoop,在它没有收到事件时候,它就会使得线程休眠,如果有事件,就调用相应的事件处理函数.</p>\n<h3 id=\"RunLoop与线程的关系\"><a href=\"#RunLoop与线程的关系\" class=\"headerlink\" title=\"RunLoop与线程的关系\"></a>RunLoop与线程的关系</h3><p>RunLoop与线程是一一对应的.每个新建的线程(MovedainThread除外,主线程会默认启动)在默认状态下,它的RunLoop是没有开启的,必须手动调用<code>CurrentRunLoop</code>等方法才会开启RunLoop.通常iOS开发中,接触到的API分别是Cocoa的NSRunLoop和CoreFoundation的CFRunLoop.</p>\n<h3 id=\"RunLoop的组成\"><a href=\"#RunLoop的组成\" class=\"headerlink\" title=\"RunLoop的组成\"></a>RunLoop的组成</h3><p>RunLoop每次只能运行在一个mode中,它与mode是一对多的关系.RunLoopMode中包括不同类型的事件源,主要分成两大类:TimerSource, 各种异步输入InputSource.与此同时,RunLoop会在不同的声明周期给外界发送通知,告知观察者自己的状态.因此可以注册RunLoopObserver来观察RunLoop的状态.他们之间的关系如下:</p>\n<ul>\n<li>一个runLoop包含多个Mode,每个Mode内有多个个Source/Timer/Observer(底层会在RunLoopMode中用Array存储)</li>\n<li>每次启动RunLoop,需要指定一个Mode(<code>CFRunLoopRunSpecific</code>函数),如果需要切换Mode,只能退出RunLoop，再重新调用<code>CFRunLoopRunSpecific</code>,重新指定Mode</li>\n<li>Source/Timer/Observer统称为mode item,每个item可以同时加入多个mode,但一个item被重复加入同一个mode无效</li>\n<li>如果一个mode中一个item都没有,RunLoop就会立即退出(AFNetworking2.x)</li>\n</ul>\n<h4 id=\"1-RunLoopMode\"><a href=\"#1-RunLoopMode\" class=\"headerlink\" title=\"1. RunLoopMode\"></a>1. RunLoopMode</h4><p>RunLoop会运行在某个特定的mode下,称为RunLoopMode,具体mode可以使用系统的NSDefaultMode或者NSTrackingMode,也可以自己定义mode.如果想让事件在任何一个mode中处理,可以把事件源加入到CommonMode中.</p>\n<h4 id=\"2-TimerSource\"><a href=\"#2-TimerSource\" class=\"headerlink\" title=\"2. TimerSource\"></a>2. TimerSource</h4><p>TimerSource在Cocoa框架中,一般是NSTimer.在使用它时,它会向RunLoop发送消息,RunLoop会根据Timer的类型去单次或者多次执行回调函数.在开始启动Timer时候,需要手动设定具体运行在哪一个RunLoopMode,默认情况下会运行在NSDefaultMode.</p>\n<p>通过RunLoop的源码发现,TimerSource的触发实际是通过Source1(MachPort)唤醒RunLoop,然后调用回调方法的.</p>\n<h4 id=\"3-异步输入源InputSource\"><a href=\"#3-异步输入源InputSource\" class=\"headerlink\" title=\"3. 异步输入源InputSource\"></a>3. 异步输入源InputSource</h4><p>异步事件其实会包括的类型:</p>\n<ul>\n<li>source0: 非port事件,可以手动触发(内部只有一个callback函数指针)</li>\n<li>source1: 基于MachPort的Source事件,大多是内核发出的,需要调用<code>mach_msg</code>方法</li>\n</ul>\n<h4 id=\"4-NSObject的selector源\"><a href=\"#4-NSObject的selector源\" class=\"headerlink\" title=\"4. NSObject的selector源\"></a>4. NSObject的selector源</h4><p>由NSObject 的 performSelector:wait:…等产生,下文会细讲.</p>\n<h4 id=\"5-RunLoopObserver\"><a href=\"#5-RunLoopObserver\" class=\"headerlink\" title=\"5. RunLoopObserver\"></a>5. RunLoopObserver</h4><p>RunLoop的事件源中,Timer是同步事件,另外的是异步事件Source0,Source1,selector. 而RunLoopObserver可以让你在某个特定的时期处理一些事情,比如在RunLoop马上进入休眠状态时,或者在刚刚被唤醒时等等.</p>\n<p>具体的RunLoop可以被观察的状态有:</p>\n<ul>\n<li>RunLoop进入时候(kCFRunLoopEntry)</li>\n<li>RunLoop将要处理Timer时(kCFRunLoopBeforeTimers)</li>\n<li>RunLoop将要处理Source0时(kCFRunLoopBeforeSources)</li>\n<li>RunLoop将要进入睡眠的时候(kCFRunLoopBeforeWaiting)</li>\n<li>RunLoop将要被唤醒的时(kCFRunLoopAfterWaiting)</li>\n<li>RunLoop停止的时候(kCFRunLoopExit)</li>\n</ul>\n<p>你可以在RunLoop中观察一个或者多个状态.观察者也可以设置是否只观察一次或者重复多次观察,如果只观察一次,那么调用回调函数以后,观察者就自动被移除了.</p>\n<h3 id=\"RunLoop具体循环逻辑\"><a href=\"#RunLoop具体循环逻辑\" class=\"headerlink\" title=\"RunLoop具体循环逻辑\"></a>RunLoop具体循环逻辑</h3><p>RunLoop源码中关键的函数有以下几个:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">CFRunLoopRunSpecific(xxxx)//入口&amp;出口</div><div class=\"line\">CFRunLoopRun(xxxx)//循环</div></pre></td></tr></table></figure>\n<p>具体的执行过程如下:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div></pre></td><td class=\"code\"><pre><div class=\"line\">1 通知观察者runLoop已经启动(DoObserver)</div><div class=\"line\">2 记录RunLoop启动事件,并设定RunLoop运行的超时条件</div><div class=\"line\"></div><div class=\"line\">**** RunLoop正式进入循环 *****</div><div class=\"line\"></div><div class=\"line\">3 通知观察者任何即将要开始的定时器(DoObserver)</div><div class=\"line\">4 通知观察着任何即将启动的非基于端口的源(DoObserver)</div><div class=\"line\">5 调用RunLoopMode的blocks链中的block方法(DoBlocks)</div><div class=\"line\">6 启动任何准备好的非基于端口的source0源(DoSource0)</div><div class=\"line\">7 调用RunLoopMode的blocks链中的block方法(DoBlocks)</div><div class=\"line\">8 检查MachPort端口是否有消息要处理，如果有立即进入步骤12</div><div class=\"line\">9 如果没有消息处理,通知观察者线程进入休眠(DoObserver)</div><div class=\"line\"></div><div class=\"line\">**** RunLoop休眠: zzz...****</div><div class=\"line\"></div><div class=\"line\">10 将线程至于休眠直到任意下面的事件发生(调用`mach_msg`使RunLoop休眠)</div><div class=\"line\">\tA.某一时间到达基于端口的源</div><div class=\"line\">\tB.定时器启动</div><div class=\"line\">\tC.runLoop设置的时间已经超过</div><div class=\"line\">\tD.runLoop被显示唤醒</div><div class=\"line\">11 通知观察者线程将被唤醒(DoObserver)</div><div class=\"line\">12 处理未处理的事件(有dispatchPort显示是哪个port的事件需要处理)</div><div class=\"line\">\tA.如果livePort为NULL,啥都不做(会跳出循环)</div><div class=\"line\">\tB.如果livePort是wakeUpPort,说明RunLoop运行超时(会跳出循环)</div><div class=\"line\">\tC.如果livePort是timerPort,说明timerSource启动(DoTimers),进入步骤3</div><div class=\"line\">\tD.如果livePort是dispatchPort,说明系统的libDispatch向主线程送消息,</div><div class=\"line\">\t  会调dispatch_async(dispatch_get_main_queue(),block)</div><div class=\"line\">\tE.如果livePort是其他port,通过mach_msg取出回调,处理事件(DoSource1),</div><div class=\"line\">\t  进入步骤3</div><div class=\"line\">\t</div><div class=\"line\">**** RunLoop退出循环 ****</div><div class=\"line\"></div><div class=\"line\">13 通知观察者runLoop结束</div></pre></td></tr></table></figure>\n<p>从RunLoop的底层源码可以看出,RunLoop中的各种DoXXXX函数与最终的callout系统调用关系如下:</p>\n<ul>\n<li>DoObserver<br><code>__CFRUNLOOP_IS_CALLING_OUT_TO_AN_OBSERVER_CALLBACK_FUNCTION__()</code></li>\n<li>DoBlocks<br><code>__CFRUNLOOP_IS_CALLING_OUT_TO_A_BLOCK__()</code></li>\n<li>DoSource0 <code>__CFRUNLOOP_IS_CALLING_OUT_TO_A_SOURCE0_PERFORM_FUNCTION__()</code></li>\n<li>DoTimers <code>__CFRUNLOOP_IS_CALLING_OUT_TO_A_TIMER_CALLBACK_FUNCTION__()</code></li>\n<li>dispatchPort <code>__CFRUNLOOP_IS_SERVICING_THE_MAIN_DISPATCH_QUEUE__()</code></li>\n<li>DoSource1 <code>__CFRUNLOOP_IS_CALLING_OUT_TO_A_SOURCE1_PERFORM_FUNCTION__()</code></li>\n</ul>\n<blockquote>\n<p>注意,以上callout调用,对于传入RunLoopMode中的Timers,Source0s,Blocks都是数组,因此对数组内的每个满足条件的成员都会进行callout调用,而source1和dispatchPort由于唤醒时每次只能执行一个.</p>\n<p>repeat Timers执行的时间不一定是完全准确的,如果错过某次执行,只能等到下次RunLoop循环时候再执行了</p>\n</blockquote>\n<h4 id=\"何时使用RunLoop\"><a href=\"#何时使用RunLoop\" class=\"headerlink\" title=\"何时使用RunLoop\"></a>何时使用RunLoop</h4><p>自己配置并启动它，你不需要再任何情况下都去启动一个线程的runLoop。runLoop在你<br>要和线程有更多的交互时才需要，比如以下情况：</p>\n<ul>\n<li>使用端口或者自定义输入源来和其他线程通信；</li>\n<li>使用线程的NSTimer (可以用dispatch_timer代替)</li>\n<li>Cocoa中使用任何performSelector</li>\n<li>让线程周期性干活(AFNetworking2.x)</li>\n</ul>\n<h3 id=\"CFRunLoop相关内容\"><a href=\"#CFRunLoop相关内容\" class=\"headerlink\" title=\"CFRunLoop相关内容\"></a>CFRunLoop相关内容</h3><h4 id=\"CoreFoundation中RunLoop的接口\"><a href=\"#CoreFoundation中RunLoop的接口\" class=\"headerlink\" title=\"CoreFoundation中RunLoop的接口\"></a>CoreFoundation中RunLoop的接口</h4><p>一般使用的是CoreFoundation中的CFRunLoop,因此这里主要总结CFRunLoop的对外接口.RunLoop中对外主要有以下几个接口类:</p>\n<ul>\n<li>CFRunLoopRef</li>\n<li>CFRunLoopModeRef (并未对外暴露)</li>\n<li>CFRunLoopSourceRef: 异步的source0和source1事件源<ul>\n<li>source0: 结构体中只有perform回调函数,它并不能主动触发事件,使用时,需要手动触发(<code>CFRunLoopSourceSignal()</code>).CFRunLoopSourceSignal将这个source标记为待处理，后调用CFRunLoopWakeUp来唤醒runLoop，让其处理这个事件(也就是source0无法唤醒RunLoop)</li>\n<li>source1: 结构体中有mach_port和invoke回调函数,通常是内核和其他进程给该RunLoop发送的消息,从源码可以看出source1,是可以自动唤醒RunLoop的,因为它是通过port方式发送事件的.</li>\n</ul>\n</li>\n<li>CFRunLoopTimerRef: 同步的TimerSource, 在Cocoa中是NSTimer.它的底层结构体中包含触发时间和回调，当其加入到runLoop中是，runLoop会注册对应的时间点，当时<br>间点到时,RunLoop会收到wakeUpPort(RunLoop的一个属性)source1类型的消息,将唤醒以执行Timer的回调；</li>\n<li>CFRunLoopObserverRef: 注册观察者时候可以添加一个回调函数,当RunLoop的状态变化时会通知注册的观察者,注册的回调函数在此时调用</li>\n</ul>\n<blockquote>\n<p>具体的demo可以参考:<br><a href=\"https://github.com/brownfeng/RunLoopDemo\">https://github.com/brownfeng/RunLoopDemo</a></p>\n<h4 id=\"RunLoop底层结构体\"><a href=\"#RunLoop底层结构体\" class=\"headerlink\" title=\"RunLoop底层结构体\"></a>RunLoop底层结构体</h4></blockquote>\n<p>CFRunLoop的结构如下</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">struct __CFRunLoop &#123;</div><div class=\"line\">    CFMutableSetRef _commonModes;     </div><div class=\"line\">    CFMutableSetRef _commonModeItems; </div><div class=\"line\">    CFRunLoopModeRef _currentMode;    </div><div class=\"line\">    CFMutableSetRef _modes;           </div><div class=\"line\">    ...</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>CFRunLoopMode的结构如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">struct __CFRunLoopMode &#123;</div><div class=\"line\">    CFStringRef _name;            </div><div class=\"line\">    CFMutableSetRef _sources0;    </div><div class=\"line\">    CFMutableSetRef _sources1;    </div><div class=\"line\">    CFMutableArrayRef _observers; </div><div class=\"line\">    CFMutableArrayRef _timers;    </div><div class=\"line\">    ...</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure>\n<p>其中，CFRunLoop对外暴露的管理Mode的接口有两个:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">CFRunLoopAddCommonMode</div><div class=\"line\">CFRunLoopRunInMode</div></pre></td></tr></table></figure>\n<p>Mode暴露的管理mode item的接口有下面几个：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">CFRunLoopAddSource</div><div class=\"line\">CFRunLoopAddObserver</div><div class=\"line\">CFRunLoopAddTimer</div><div class=\"line\">CFRunLoopRemoveSource</div><div class=\"line\">CFRunLoopRemoveObserver</div><div class=\"line\">CFRunLoopRemoveTimer</div></pre></td></tr></table></figure>\n<h3 id=\"Cocoa框架中RunLoop相关的内容\"><a href=\"#Cocoa框架中RunLoop相关的内容\" class=\"headerlink\" title=\"Cocoa框架中RunLoop相关的内容\"></a>Cocoa框架中RunLoop相关的内容</h3><h4 id=\"1-AutoReleasePool\"><a href=\"#1-AutoReleasePool\" class=\"headerlink\" title=\"1 AutoReleasePool\"></a>1 AutoReleasePool</h4><p>AutoReleasePool是Apple中清理临时变量,释放内容的机制,在app的main函数中,所有的内容都是包裹在一个AutoReleasePool中的.这个过程中,Apple在RunLoop中注册三个RunLoopObserver:</p>\n<ul>\n<li>第一个RunLoopObserver关注kCFRunLoopEntry状态,callback函数中会自动创建autoReleasePool,并且observer优先级最高(RunLoop第一个调用它的回调),保证应用启动后所有的操作都在autoreleasePool中运行.</li>\n<li>第二个RunLoopObserver关注kCFRunLoopBeforeWaiting状态,在此时autoreleasePool会释放旧的池,并创建一个新的autoreleasePool.</li>\n<li>第三个RunLoopObserver关注kCFRunLoopExit状态,此时释放autoreleasePool,并且保证observer的优先级最低,即在所有其他的observers都执行完以后才执行autorelease相关的observer的callback函数.</li>\n</ul>\n<blockquote>\n<p>深入学习可以参考:<br><a href=\"http://blog.sunnyxx.com/2014/10/15/behind-autorelease/\">http://blog.sunnyxx.com/2014/10/15/behind-autorelease/</a></p>\n</blockquote>\n<h4 id=\"2-iOS事件响应\"><a href=\"#2-iOS事件响应\" class=\"headerlink\" title=\"2 iOS事件响应\"></a>2 iOS事件响应</h4><p>RunLoop解释了为何iOS应用能够接受到屏幕触摸等事件.Apple在iOS app启动时候在main RunLoop中注册一个Source1事件,是基于Mach Port(进程间通信)的系统层面的进程.</p>\n<p>当一个触摸事件发生以后,IOKit会生成一个IOHIDEvent事件,并由系统层面的SpringBoard接受按键(锁屏/静音),触摸,加速,接近传感器等几种Event,然后通过进程间通信的mach port发送给App的main RunLoop.然后在DoSource1中,source1的回调会触发,内部会调用系统<code>_UIApplicationHandleEventQueue()</code>进行事件分发.</p>\n<p><code>_UIApplicationHandleEventQueue()</code>方法会把IOHIDEvent处理,并包装成常见的UIEvent事件进行处理或分发,其中包括识别UIGesture/处理屏幕旋转/发送给UIWindow等.通常事件比如UIButton点击,touchesBegin/Move/End/Cancel等事件都是在这个Source1回调中完成.</p>\n<h4 id=\"3-手势识别GestureRecognizer\"><a href=\"#3-手势识别GestureRecognizer\" class=\"headerlink\" title=\"3 手势识别GestureRecognizer\"></a>3 手势识别GestureRecognizer</h4><p>在上面的事件响应中,回调方法<code>_UIApplicationHandleEventQueue()</code>识别了一个手势以后,首先会调用Cancel将当前的touchesBegin/Move/End系列的回调打断.随后系统会将UIGestureRecognizer标记为等待处理.</p>\n<p>然后苹果会在RunLoop中注册一个Observer观察kCFRunLoopBeforeWaiting(睡眠前).这个观察者的回调函数是<code>_UIGestureRecognizerUpdateObserver()</code>,它会获取刚才所有被标记为等待处理等的GestureRecognizer,并且执行GestureRecognizer的回调.</p>\n<h4 id=\"4-界面更新-UI-update\"><a href=\"#4-界面更新-UI-update\" class=\"headerlink\" title=\"4 界面更新(UI update)\"></a>4 界面更新(UI update)</h4><p>在App改变UI时,例如修改view的Frame,更新UIView/CALayer的层次,或者手动调用UIView/CALayer的setNeedsLayout/setNeedsDisplay方法以后,这个UIView/CALayer就被标记为等待处理,并且提交到一个全局容器.</p>\n<p>然后苹果在RunLoop中注册一个Observer观察kCFRunLoopBeforeWaiting和kCFRunLoopExit状态.在状态触发DoObserver的callback中会调用函数<code>_ZN2CA11Transaction17observer_callbackEP19__CFRunLoopObservermPv()</code>.这个函数中会遍历所有的等待处理的UIView/CALayer,执行绘制和调整,更新UI界面.</p>\n<p>这个函数的调用栈如下:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">_ZN2CA11Transaction17observer_callbackEP19__CFRunLoopObservermPv()</div><div class=\"line\">    QuartzCore:CA::Transaction::observer_callback://DoObserver</div><div class=\"line\">        CA::Transaction::commit();//提交到全局容器</div><div class=\"line\">            CA::Context::commit_transaction();</div><div class=\"line\">                CA::Layer::layout_and_display_if_needed();</div><div class=\"line\">                    CA::Layer::layout_if_needed();</div><div class=\"line\">                        [CALayer layoutSublayers];</div><div class=\"line\">                            [UIView layoutSubviews];</div><div class=\"line\">                    CA::Layer::display_if_needed();</div><div class=\"line\">                        [CALayer display];</div><div class=\"line\">                            [UIView drawRect];</div></pre></td></tr></table></figure>\n<h4 id=\"5-NSTimer定时器\"><a href=\"#5-NSTimer定时器\" class=\"headerlink\" title=\"5 NSTimer定时器\"></a>5 NSTimer定时器</h4><p>NSTimer的CF层是CFRunLoopTimerRef,它们可以toll-free bridged.当使用<code>CFRunLoopAddTimer()</code>将NSTimer&amp;CFRunLoopTimerRef注册到RunLoop中以后,RunLoop会为它重复的时间点注册好事件(有一定的时间容忍度,触发的时间误差).如果中间某次触发被错过,那么这次触发时间点的回调也会被跳过去. 实际中NSTimer的触发是通过source1触发的,如果timer被触发会将RunLoop的有一个Mach Port-<code>timePort</code>-发消息,然后会唤醒RunLoop,调用DoTimers.可以参考上面的RunLoop运行流程.</p>\n<blockquote>\n<p>CADisplayLink 是一个和屏幕刷新一致的定时器,如果两次屏幕刷新时候在执行一个长时间任务,那其中就会有一帧被跳过,造成界面卡顿.Facebook的AsyncDisplayLink使用RunLoop来解决丢帧的问题.</p>\n<p>还有一个GCD的定时器<code>dispatch_timer</code>,RunLoop的超时机制也是使用<code>dispatch_timer</code>.</p>\n</blockquote>\n<h4 id=\"6-部分PerformSelector事件源\"><a href=\"#6-部分PerformSelector事件源\" class=\"headerlink\" title=\"6 部分PerformSelector事件源\"></a>6 部分PerformSelector事件源</h4><p>NSObject的部分与时间相关的PerformSelector方法与RunLoop密切相关.当调用NSObject的performSelector:afterDelay以后,实际内部会创建一个Timer并添加RunLoopMode中,等待Timer触发,然后调用DoTimers,其中包含的回调方法就是selector.因此如果当前线程没有开启RunLoop,这个方法无效.类似的NSObject的performSelector:onThread:也是类似,需要线程开启RunLoop.具体涉及到的perform方法包括:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">- performSelector:afterDelay:</div><div class=\"line\">– performSelector:withObject:afterDelay:</div><div class=\"line\">– performSelectorOnMainThread:withObject:waitUntilDone:</div><div class=\"line\">– performSelectorOnMainThread:withObject:waitUntilDone:modes:</div><div class=\"line\">– performSelector:onThread:withObject:waitUntilDone:modes:</div><div class=\"line\">- performSelector:onThread:withObject:waitUntilDone:</div></pre></td></tr></table></figure>\n<h4 id=\"7-GCD与RunLoop\"><a href=\"#7-GCD与RunLoop\" class=\"headerlink\" title=\"7 GCD与RunLoop\"></a>7 GCD与RunLoop</h4><p>GCD的中部分接口使用了RunLoop,提交到mainQueue的blocks.当调用<code>dispatch_async(dispatch_get_main_queue(), block)</code>时,libDispatch(系统的某个进程)会向app的进程的main runloop的dispatchPort(是一个mach port)发送消息.此时RunLoop如果在休眠状态,会被唤醒,并从dispatch port中取出消息,并在会在回调方法<code>__CFRUNLOOP_IS_SERVICING_THE_MAIN_DISPATCH_QUEUE__()</code>中执行这个block.</p>\n<blockquote>\n<p>libDispatch分发到main queue的block才会与RunLoop有关,分发到其他的子线程的内容,还是又libDispatch处理.</p>\n<p>RunLoop的运行超时是由GCD的dispatch_timer控制的.</p>\n</blockquote>\n<h4 id=\"8-关于网络请求\"><a href=\"#8-关于网络请求\" class=\"headerlink\" title=\"8 关于网络请求\"></a>8 关于网络请求</h4><p>关于网络请求的接口，主要有以下几层:</p>\n<ul>\n<li>CFSocket:是最底层的接口，只负责socket的通信</li>\n<li>CFNetwork:是基于CFSocket等接口的上层封装,ASIHttpRequest工作在这层</li>\n<li>NSURLConnection:是基于CFNetwork的更高层的封装，提供面向对象的接口，<br>AFNetworking2.x工作于这一层</li>\n<li>NSURLSession:是ios7中新增的接口,表面上和NSURLConnection并列,但底层<br>仍然用到NSURLConnection的部分功能,AFNetworking3.x和Alamofire在这层</li>\n</ul>\n<p>因此,现在使用的大多网络库都与CFSocket,CFNetwork层相关.</p>\n<p>通常使用NSURLConnection时,你会传入一个Delegate,当调用[connection start]后，这个Delegate就会不停收到事件回调。实际上,start 这个函数的内部会<br>会获取 CurrentRunLoop，然后在其中的 DefaultMode 添加了4个 Source0 (即需要手动<br>触发的Source)。CFMultiplexerSource 是负责各种 Delegate 回调的，CFHTTPCookie<br>Storage 是处理各种 Cookie 的。</p>\n<blockquote>\n<p>NSURLConnection的工作过程可以参考:<br><a href=\"http://blog.ibireme.com/2015/05/18/runloop/#base\">http://blog.ibireme.com/2015/05/18/runloop/#base</a></p>\n</blockquote>\n<h3 id=\"RunLoop的应用举例\"><a href=\"#RunLoop的应用举例\" class=\"headerlink\" title=\"RunLoop的应用举例\"></a>RunLoop的应用举例</h3><h4 id=\"1-AFNetworking2-x\"><a href=\"#1-AFNetworking2-x\" class=\"headerlink\" title=\"1 AFNetworking2.x\"></a>1 AFNetworking2.x</h4><p>使用NSURLConnection时候,需要在后台自定义线程中接受Delegate回调.AFNetworking创建一个自定义线程,然后在其中添加一个Mach Port,然后启动RunLoop(前面提到过,如果RunLoop中没有source/timer/observer会立即退出).在以后线程以后通过NSObject的<code>performSelector:onThread:withObject:waitUnitlDone:modes:</code>方法,将<code>[self.connection scheduleInRunLoop:runLoop forMode:runLoopMode]</code>.</p>\n<blockquote>\n<p>可以参考:<br><a href=\"https://github.com/brownfeng/SourceSet/tree/master/AFNetworking2.x\">https://github.com/brownfeng/SourceSet/tree/master/AFNetworking2.x</a></p>\n</blockquote>\n<h4 id=\"2-AsyncDisplayKit\"><a href=\"#2-AsyncDisplayKit\" class=\"headerlink\" title=\"2 AsyncDisplayKit\"></a>2 AsyncDisplayKit</h4><p>AsyncDisplayKit是Facebook推出的用于保持界面流畅性的框架,其原理大致如下：</p>\n<p>UI线程中一旦出现繁重的任务就会导致界面卡顿,这类任务通常分为3类:排版,绘制,UI对象操作。通过各种方法将前两种任务丢到后台运行,最后一类操作只能在主线程中执行,因此,ASDK仿照QuartzCore/UIKit框架的模式,实现了一套类似的界面更新的机制:即在主线程的RunLoop中添加一个 Observer,监听kCFRunLoopBeforeWaiting和 kCFRunLoopExit事件,在收到回调时,遍历所有之前放入队列的待处理的任务,然后一一执行。</p>\n<h4 id=\"3-NSTimer在TrackingMode运行\"><a href=\"#3-NSTimer在TrackingMode运行\" class=\"headerlink\" title=\"3 NSTimer在TrackingMode运行\"></a>3 NSTimer在TrackingMode运行</h4><h4 id=\"4-TableView延迟加载图片\"><a href=\"#4-TableView延迟加载图片\" class=\"headerlink\" title=\"4 TableView延迟加载图片\"></a>4 TableView延迟加载图片</h4><h4 id=\"5-RunLoop解决大图加载的问题\"><a href=\"#5-RunLoop解决大图加载的问题\" class=\"headerlink\" title=\"5 RunLoop解决大图加载的问题\"></a>5 RunLoop解决大图加载的问题</h4><h4 id=\"6-RunLoop监控App卡顿\"><a href=\"#6-RunLoop监控App卡顿\" class=\"headerlink\" title=\"6 RunLoop监控App卡顿\"></a>6 RunLoop监控App卡顿</h4><blockquote>\n<p>可以参考文章:</p>\n<p><a href=\"http://www.jianshu.com/p/929d855c5a5a\">http://www.jianshu.com/p/929d855c5a5a</a><br><a href=\"http://www.jianshu.com/p/924cb2b218f5\">http://www.jianshu.com/p/924cb2b218f5</a></p>\n</blockquote>\n<h3 id=\"参考内容\"><a href=\"#参考内容\" class=\"headerlink\" title=\"参考内容\"></a>参考内容</h3><p><a href=\"http://blog.ibireme.com/2015/05/18/runloop/\">http://blog.ibireme.com/2015/05/18/runloop/</a></p>\n<p><a href=\"http://yun.baidu.com/share/link?shareid=2268593032&amp;uk=2885973690\">http://yun.baidu.com/share/link?shareid=2268593032&amp;uk=2885973690</a></p>\n"},{"title":"iOS音频系列(一)--音频基础","_content":"\n前些日子由于项目需要,一直在研究iOS CoreAudio相关的内容.在这里记录一些笔记.现实生活中，我们听到的声音都是时间连续的，我们称为这种信号叫模拟信号。模拟信号需要进行数字化以后才能在计算机中使用。数字化的过程如下：\n\n`采样 -> 量化 -> 编码`\n\n通过获取间隔相同时间的某个模拟信号的值，然后对这些采样以后得到的值进行量化，然后使用一定的bit进行编码存储，整个过程结束后就会输出PCM数据。在iOS的Core Audio Services中使用的音频数据只能是线性PCM格式的音频数据，这是一种未进过压缩的音频数据格式。要理解整个过程就需要理解多个重要概念：采样频率和采样位数，比特率等。\n\n### 采样频率\n\n采样频率是指单位时间内对声音模拟信号的采样次数。采样率类似于视频的帧数，比如电影的采样率是24Hz。当我们把采样到的一个个静止画面再以采样率同样的速度回放时，看到的就是连续的画面。同样的道理，把以44.1kHZ采样率记录的CD以同样的速率播放时，就能听到连续的声音。显然，这个采样率越高，听到的声音和看到的图像就越连贯。当然，人的听觉和视觉器官能分辨的采样率是有限的。对同一段声音，用20kHz和44.1kHz来采样，重放时，可能可以听出其中的差别，而基本上高于44.1kHZ采样的声音，比如说96kHz采样，绝大部分人已经觉察不到两种采样出来的声音的分别了。之所以使用44.1kHZ这个数值是因为经过了反复实验，人们发现这个采样精度最合适，低于这个值就会有较明显的损失，而高于这个值人的耳朵已经很难分辨，而且增大了数字音频所占用的空间。我们所使用的CD的采样标准就是44.1k。\n\n### 采样位数\n\n采样位数可以理解为采集卡处理声音的解析度。这个数值越大，解析度就越高，录制和回放的声音就越真实。我们首先要知道：电脑中的声音文件是用数字0和1来表示的。连续的模拟信号按一定的采样频率经数码脉冲取样后，每一个离散的脉冲信号被以一定的量化精度量化成一串二进制编码流，这串编码流的位数即为采样位数，也称为量化精度。\n\n在电脑上录音的本质就是把模拟声音信号转换成数字信号。反之，在播放时则是把数字信号还原成模拟声音信号输出。采集卡的位是指采集卡在采集和播放声音文件时所使用数字声音信号的二进制位数。采集卡的位客观地反映了数字声音信号对输入声音信号描述的准确程度。例如，同一段音频信息，使用8bit描述单个采样信息，那么采样量化的范围就是0~255,如果使用16bit表示单个采样值,那么相应的采样量化的范围为0~64k。与8位采样位数相比，16位采样的动态范围的宽度更小，动态范围更宽广，声音的被记录的更加精细。一般CD使用的采样位数为16位。\n\n16位二进制数的最小值是0000000000000000，最大值是1111111111111111，对应的十进制数就是0和65535，也就是最大和最小值之间的差值是65535，也就是说，它量化的模拟量的动态范围可以差65535，也就是96.32分贝（20 * lg65535）），所以，量化精度只和动态范围有关，和频率响应没关系。动态范围定在96分贝也是有道理的，人耳的无痛苦极限声压是90分贝，96分贝的动态范围在普通应用中足够使用，所以96分贝动态范围内的模拟波，经量化后，不会产生削波失真的。\n\n>所谓分贝是指两个相同的物理量（例A1和A0）之比取以10为底的对数并乘以10（或20）。N = 10lg(A1/A0) 分贝符号为\"dB\"，它是无量纲的。式中A0是基准量（或参考量），A是被量度量。被量度量和基准量之比取对数，这对数值称为被量度量的\"级\"。亦即用对数标度时，所得到的是比值，它代表被量度量比基准量高出多少\"级\"。\n\n### 位速/比特率/码率\n\n位速/比特率/码率描述的都是一个东西，是指在一个数据流中每秒钟能通过的信息量。我们可能看到过音频文件用 “128–Kbps MP3” 或 “64–Kbps WMA” 进行描述的情形。Kbps 表示 “每秒千位数”，因此数值越大表示数据越多：128–Kbps MP3 音频文件包含的数据量是 64–Kbps WMA 文件的两倍，并占用两倍的空间。（不过在这种情况下，这两种文件听起来没什么两样。原因是什么呢？有些文件格式比其他文件能够更有效地利用数据， 64–Kbps WMA 文件的音质与 128–Kbps MP3 的音质相同。）需要了解的重要一点是，位速越高，信息量越大，对这些信息进行解码的处理量就越大，文件需要占用的空间也就越多。\n\n从码率的计算公式中可以清楚的看出码率和采样位数的关系:\n\n`码率=取样频率×量化精度×声道数`\n\n\n>一张CD,双声道,采样率44.1kHz，每个采样位数13bit，时长74分钟(4440秒)，则CD的容量为`13*2*44100*4440`约等于`640MB`。\n\n### VBR、ABR、CBR\n\nVBR(Variable Bitrate)动态比特率。也就是没有固定的比特率，压缩软件在压缩时根据音频数据即时确定使用什么比特率。这是新发展的算法，他们将一首歌的复杂部分用高Bitrate编码，简单部分用低Bitrate编码。主意虽然不错，可惜新编码器的VBR算法很差，音质与CBR相去甚远。幸运的是， Lame完美地优化了VBR算法，使之成为MP3的最佳编码模式。这是以质量为前提兼顾文件大小的方式，推荐编码模式。\n\nABR(Average Bitrate)平均比特率，是VBR的一种插值参数。Lame针对CBR不佳的文件体积比和VBR生成文件大小不定的特点独创了这种编码模式。ABR也被称为“Safe VBR”，它是在指定的平均Bitrate内，以每50帧(30帧约1秒)为一段，低频和不敏感频率使用相对低的流量，高频和大动态表现时使用高流量。举例来说，当指定用192kbps ABR对一段wav文件进行编码时，Lame会将该文件的85%用192kbps固定编码，然后对剩余15%进行动态优化：复杂部分用高于192kbps 来编码、简单部分用低于192kbps来编码。与192kbps CBR相比，192kbps ABR在文件大小上相差不多，音质却提高不少。ABR编码在速度上是VBR编码的2到3倍，在128-256kbps范围内质量要好于CBR。可以做为 VBR和CBR的一种折衷选择。\n\nCBR(Constant Bitrate)，常数比特率，指文件从头到尾都是一种位速率。相对于VBR和ABR来讲，它压缩出来的文件体积很大，但音质却不会有明显的提高。\n\n### PCM格式与LPCM格式\n\nPCM（脉冲编码调制）是一种将模拟语音信号变换为数字信号的编码方式。主要经过3个过程：抽样、量化和编码。抽样过程将连续时间模拟信号变为离散时间、连续幅度的抽样信号，量化过程将抽样信号变为离散时间、离散幅度的数字信号，编码过程将量化后的信号编码成为一个二进制码组输出。\n\n量化分为线性量化和非线性量化。线性量化在整个量化范围内，量化间隔均相等，称为LPCM。非线性量化采用不等的量化间隔。量化间隔数由编码的二进制位数决定。例如，CD采用16bit线性量化，则量化间隔数L=65536。位数（n)越多，精度越高，信噪比`SNR=6.02n+1.76`(dB)也越高。但编码的二进制位数不是无限制的，需要根据所需的数据率确定。比如：CD可以达到的数据率为2×44.1×16=1411.2Kbit/s。\n\n总而言之，LPCM格式中的音频数据是未压缩的线性量化后的音频数据。\n\n>用iOS的官方文档中对几个关键词的解释：\n>\n>* A sample is single numerical value for a single channel.\n* A frame is a collection of time-coincident samples. For instance, a stereo sound file has two samples per frame, one for the left channel and one for the right channel.\n* A packet is a collection of one or more contiguous frames. In linear PCM audio, a packet is always a single frame. In compressed formats, it is typically more. A packet defines the smallest meaningful set of frames for a given audio data format.\n\n### 压缩过的音频格式\n\n在常见的音频格式对PCM原始帧进行封装时也是以frame帧为单位的，我们一般将压缩后的音频数据帧称为媒体帧，对应原始的PCM数据称为原始帧。每个媒体帧又分成head头，body数据体。在帧头中，会存储这个媒体帧中body部分的码率，采样率等解码必须的信息，因此每一个媒体帧都可以独立于文件存在和播放。在body中存储着一个或者多个媒体帧，这些媒体真是若干个PCM原始帧经过特定的压缩算法压缩得到的。通常情况下，我们将单位时间的媒体帧的个数称为帧率。\n\n上文的采样率和帧率这两个概念都描述了音频媒体的“连续”性，二者的区别在于每个音频的媒体帧中会包含多个音频采样(多个PCM data)，如1个AAC帧中包含1024个采样。\n\n> 在学习音频/视频相关内容之前,首先需要弄清楚的的是音频的文件类型和音频格式是有本质区别的.封装类型比如.ogg,音频格式比如.mp3.(具体的区别可以百度)","source":"_posts/iOS音频系列(一).md","raw":"---\ntitle: iOS音频系列(一)--音频基础\ntags:\n- iOS\n- CoreAudio\n---\n\n前些日子由于项目需要,一直在研究iOS CoreAudio相关的内容.在这里记录一些笔记.现实生活中，我们听到的声音都是时间连续的，我们称为这种信号叫模拟信号。模拟信号需要进行数字化以后才能在计算机中使用。数字化的过程如下：\n\n`采样 -> 量化 -> 编码`\n\n通过获取间隔相同时间的某个模拟信号的值，然后对这些采样以后得到的值进行量化，然后使用一定的bit进行编码存储，整个过程结束后就会输出PCM数据。在iOS的Core Audio Services中使用的音频数据只能是线性PCM格式的音频数据，这是一种未进过压缩的音频数据格式。要理解整个过程就需要理解多个重要概念：采样频率和采样位数，比特率等。\n\n### 采样频率\n\n采样频率是指单位时间内对声音模拟信号的采样次数。采样率类似于视频的帧数，比如电影的采样率是24Hz。当我们把采样到的一个个静止画面再以采样率同样的速度回放时，看到的就是连续的画面。同样的道理，把以44.1kHZ采样率记录的CD以同样的速率播放时，就能听到连续的声音。显然，这个采样率越高，听到的声音和看到的图像就越连贯。当然，人的听觉和视觉器官能分辨的采样率是有限的。对同一段声音，用20kHz和44.1kHz来采样，重放时，可能可以听出其中的差别，而基本上高于44.1kHZ采样的声音，比如说96kHz采样，绝大部分人已经觉察不到两种采样出来的声音的分别了。之所以使用44.1kHZ这个数值是因为经过了反复实验，人们发现这个采样精度最合适，低于这个值就会有较明显的损失，而高于这个值人的耳朵已经很难分辨，而且增大了数字音频所占用的空间。我们所使用的CD的采样标准就是44.1k。\n\n### 采样位数\n\n采样位数可以理解为采集卡处理声音的解析度。这个数值越大，解析度就越高，录制和回放的声音就越真实。我们首先要知道：电脑中的声音文件是用数字0和1来表示的。连续的模拟信号按一定的采样频率经数码脉冲取样后，每一个离散的脉冲信号被以一定的量化精度量化成一串二进制编码流，这串编码流的位数即为采样位数，也称为量化精度。\n\n在电脑上录音的本质就是把模拟声音信号转换成数字信号。反之，在播放时则是把数字信号还原成模拟声音信号输出。采集卡的位是指采集卡在采集和播放声音文件时所使用数字声音信号的二进制位数。采集卡的位客观地反映了数字声音信号对输入声音信号描述的准确程度。例如，同一段音频信息，使用8bit描述单个采样信息，那么采样量化的范围就是0~255,如果使用16bit表示单个采样值,那么相应的采样量化的范围为0~64k。与8位采样位数相比，16位采样的动态范围的宽度更小，动态范围更宽广，声音的被记录的更加精细。一般CD使用的采样位数为16位。\n\n16位二进制数的最小值是0000000000000000，最大值是1111111111111111，对应的十进制数就是0和65535，也就是最大和最小值之间的差值是65535，也就是说，它量化的模拟量的动态范围可以差65535，也就是96.32分贝（20 * lg65535）），所以，量化精度只和动态范围有关，和频率响应没关系。动态范围定在96分贝也是有道理的，人耳的无痛苦极限声压是90分贝，96分贝的动态范围在普通应用中足够使用，所以96分贝动态范围内的模拟波，经量化后，不会产生削波失真的。\n\n>所谓分贝是指两个相同的物理量（例A1和A0）之比取以10为底的对数并乘以10（或20）。N = 10lg(A1/A0) 分贝符号为\"dB\"，它是无量纲的。式中A0是基准量（或参考量），A是被量度量。被量度量和基准量之比取对数，这对数值称为被量度量的\"级\"。亦即用对数标度时，所得到的是比值，它代表被量度量比基准量高出多少\"级\"。\n\n### 位速/比特率/码率\n\n位速/比特率/码率描述的都是一个东西，是指在一个数据流中每秒钟能通过的信息量。我们可能看到过音频文件用 “128–Kbps MP3” 或 “64–Kbps WMA” 进行描述的情形。Kbps 表示 “每秒千位数”，因此数值越大表示数据越多：128–Kbps MP3 音频文件包含的数据量是 64–Kbps WMA 文件的两倍，并占用两倍的空间。（不过在这种情况下，这两种文件听起来没什么两样。原因是什么呢？有些文件格式比其他文件能够更有效地利用数据， 64–Kbps WMA 文件的音质与 128–Kbps MP3 的音质相同。）需要了解的重要一点是，位速越高，信息量越大，对这些信息进行解码的处理量就越大，文件需要占用的空间也就越多。\n\n从码率的计算公式中可以清楚的看出码率和采样位数的关系:\n\n`码率=取样频率×量化精度×声道数`\n\n\n>一张CD,双声道,采样率44.1kHz，每个采样位数13bit，时长74分钟(4440秒)，则CD的容量为`13*2*44100*4440`约等于`640MB`。\n\n### VBR、ABR、CBR\n\nVBR(Variable Bitrate)动态比特率。也就是没有固定的比特率，压缩软件在压缩时根据音频数据即时确定使用什么比特率。这是新发展的算法，他们将一首歌的复杂部分用高Bitrate编码，简单部分用低Bitrate编码。主意虽然不错，可惜新编码器的VBR算法很差，音质与CBR相去甚远。幸运的是， Lame完美地优化了VBR算法，使之成为MP3的最佳编码模式。这是以质量为前提兼顾文件大小的方式，推荐编码模式。\n\nABR(Average Bitrate)平均比特率，是VBR的一种插值参数。Lame针对CBR不佳的文件体积比和VBR生成文件大小不定的特点独创了这种编码模式。ABR也被称为“Safe VBR”，它是在指定的平均Bitrate内，以每50帧(30帧约1秒)为一段，低频和不敏感频率使用相对低的流量，高频和大动态表现时使用高流量。举例来说，当指定用192kbps ABR对一段wav文件进行编码时，Lame会将该文件的85%用192kbps固定编码，然后对剩余15%进行动态优化：复杂部分用高于192kbps 来编码、简单部分用低于192kbps来编码。与192kbps CBR相比，192kbps ABR在文件大小上相差不多，音质却提高不少。ABR编码在速度上是VBR编码的2到3倍，在128-256kbps范围内质量要好于CBR。可以做为 VBR和CBR的一种折衷选择。\n\nCBR(Constant Bitrate)，常数比特率，指文件从头到尾都是一种位速率。相对于VBR和ABR来讲，它压缩出来的文件体积很大，但音质却不会有明显的提高。\n\n### PCM格式与LPCM格式\n\nPCM（脉冲编码调制）是一种将模拟语音信号变换为数字信号的编码方式。主要经过3个过程：抽样、量化和编码。抽样过程将连续时间模拟信号变为离散时间、连续幅度的抽样信号，量化过程将抽样信号变为离散时间、离散幅度的数字信号，编码过程将量化后的信号编码成为一个二进制码组输出。\n\n量化分为线性量化和非线性量化。线性量化在整个量化范围内，量化间隔均相等，称为LPCM。非线性量化采用不等的量化间隔。量化间隔数由编码的二进制位数决定。例如，CD采用16bit线性量化，则量化间隔数L=65536。位数（n)越多，精度越高，信噪比`SNR=6.02n+1.76`(dB)也越高。但编码的二进制位数不是无限制的，需要根据所需的数据率确定。比如：CD可以达到的数据率为2×44.1×16=1411.2Kbit/s。\n\n总而言之，LPCM格式中的音频数据是未压缩的线性量化后的音频数据。\n\n>用iOS的官方文档中对几个关键词的解释：\n>\n>* A sample is single numerical value for a single channel.\n* A frame is a collection of time-coincident samples. For instance, a stereo sound file has two samples per frame, one for the left channel and one for the right channel.\n* A packet is a collection of one or more contiguous frames. In linear PCM audio, a packet is always a single frame. In compressed formats, it is typically more. A packet defines the smallest meaningful set of frames for a given audio data format.\n\n### 压缩过的音频格式\n\n在常见的音频格式对PCM原始帧进行封装时也是以frame帧为单位的，我们一般将压缩后的音频数据帧称为媒体帧，对应原始的PCM数据称为原始帧。每个媒体帧又分成head头，body数据体。在帧头中，会存储这个媒体帧中body部分的码率，采样率等解码必须的信息，因此每一个媒体帧都可以独立于文件存在和播放。在body中存储着一个或者多个媒体帧，这些媒体真是若干个PCM原始帧经过特定的压缩算法压缩得到的。通常情况下，我们将单位时间的媒体帧的个数称为帧率。\n\n上文的采样率和帧率这两个概念都描述了音频媒体的“连续”性，二者的区别在于每个音频的媒体帧中会包含多个音频采样(多个PCM data)，如1个AAC帧中包含1024个采样。\n\n> 在学习音频/视频相关内容之前,首先需要弄清楚的的是音频的文件类型和音频格式是有本质区别的.封装类型比如.ogg,音频格式比如.mp3.(具体的区别可以百度)","slug":"iOS音频系列(一)","published":1,"date":"2016-08-26T16:19:12.000Z","updated":"2016-08-26T16:19:12.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cisbyxx8800096squjp73s8x3","content":"<p>前些日子由于项目需要,一直在研究iOS CoreAudio相关的内容.在这里记录一些笔记.现实生活中，我们听到的声音都是时间连续的，我们称为这种信号叫模拟信号。模拟信号需要进行数字化以后才能在计算机中使用。数字化的过程如下：</p>\n<p><code>采样 -&gt; 量化 -&gt; 编码</code></p>\n<p>通过获取间隔相同时间的某个模拟信号的值，然后对这些采样以后得到的值进行量化，然后使用一定的bit进行编码存储，整个过程结束后就会输出PCM数据。在iOS的Core Audio Services中使用的音频数据只能是线性PCM格式的音频数据，这是一种未进过压缩的音频数据格式。要理解整个过程就需要理解多个重要概念：采样频率和采样位数，比特率等。</p>\n<h3 id=\"采样频率\"><a href=\"#采样频率\" class=\"headerlink\" title=\"采样频率\"></a>采样频率</h3><p>采样频率是指单位时间内对声音模拟信号的采样次数。采样率类似于视频的帧数，比如电影的采样率是24Hz。当我们把采样到的一个个静止画面再以采样率同样的速度回放时，看到的就是连续的画面。同样的道理，把以44.1kHZ采样率记录的CD以同样的速率播放时，就能听到连续的声音。显然，这个采样率越高，听到的声音和看到的图像就越连贯。当然，人的听觉和视觉器官能分辨的采样率是有限的。对同一段声音，用20kHz和44.1kHz来采样，重放时，可能可以听出其中的差别，而基本上高于44.1kHZ采样的声音，比如说96kHz采样，绝大部分人已经觉察不到两种采样出来的声音的分别了。之所以使用44.1kHZ这个数值是因为经过了反复实验，人们发现这个采样精度最合适，低于这个值就会有较明显的损失，而高于这个值人的耳朵已经很难分辨，而且增大了数字音频所占用的空间。我们所使用的CD的采样标准就是44.1k。</p>\n<h3 id=\"采样位数\"><a href=\"#采样位数\" class=\"headerlink\" title=\"采样位数\"></a>采样位数</h3><p>采样位数可以理解为采集卡处理声音的解析度。这个数值越大，解析度就越高，录制和回放的声音就越真实。我们首先要知道：电脑中的声音文件是用数字0和1来表示的。连续的模拟信号按一定的采样频率经数码脉冲取样后，每一个离散的脉冲信号被以一定的量化精度量化成一串二进制编码流，这串编码流的位数即为采样位数，也称为量化精度。</p>\n<p>在电脑上录音的本质就是把模拟声音信号转换成数字信号。反之，在播放时则是把数字信号还原成模拟声音信号输出。采集卡的位是指采集卡在采集和播放声音文件时所使用数字声音信号的二进制位数。采集卡的位客观地反映了数字声音信号对输入声音信号描述的准确程度。例如，同一段音频信息，使用8bit描述单个采样信息，那么采样量化的范围就是0~255,如果使用16bit表示单个采样值,那么相应的采样量化的范围为0~64k。与8位采样位数相比，16位采样的动态范围的宽度更小，动态范围更宽广，声音的被记录的更加精细。一般CD使用的采样位数为16位。</p>\n<p>16位二进制数的最小值是0000000000000000，最大值是1111111111111111，对应的十进制数就是0和65535，也就是最大和最小值之间的差值是65535，也就是说，它量化的模拟量的动态范围可以差65535，也就是96.32分贝（20 * lg65535）），所以，量化精度只和动态范围有关，和频率响应没关系。动态范围定在96分贝也是有道理的，人耳的无痛苦极限声压是90分贝，96分贝的动态范围在普通应用中足够使用，所以96分贝动态范围内的模拟波，经量化后，不会产生削波失真的。</p>\n<blockquote>\n<p>所谓分贝是指两个相同的物理量（例A1和A0）之比取以10为底的对数并乘以10（或20）。N = 10lg(A1/A0) 分贝符号为”dB”，它是无量纲的。式中A0是基准量（或参考量），A是被量度量。被量度量和基准量之比取对数，这对数值称为被量度量的”级”。亦即用对数标度时，所得到的是比值，它代表被量度量比基准量高出多少”级”。</p>\n</blockquote>\n<h3 id=\"位速-比特率-码率\"><a href=\"#位速-比特率-码率\" class=\"headerlink\" title=\"位速/比特率/码率\"></a>位速/比特率/码率</h3><p>位速/比特率/码率描述的都是一个东西，是指在一个数据流中每秒钟能通过的信息量。我们可能看到过音频文件用 “128–Kbps MP3” 或 “64–Kbps WMA” 进行描述的情形。Kbps 表示 “每秒千位数”，因此数值越大表示数据越多：128–Kbps MP3 音频文件包含的数据量是 64–Kbps WMA 文件的两倍，并占用两倍的空间。（不过在这种情况下，这两种文件听起来没什么两样。原因是什么呢？有些文件格式比其他文件能够更有效地利用数据， 64–Kbps WMA 文件的音质与 128–Kbps MP3 的音质相同。）需要了解的重要一点是，位速越高，信息量越大，对这些信息进行解码的处理量就越大，文件需要占用的空间也就越多。</p>\n<p>从码率的计算公式中可以清楚的看出码率和采样位数的关系:</p>\n<p><code>码率=取样频率×量化精度×声道数</code></p>\n<blockquote>\n<p>一张CD,双声道,采样率44.1kHz，每个采样位数13bit，时长74分钟(4440秒)，则CD的容量为<code>13*2*44100*4440</code>约等于<code>640MB</code>。</p>\n</blockquote>\n<h3 id=\"VBR、ABR、CBR\"><a href=\"#VBR、ABR、CBR\" class=\"headerlink\" title=\"VBR、ABR、CBR\"></a>VBR、ABR、CBR</h3><p>VBR(Variable Bitrate)动态比特率。也就是没有固定的比特率，压缩软件在压缩时根据音频数据即时确定使用什么比特率。这是新发展的算法，他们将一首歌的复杂部分用高Bitrate编码，简单部分用低Bitrate编码。主意虽然不错，可惜新编码器的VBR算法很差，音质与CBR相去甚远。幸运的是， Lame完美地优化了VBR算法，使之成为MP3的最佳编码模式。这是以质量为前提兼顾文件大小的方式，推荐编码模式。</p>\n<p>ABR(Average Bitrate)平均比特率，是VBR的一种插值参数。Lame针对CBR不佳的文件体积比和VBR生成文件大小不定的特点独创了这种编码模式。ABR也被称为“Safe VBR”，它是在指定的平均Bitrate内，以每50帧(30帧约1秒)为一段，低频和不敏感频率使用相对低的流量，高频和大动态表现时使用高流量。举例来说，当指定用192kbps ABR对一段wav文件进行编码时，Lame会将该文件的85%用192kbps固定编码，然后对剩余15%进行动态优化：复杂部分用高于192kbps 来编码、简单部分用低于192kbps来编码。与192kbps CBR相比，192kbps ABR在文件大小上相差不多，音质却提高不少。ABR编码在速度上是VBR编码的2到3倍，在128-256kbps范围内质量要好于CBR。可以做为 VBR和CBR的一种折衷选择。</p>\n<p>CBR(Constant Bitrate)，常数比特率，指文件从头到尾都是一种位速率。相对于VBR和ABR来讲，它压缩出来的文件体积很大，但音质却不会有明显的提高。</p>\n<h3 id=\"PCM格式与LPCM格式\"><a href=\"#PCM格式与LPCM格式\" class=\"headerlink\" title=\"PCM格式与LPCM格式\"></a>PCM格式与LPCM格式</h3><p>PCM（脉冲编码调制）是一种将模拟语音信号变换为数字信号的编码方式。主要经过3个过程：抽样、量化和编码。抽样过程将连续时间模拟信号变为离散时间、连续幅度的抽样信号，量化过程将抽样信号变为离散时间、离散幅度的数字信号，编码过程将量化后的信号编码成为一个二进制码组输出。</p>\n<p>量化分为线性量化和非线性量化。线性量化在整个量化范围内，量化间隔均相等，称为LPCM。非线性量化采用不等的量化间隔。量化间隔数由编码的二进制位数决定。例如，CD采用16bit线性量化，则量化间隔数L=65536。位数（n)越多，精度越高，信噪比<code>SNR=6.02n+1.76</code>(dB)也越高。但编码的二进制位数不是无限制的，需要根据所需的数据率确定。比如：CD可以达到的数据率为2×44.1×16=1411.2Kbit/s。</p>\n<p>总而言之，LPCM格式中的音频数据是未压缩的线性量化后的音频数据。</p>\n<blockquote>\n<p>用iOS的官方文档中对几个关键词的解释：</p>\n<ul>\n<li>A sample is single numerical value for a single channel.</li>\n<li>A frame is a collection of time-coincident samples. For instance, a stereo sound file has two samples per frame, one for the left channel and one for the right channel.</li>\n<li>A packet is a collection of one or more contiguous frames. In linear PCM audio, a packet is always a single frame. In compressed formats, it is typically more. A packet defines the smallest meaningful set of frames for a given audio data format.</li>\n</ul>\n</blockquote>\n<h3 id=\"压缩过的音频格式\"><a href=\"#压缩过的音频格式\" class=\"headerlink\" title=\"压缩过的音频格式\"></a>压缩过的音频格式</h3><p>在常见的音频格式对PCM原始帧进行封装时也是以frame帧为单位的，我们一般将压缩后的音频数据帧称为媒体帧，对应原始的PCM数据称为原始帧。每个媒体帧又分成head头，body数据体。在帧头中，会存储这个媒体帧中body部分的码率，采样率等解码必须的信息，因此每一个媒体帧都可以独立于文件存在和播放。在body中存储着一个或者多个媒体帧，这些媒体真是若干个PCM原始帧经过特定的压缩算法压缩得到的。通常情况下，我们将单位时间的媒体帧的个数称为帧率。</p>\n<p>上文的采样率和帧率这两个概念都描述了音频媒体的“连续”性，二者的区别在于每个音频的媒体帧中会包含多个音频采样(多个PCM data)，如1个AAC帧中包含1024个采样。</p>\n<blockquote>\n<p>在学习音频/视频相关内容之前,首先需要弄清楚的的是音频的文件类型和音频格式是有本质区别的.封装类型比如.ogg,音频格式比如.mp3.(具体的区别可以百度)</p>\n</blockquote>\n","excerpt":"","more":"<p>前些日子由于项目需要,一直在研究iOS CoreAudio相关的内容.在这里记录一些笔记.现实生活中，我们听到的声音都是时间连续的，我们称为这种信号叫模拟信号。模拟信号需要进行数字化以后才能在计算机中使用。数字化的过程如下：</p>\n<p><code>采样 -&gt; 量化 -&gt; 编码</code></p>\n<p>通过获取间隔相同时间的某个模拟信号的值，然后对这些采样以后得到的值进行量化，然后使用一定的bit进行编码存储，整个过程结束后就会输出PCM数据。在iOS的Core Audio Services中使用的音频数据只能是线性PCM格式的音频数据，这是一种未进过压缩的音频数据格式。要理解整个过程就需要理解多个重要概念：采样频率和采样位数，比特率等。</p>\n<h3 id=\"采样频率\"><a href=\"#采样频率\" class=\"headerlink\" title=\"采样频率\"></a>采样频率</h3><p>采样频率是指单位时间内对声音模拟信号的采样次数。采样率类似于视频的帧数，比如电影的采样率是24Hz。当我们把采样到的一个个静止画面再以采样率同样的速度回放时，看到的就是连续的画面。同样的道理，把以44.1kHZ采样率记录的CD以同样的速率播放时，就能听到连续的声音。显然，这个采样率越高，听到的声音和看到的图像就越连贯。当然，人的听觉和视觉器官能分辨的采样率是有限的。对同一段声音，用20kHz和44.1kHz来采样，重放时，可能可以听出其中的差别，而基本上高于44.1kHZ采样的声音，比如说96kHz采样，绝大部分人已经觉察不到两种采样出来的声音的分别了。之所以使用44.1kHZ这个数值是因为经过了反复实验，人们发现这个采样精度最合适，低于这个值就会有较明显的损失，而高于这个值人的耳朵已经很难分辨，而且增大了数字音频所占用的空间。我们所使用的CD的采样标准就是44.1k。</p>\n<h3 id=\"采样位数\"><a href=\"#采样位数\" class=\"headerlink\" title=\"采样位数\"></a>采样位数</h3><p>采样位数可以理解为采集卡处理声音的解析度。这个数值越大，解析度就越高，录制和回放的声音就越真实。我们首先要知道：电脑中的声音文件是用数字0和1来表示的。连续的模拟信号按一定的采样频率经数码脉冲取样后，每一个离散的脉冲信号被以一定的量化精度量化成一串二进制编码流，这串编码流的位数即为采样位数，也称为量化精度。</p>\n<p>在电脑上录音的本质就是把模拟声音信号转换成数字信号。反之，在播放时则是把数字信号还原成模拟声音信号输出。采集卡的位是指采集卡在采集和播放声音文件时所使用数字声音信号的二进制位数。采集卡的位客观地反映了数字声音信号对输入声音信号描述的准确程度。例如，同一段音频信息，使用8bit描述单个采样信息，那么采样量化的范围就是0~255,如果使用16bit表示单个采样值,那么相应的采样量化的范围为0~64k。与8位采样位数相比，16位采样的动态范围的宽度更小，动态范围更宽广，声音的被记录的更加精细。一般CD使用的采样位数为16位。</p>\n<p>16位二进制数的最小值是0000000000000000，最大值是1111111111111111，对应的十进制数就是0和65535，也就是最大和最小值之间的差值是65535，也就是说，它量化的模拟量的动态范围可以差65535，也就是96.32分贝（20 * lg65535）），所以，量化精度只和动态范围有关，和频率响应没关系。动态范围定在96分贝也是有道理的，人耳的无痛苦极限声压是90分贝，96分贝的动态范围在普通应用中足够使用，所以96分贝动态范围内的模拟波，经量化后，不会产生削波失真的。</p>\n<blockquote>\n<p>所谓分贝是指两个相同的物理量（例A1和A0）之比取以10为底的对数并乘以10（或20）。N = 10lg(A1/A0) 分贝符号为”dB”，它是无量纲的。式中A0是基准量（或参考量），A是被量度量。被量度量和基准量之比取对数，这对数值称为被量度量的”级”。亦即用对数标度时，所得到的是比值，它代表被量度量比基准量高出多少”级”。</p>\n</blockquote>\n<h3 id=\"位速-比特率-码率\"><a href=\"#位速-比特率-码率\" class=\"headerlink\" title=\"位速/比特率/码率\"></a>位速/比特率/码率</h3><p>位速/比特率/码率描述的都是一个东西，是指在一个数据流中每秒钟能通过的信息量。我们可能看到过音频文件用 “128–Kbps MP3” 或 “64–Kbps WMA” 进行描述的情形。Kbps 表示 “每秒千位数”，因此数值越大表示数据越多：128–Kbps MP3 音频文件包含的数据量是 64–Kbps WMA 文件的两倍，并占用两倍的空间。（不过在这种情况下，这两种文件听起来没什么两样。原因是什么呢？有些文件格式比其他文件能够更有效地利用数据， 64–Kbps WMA 文件的音质与 128–Kbps MP3 的音质相同。）需要了解的重要一点是，位速越高，信息量越大，对这些信息进行解码的处理量就越大，文件需要占用的空间也就越多。</p>\n<p>从码率的计算公式中可以清楚的看出码率和采样位数的关系:</p>\n<p><code>码率=取样频率×量化精度×声道数</code></p>\n<blockquote>\n<p>一张CD,双声道,采样率44.1kHz，每个采样位数13bit，时长74分钟(4440秒)，则CD的容量为<code>13*2*44100*4440</code>约等于<code>640MB</code>。</p>\n</blockquote>\n<h3 id=\"VBR、ABR、CBR\"><a href=\"#VBR、ABR、CBR\" class=\"headerlink\" title=\"VBR、ABR、CBR\"></a>VBR、ABR、CBR</h3><p>VBR(Variable Bitrate)动态比特率。也就是没有固定的比特率，压缩软件在压缩时根据音频数据即时确定使用什么比特率。这是新发展的算法，他们将一首歌的复杂部分用高Bitrate编码，简单部分用低Bitrate编码。主意虽然不错，可惜新编码器的VBR算法很差，音质与CBR相去甚远。幸运的是， Lame完美地优化了VBR算法，使之成为MP3的最佳编码模式。这是以质量为前提兼顾文件大小的方式，推荐编码模式。</p>\n<p>ABR(Average Bitrate)平均比特率，是VBR的一种插值参数。Lame针对CBR不佳的文件体积比和VBR生成文件大小不定的特点独创了这种编码模式。ABR也被称为“Safe VBR”，它是在指定的平均Bitrate内，以每50帧(30帧约1秒)为一段，低频和不敏感频率使用相对低的流量，高频和大动态表现时使用高流量。举例来说，当指定用192kbps ABR对一段wav文件进行编码时，Lame会将该文件的85%用192kbps固定编码，然后对剩余15%进行动态优化：复杂部分用高于192kbps 来编码、简单部分用低于192kbps来编码。与192kbps CBR相比，192kbps ABR在文件大小上相差不多，音质却提高不少。ABR编码在速度上是VBR编码的2到3倍，在128-256kbps范围内质量要好于CBR。可以做为 VBR和CBR的一种折衷选择。</p>\n<p>CBR(Constant Bitrate)，常数比特率，指文件从头到尾都是一种位速率。相对于VBR和ABR来讲，它压缩出来的文件体积很大，但音质却不会有明显的提高。</p>\n<h3 id=\"PCM格式与LPCM格式\"><a href=\"#PCM格式与LPCM格式\" class=\"headerlink\" title=\"PCM格式与LPCM格式\"></a>PCM格式与LPCM格式</h3><p>PCM（脉冲编码调制）是一种将模拟语音信号变换为数字信号的编码方式。主要经过3个过程：抽样、量化和编码。抽样过程将连续时间模拟信号变为离散时间、连续幅度的抽样信号，量化过程将抽样信号变为离散时间、离散幅度的数字信号，编码过程将量化后的信号编码成为一个二进制码组输出。</p>\n<p>量化分为线性量化和非线性量化。线性量化在整个量化范围内，量化间隔均相等，称为LPCM。非线性量化采用不等的量化间隔。量化间隔数由编码的二进制位数决定。例如，CD采用16bit线性量化，则量化间隔数L=65536。位数（n)越多，精度越高，信噪比<code>SNR=6.02n+1.76</code>(dB)也越高。但编码的二进制位数不是无限制的，需要根据所需的数据率确定。比如：CD可以达到的数据率为2×44.1×16=1411.2Kbit/s。</p>\n<p>总而言之，LPCM格式中的音频数据是未压缩的线性量化后的音频数据。</p>\n<blockquote>\n<p>用iOS的官方文档中对几个关键词的解释：</p>\n<ul>\n<li>A sample is single numerical value for a single channel.</li>\n<li>A frame is a collection of time-coincident samples. For instance, a stereo sound file has two samples per frame, one for the left channel and one for the right channel.</li>\n<li>A packet is a collection of one or more contiguous frames. In linear PCM audio, a packet is always a single frame. In compressed formats, it is typically more. A packet defines the smallest meaningful set of frames for a given audio data format.</li>\n</ul>\n</blockquote>\n<h3 id=\"压缩过的音频格式\"><a href=\"#压缩过的音频格式\" class=\"headerlink\" title=\"压缩过的音频格式\"></a>压缩过的音频格式</h3><p>在常见的音频格式对PCM原始帧进行封装时也是以frame帧为单位的，我们一般将压缩后的音频数据帧称为媒体帧，对应原始的PCM数据称为原始帧。每个媒体帧又分成head头，body数据体。在帧头中，会存储这个媒体帧中body部分的码率，采样率等解码必须的信息，因此每一个媒体帧都可以独立于文件存在和播放。在body中存储着一个或者多个媒体帧，这些媒体真是若干个PCM原始帧经过特定的压缩算法压缩得到的。通常情况下，我们将单位时间的媒体帧的个数称为帧率。</p>\n<p>上文的采样率和帧率这两个概念都描述了音频媒体的“连续”性，二者的区别在于每个音频的媒体帧中会包含多个音频采样(多个PCM data)，如1个AAC帧中包含1024个采样。</p>\n<blockquote>\n<p>在学习音频/视频相关内容之前,首先需要弄清楚的的是音频的文件类型和音频格式是有本质区别的.封装类型比如.ogg,音频格式比如.mp3.(具体的区别可以百度)</p>\n</blockquote>\n"},{"title":"iOS音频系列(三)--AudioQueue","_content":"\n本篇是AudioQueue的官方文档的笔记。Audio Queue Services可以play和record以下三类任何audio data：\n\n* Linear PCM.\n* Any compressed format supported natively on the Apple platform you are developing for.\n* Any other format for which a user has an installed codec.\n\n对于最后一种类型，我们可以在使用AudioQueue同时自己将自己需要的format转化成LPCM。AudioQueue是对mic和speaker的高度抽象，同时可以非常简单的时间音频codecs。与此同时，它也有一些高级功能，例如多个音频的同步播放，回放等等。\n\n****\n\n## About Audio Queues\n\n\n这章会了解到audio queue的功能，结构体，以及内部运行的机理。具体的内容包括audio queues，audio queue buffers，audio queue会使用到的callback等。还有就是audio queue的状态以及参数。\n\n### What Is an Audio Queue?\n\nAn audio queue 是iOS中play和record audio的对象.底层是`AudioQueueRef`。Audio queue可以完成以下工作：\n\n* Connecting to audio hardware\n* Managing memory\n* Employing codecs, as needed, for compressed audio formats\n* Mediating recording or playback\n\n#### Audio Queue Architecture\n\nAudio queue的具体结构有以下几个部分构成：\n\n* A set of **audio queue buffers**, each of which is a temporary repository for some audio data\n* A **buffer queue**, an ordered list for the audio queue buffers\n* An **audio queue callback** function, that you write\n\n根据我们使用audio queue的用途（record or play），具体的结构略有不同，仅仅只是callback函数函数的内容不同。\n\n#### Audio Queues for Recording\n\n一个用于record 的audio queue，需要使用`AudioQueueNewInput`方法创建，它的具体结构如图：\n\n![A recording audio queue](https://developer.apple.com/library/prerelease/content/documentation/MusicAudio/Conceptual/AudioQueueProgrammingGuide/Art/recording_architecture_2x.png)\n\n#### Audio Queues for Playback\n\n一个用于play的audio queue，需要使用`AudioQueueNewOutput`函数创建，\n\n![A playback audio queue](https://developer.apple.com/library/prerelease/content/documentation/MusicAudio/Conceptual/AudioQueueProgrammingGuide/Art/playback_architecture_2x.png)\n\n#### Audio Queue Buffers\n\n**audio queue buffer**的数据结构如下：\n\n```objc\ntypedef struct AudioQueueBuffer {\n    const UInt32   mAudioDataBytesCapacity;\n    void *const    mAudioData;\n    UInt32         mAudioDataByteSize;\n    void           *mUserData;\n} AudioQueueBuffer;\ntypedef AudioQueueBuffer *AudioQueueBufferRef;\n```\n\n其中**mAudioData**字段表示这个buffer中的有用数据的地址，其他的字段用来辅助audio queue来管理使用这个buffer。一个audio queue可以使用任何数目的buffers。但是我们一般选择3个，比较好管理。\n\nAudio queue通过下面的方式管理它们内部的buffers：\n\n* An audio queue allocates a buffer when you call the AudioQueueAllocateBuffer function.\n* When you release an audio queue by calling the AudioQueueDispose function, the queue releases its buffers.\n\n### The Buffer Queue and Enqueuing\n\nbuffer queue是由audio buffers组成的，是audio queue中的buffers。我们前面介绍了audio queue是如何使用callback管理内部的buffers。不论当前是用于record或者是pleyback，将buffer放到audio queue都是需要我们在callback函数中去手动调用的。\n\n#### The Recording Process\n\n![The recording process](https://developer.apple.com/library/prerelease/content/documentation/MusicAudio/Conceptual/AudioQueueProgrammingGuide/Art/recording_callback_function_2x.png)\n\n1. In step 1 , recording begins. The audio queue fills a buffer with acquired data.\n2. In step 2, the first buffer has been filled. The audio queue invokes the callback, handing it the full buffer (buffer 1). The callback (step 3) writes the contents of the buffer to an audio file. At the same time, the audio queue fills another buffer (buffer 2) with freshly acquired data.\n3. In step 4, the callback enqueues the buffer (buffer 1) that it has just written to disk, putting it in line to be filled again. The audio queue again invokes the callback (step 5), handing it the next full buffer (buffer 2). The callback (step 6) writes the contents of this buffer to the audio file. This looping steady state continues until the user stops the recording.\n\n#### The Playback Process\n\n![The playback process](https://developer.apple.com/library/prerelease/content/documentation/MusicAudio/Conceptual/AudioQueueProgrammingGuide/Art/playback_callback_function_2x.png)\n\n#### Controlling the Playback Process\n\nAudio queue buffers在queue是顺序播放的，我们可以通`theAudioQueueEnqueueBufferWithParameters`方法来进行控制\n\n### The Audio Queue Callback Function\n\nAudio queue在运行过程中会不断的调用callback函数，通常间隔时间和audio queue buffer的大小相关，一般是几秒一次。\n\naudio queue callback主要任务是将audio queue buffer归还给audio queue。callback中通过`AudioQueueEnqueueBuffer`方法将buffer加载到audio queue的最后。在playback中，可以使用`AudioQueueEnqueueBufferWithParameters`在enqueue的过程中进行更多的控制。\n\n#### The Recording Audio Queue Callback Function\n\n如果你仅仅使用audio queue去将record的audio data写入file system，callback的方法实现的原型如下：\n\n```objc\nAudioQueueInputCallback (\n    void                               *inUserData,\n    AudioQueueRef                      inAQ,\n    AudioQueueBufferRef                inBuffer,\n    const AudioTimeStamp               *inStartTime,\n    UInt32                             inNumberPacketDescriptions,\n    const AudioStreamPacketDescription *inPacketDescs\n);\n```\n\n一个recording audio queue会触发我们注册的callback，会在callback的参数中传入所有需要的关于audio data的相关信息：\n\n* **inUserData** 是一个自定义的结构体，用来存储audio queueu以及audio queue buffer的状态信息，也包括AudioFileID，audio data format等。\n* **inAQ** 表示哪个audio queue触发这个callback。\n* **inBuffer** 是一个audio queue buffer，它的内容是由audio queue填充的，内部包括最新的audio data。并且这些audio data已经根据初始化时候传递的格式参数格式化好的数据。\n* **inStartTime** 表示这个buffer中的第一个采样的采样时间点，一般app中不太需要这个参数。\n* **inNumberPacketDescriptions** 表示**inPacketDescs**参数中的packet descriptions的个数。如果你是录入VBR format，audio queue就会在callback中提供这个参数，如果是CBR，audio queue就不会使用packet descriptions参数，这个参数会是NULL。\n* **inPacketDescs** 表示buffer中samples相关的一系列的packet descriptions。是否设置同上一个参数。\n\n#### The Playback Audio Queue Callback Function\n\n这个片段会介绍如果使用playing audio queue，那么callback应该的信息：\n\n```objc\nAudioQueueOutputCallback (\n    void                  *inUserData,\n    AudioQueueRef         inAQ,\n    AudioQueueBufferRef   inBuffer\n);\n```\n\n一个playback audio queue会触发这个callback，提供一些关于audio data的有用信息：\n\n* **inUserData** 见上\n* **inAQ** 表示哪个audio queue触发这个callback。\n* **inBuffer** 表示被audio queue设置为空的audio queue buffer，你需要在callback中将其内部信息填满，填充内容是你从AudioFile中读取的audio data。\n\n****\n\n### Using Codecs and Audio Data Formats\n\n我们日常使用Audio Queue Services时，都会使用codecs（audio data coding/decoding componets）用来在不同audio format之间进行转化。\n\n每个audio queue都有一个audio data format，可以在`AudioStreamBasicDescription`结构体中得到。当我们在ASBD中指定了`mFormatID`以后，audio queue在向buffer中填充数据时候就会使用相应的codec。同样如果指定sample rate和channel count，audio queue也会同样。具体的过程见下图：\n\n![Audio format conversion during recording](https://developer.apple.com/library/ios/documentation/MusicAudio/Conceptual/AudioQueueProgrammingGuide/Art/recording_codec_2x.png)\n\n* 第一步中，app会告知audio queue开始record，同时告诉它使用的的data format。\n* 第二步中，audio queue将获取到的new data使用codec转化成目标format。然后audio queue会调用callback函数，传入格式化以后的audio data。\n* 第三步中，callback函数会将格式化以后的audio data写入file中。\n\n整个过程中，callback函数压根就不需要知道data fromat是什么。\n\n![Audio format conversion during playback](https://developer.apple.com/library/ios/documentation/MusicAudio/Conceptual/AudioQueueProgrammingGuide/Art/playback_codec_2x.png)\n\n在播放过程中，正好和录音过程相反，只需要在创建audio queue时候将data format告知即可。\n\n*****\n\n### Audio Queue Control and State\n\naudio queue在创建和销毁的过程有一个声明周期。app需要管理它的声明周期，控制它的状态，具体控制状态的方法如下：\n\n* Start (`AudioQueueStart`).初始化audio queue用来record或者playback。\n* Prime (`AudioQueuePrime`).对于playback，在调用`AudioQueueStart`挚爱去哪调用确保数据可用，这个方法和record没有关系。\n* Stop (`AudioQueueStop`). 调用以后会重置audio queue，然后会停止record或者playback。在playback应用中，一般在没有audio data可以播放时候调用。\n* Pause (`AudioQueuePause`). 在record或者playback中调用这个方法不会影响到buffers。如果需要恢复，调用`AudioQueueStart`。\n* Flush (`AudioQueueFlush`). 在enqueue最后一个audio queue buffer以后调用这个方法，确保所有的数据被record或者play（主要是在midst processing的数据）。\n* Reset (`AudioQueueReset`). 调用以后立即停止audio queue，然后将所有的buffers移除，重置所有的DSP状态等到。\n\n在调用`AudioQueueStop`方法时候有两种模式：同步和异步。\n\n* Synchronous stopping happens immediately, without regard for previously buffered audio data.\n* Asynchronous stopping happens after all queued buffers have been played or recorded.\n\n****\n\n## Recording Audio\n\n当我们的record使用Audio Queue Services，存储的路径可以是磁盘上的任何地方，或者网络，或者内存中。这部分内容记录大多数的使用场景，存储在磁盘中。\n\n具体的步骤如下：\n\n1. 定义一个结构体去存储状态，format，文件路径等信息。\n2. 完成audio queue callback函数，其中将record以后的数据进行存储。\n3. 为audio queue buffers计算出合适的大小，并且在file中写入magic cookies。\n4. 初始化自定义的结构体\n5. 创建recording audio queue，然后给它创建3个audio queue buffers，然后创建一个file用来存储record以后的audio data。\n6. 启动audio queue\n7. 当audio queue停止以后，dispose它以及buffers\n\n具体的实现内容可以参考Apple官方文档：[Recording Audio](https://developer.apple.com/library/prerelease/content/documentation/MusicAudio/Conceptual/AudioQueueProgrammingGuide/AQRecord/RecordingAudio.html)。\n\n\n## Playing Audio\n\n当我们使用Audio Queue Service去play audio时，音频源文件可以是任何在disk file或者memory中，这部分内容是如何用Audio Queue Service播放存储在disk上的audio file。\n\n具体的步骤如下：\n\n1. 定义一个结构体管理Audio queue的状态，format，file path等\n2. 完成audio queue callback函数去进行实际的播放\n3. 创建一个函数用来计算最适合的audio queue buffer的大小\n4. 打开audio file，确定它的audio data format\n5. 创建audio queue，对它进行配置\n6. 为audio queue创建buffers，然后启动audio queue，当播放结束，callback让audio queue停止播放\n7. 销毁audio queue\n\n具体的实现内容可以参考Apple官方文档：[Playing Audio](https://developer.apple.com/library/prerelease/content/documentation/MusicAudio/Conceptual/AudioQueueProgrammingGuide/AQPlayback/PlayingAudio.html)。\n\n\n\n## 可运行的Demo\n\n请参考我的github: [https://github.com/brownfeng/AudioQueueServiceDemo](https://github.com/brownfeng/AudioQueueServiceDemo)\n\n\n","source":"_posts/iOS音频系列(三).md","raw":"---\ntitle: iOS音频系列(三)--AudioQueue\ntags:\n- iOS\n- CoreAudio\n- AudioQueue\n---\n\n本篇是AudioQueue的官方文档的笔记。Audio Queue Services可以play和record以下三类任何audio data：\n\n* Linear PCM.\n* Any compressed format supported natively on the Apple platform you are developing for.\n* Any other format for which a user has an installed codec.\n\n对于最后一种类型，我们可以在使用AudioQueue同时自己将自己需要的format转化成LPCM。AudioQueue是对mic和speaker的高度抽象，同时可以非常简单的时间音频codecs。与此同时，它也有一些高级功能，例如多个音频的同步播放，回放等等。\n\n****\n\n## About Audio Queues\n\n\n这章会了解到audio queue的功能，结构体，以及内部运行的机理。具体的内容包括audio queues，audio queue buffers，audio queue会使用到的callback等。还有就是audio queue的状态以及参数。\n\n### What Is an Audio Queue?\n\nAn audio queue 是iOS中play和record audio的对象.底层是`AudioQueueRef`。Audio queue可以完成以下工作：\n\n* Connecting to audio hardware\n* Managing memory\n* Employing codecs, as needed, for compressed audio formats\n* Mediating recording or playback\n\n#### Audio Queue Architecture\n\nAudio queue的具体结构有以下几个部分构成：\n\n* A set of **audio queue buffers**, each of which is a temporary repository for some audio data\n* A **buffer queue**, an ordered list for the audio queue buffers\n* An **audio queue callback** function, that you write\n\n根据我们使用audio queue的用途（record or play），具体的结构略有不同，仅仅只是callback函数函数的内容不同。\n\n#### Audio Queues for Recording\n\n一个用于record 的audio queue，需要使用`AudioQueueNewInput`方法创建，它的具体结构如图：\n\n![A recording audio queue](https://developer.apple.com/library/prerelease/content/documentation/MusicAudio/Conceptual/AudioQueueProgrammingGuide/Art/recording_architecture_2x.png)\n\n#### Audio Queues for Playback\n\n一个用于play的audio queue，需要使用`AudioQueueNewOutput`函数创建，\n\n![A playback audio queue](https://developer.apple.com/library/prerelease/content/documentation/MusicAudio/Conceptual/AudioQueueProgrammingGuide/Art/playback_architecture_2x.png)\n\n#### Audio Queue Buffers\n\n**audio queue buffer**的数据结构如下：\n\n```objc\ntypedef struct AudioQueueBuffer {\n    const UInt32   mAudioDataBytesCapacity;\n    void *const    mAudioData;\n    UInt32         mAudioDataByteSize;\n    void           *mUserData;\n} AudioQueueBuffer;\ntypedef AudioQueueBuffer *AudioQueueBufferRef;\n```\n\n其中**mAudioData**字段表示这个buffer中的有用数据的地址，其他的字段用来辅助audio queue来管理使用这个buffer。一个audio queue可以使用任何数目的buffers。但是我们一般选择3个，比较好管理。\n\nAudio queue通过下面的方式管理它们内部的buffers：\n\n* An audio queue allocates a buffer when you call the AudioQueueAllocateBuffer function.\n* When you release an audio queue by calling the AudioQueueDispose function, the queue releases its buffers.\n\n### The Buffer Queue and Enqueuing\n\nbuffer queue是由audio buffers组成的，是audio queue中的buffers。我们前面介绍了audio queue是如何使用callback管理内部的buffers。不论当前是用于record或者是pleyback，将buffer放到audio queue都是需要我们在callback函数中去手动调用的。\n\n#### The Recording Process\n\n![The recording process](https://developer.apple.com/library/prerelease/content/documentation/MusicAudio/Conceptual/AudioQueueProgrammingGuide/Art/recording_callback_function_2x.png)\n\n1. In step 1 , recording begins. The audio queue fills a buffer with acquired data.\n2. In step 2, the first buffer has been filled. The audio queue invokes the callback, handing it the full buffer (buffer 1). The callback (step 3) writes the contents of the buffer to an audio file. At the same time, the audio queue fills another buffer (buffer 2) with freshly acquired data.\n3. In step 4, the callback enqueues the buffer (buffer 1) that it has just written to disk, putting it in line to be filled again. The audio queue again invokes the callback (step 5), handing it the next full buffer (buffer 2). The callback (step 6) writes the contents of this buffer to the audio file. This looping steady state continues until the user stops the recording.\n\n#### The Playback Process\n\n![The playback process](https://developer.apple.com/library/prerelease/content/documentation/MusicAudio/Conceptual/AudioQueueProgrammingGuide/Art/playback_callback_function_2x.png)\n\n#### Controlling the Playback Process\n\nAudio queue buffers在queue是顺序播放的，我们可以通`theAudioQueueEnqueueBufferWithParameters`方法来进行控制\n\n### The Audio Queue Callback Function\n\nAudio queue在运行过程中会不断的调用callback函数，通常间隔时间和audio queue buffer的大小相关，一般是几秒一次。\n\naudio queue callback主要任务是将audio queue buffer归还给audio queue。callback中通过`AudioQueueEnqueueBuffer`方法将buffer加载到audio queue的最后。在playback中，可以使用`AudioQueueEnqueueBufferWithParameters`在enqueue的过程中进行更多的控制。\n\n#### The Recording Audio Queue Callback Function\n\n如果你仅仅使用audio queue去将record的audio data写入file system，callback的方法实现的原型如下：\n\n```objc\nAudioQueueInputCallback (\n    void                               *inUserData,\n    AudioQueueRef                      inAQ,\n    AudioQueueBufferRef                inBuffer,\n    const AudioTimeStamp               *inStartTime,\n    UInt32                             inNumberPacketDescriptions,\n    const AudioStreamPacketDescription *inPacketDescs\n);\n```\n\n一个recording audio queue会触发我们注册的callback，会在callback的参数中传入所有需要的关于audio data的相关信息：\n\n* **inUserData** 是一个自定义的结构体，用来存储audio queueu以及audio queue buffer的状态信息，也包括AudioFileID，audio data format等。\n* **inAQ** 表示哪个audio queue触发这个callback。\n* **inBuffer** 是一个audio queue buffer，它的内容是由audio queue填充的，内部包括最新的audio data。并且这些audio data已经根据初始化时候传递的格式参数格式化好的数据。\n* **inStartTime** 表示这个buffer中的第一个采样的采样时间点，一般app中不太需要这个参数。\n* **inNumberPacketDescriptions** 表示**inPacketDescs**参数中的packet descriptions的个数。如果你是录入VBR format，audio queue就会在callback中提供这个参数，如果是CBR，audio queue就不会使用packet descriptions参数，这个参数会是NULL。\n* **inPacketDescs** 表示buffer中samples相关的一系列的packet descriptions。是否设置同上一个参数。\n\n#### The Playback Audio Queue Callback Function\n\n这个片段会介绍如果使用playing audio queue，那么callback应该的信息：\n\n```objc\nAudioQueueOutputCallback (\n    void                  *inUserData,\n    AudioQueueRef         inAQ,\n    AudioQueueBufferRef   inBuffer\n);\n```\n\n一个playback audio queue会触发这个callback，提供一些关于audio data的有用信息：\n\n* **inUserData** 见上\n* **inAQ** 表示哪个audio queue触发这个callback。\n* **inBuffer** 表示被audio queue设置为空的audio queue buffer，你需要在callback中将其内部信息填满，填充内容是你从AudioFile中读取的audio data。\n\n****\n\n### Using Codecs and Audio Data Formats\n\n我们日常使用Audio Queue Services时，都会使用codecs（audio data coding/decoding componets）用来在不同audio format之间进行转化。\n\n每个audio queue都有一个audio data format，可以在`AudioStreamBasicDescription`结构体中得到。当我们在ASBD中指定了`mFormatID`以后，audio queue在向buffer中填充数据时候就会使用相应的codec。同样如果指定sample rate和channel count，audio queue也会同样。具体的过程见下图：\n\n![Audio format conversion during recording](https://developer.apple.com/library/ios/documentation/MusicAudio/Conceptual/AudioQueueProgrammingGuide/Art/recording_codec_2x.png)\n\n* 第一步中，app会告知audio queue开始record，同时告诉它使用的的data format。\n* 第二步中，audio queue将获取到的new data使用codec转化成目标format。然后audio queue会调用callback函数，传入格式化以后的audio data。\n* 第三步中，callback函数会将格式化以后的audio data写入file中。\n\n整个过程中，callback函数压根就不需要知道data fromat是什么。\n\n![Audio format conversion during playback](https://developer.apple.com/library/ios/documentation/MusicAudio/Conceptual/AudioQueueProgrammingGuide/Art/playback_codec_2x.png)\n\n在播放过程中，正好和录音过程相反，只需要在创建audio queue时候将data format告知即可。\n\n*****\n\n### Audio Queue Control and State\n\naudio queue在创建和销毁的过程有一个声明周期。app需要管理它的声明周期，控制它的状态，具体控制状态的方法如下：\n\n* Start (`AudioQueueStart`).初始化audio queue用来record或者playback。\n* Prime (`AudioQueuePrime`).对于playback，在调用`AudioQueueStart`挚爱去哪调用确保数据可用，这个方法和record没有关系。\n* Stop (`AudioQueueStop`). 调用以后会重置audio queue，然后会停止record或者playback。在playback应用中，一般在没有audio data可以播放时候调用。\n* Pause (`AudioQueuePause`). 在record或者playback中调用这个方法不会影响到buffers。如果需要恢复，调用`AudioQueueStart`。\n* Flush (`AudioQueueFlush`). 在enqueue最后一个audio queue buffer以后调用这个方法，确保所有的数据被record或者play（主要是在midst processing的数据）。\n* Reset (`AudioQueueReset`). 调用以后立即停止audio queue，然后将所有的buffers移除，重置所有的DSP状态等到。\n\n在调用`AudioQueueStop`方法时候有两种模式：同步和异步。\n\n* Synchronous stopping happens immediately, without regard for previously buffered audio data.\n* Asynchronous stopping happens after all queued buffers have been played or recorded.\n\n****\n\n## Recording Audio\n\n当我们的record使用Audio Queue Services，存储的路径可以是磁盘上的任何地方，或者网络，或者内存中。这部分内容记录大多数的使用场景，存储在磁盘中。\n\n具体的步骤如下：\n\n1. 定义一个结构体去存储状态，format，文件路径等信息。\n2. 完成audio queue callback函数，其中将record以后的数据进行存储。\n3. 为audio queue buffers计算出合适的大小，并且在file中写入magic cookies。\n4. 初始化自定义的结构体\n5. 创建recording audio queue，然后给它创建3个audio queue buffers，然后创建一个file用来存储record以后的audio data。\n6. 启动audio queue\n7. 当audio queue停止以后，dispose它以及buffers\n\n具体的实现内容可以参考Apple官方文档：[Recording Audio](https://developer.apple.com/library/prerelease/content/documentation/MusicAudio/Conceptual/AudioQueueProgrammingGuide/AQRecord/RecordingAudio.html)。\n\n\n## Playing Audio\n\n当我们使用Audio Queue Service去play audio时，音频源文件可以是任何在disk file或者memory中，这部分内容是如何用Audio Queue Service播放存储在disk上的audio file。\n\n具体的步骤如下：\n\n1. 定义一个结构体管理Audio queue的状态，format，file path等\n2. 完成audio queue callback函数去进行实际的播放\n3. 创建一个函数用来计算最适合的audio queue buffer的大小\n4. 打开audio file，确定它的audio data format\n5. 创建audio queue，对它进行配置\n6. 为audio queue创建buffers，然后启动audio queue，当播放结束，callback让audio queue停止播放\n7. 销毁audio queue\n\n具体的实现内容可以参考Apple官方文档：[Playing Audio](https://developer.apple.com/library/prerelease/content/documentation/MusicAudio/Conceptual/AudioQueueProgrammingGuide/AQPlayback/PlayingAudio.html)。\n\n\n\n## 可运行的Demo\n\n请参考我的github: [https://github.com/brownfeng/AudioQueueServiceDemo](https://github.com/brownfeng/AudioQueueServiceDemo)\n\n\n","slug":"iOS音频系列(三)","published":1,"date":"2016-08-26T16:19:12.000Z","updated":"2016-08-26T16:19:12.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cisbyxx8a000a6squwqded6b9","content":"<p>本篇是AudioQueue的官方文档的笔记。Audio Queue Services可以play和record以下三类任何audio data：</p>\n<ul>\n<li>Linear PCM.</li>\n<li>Any compressed format supported natively on the Apple platform you are developing for.</li>\n<li>Any other format for which a user has an installed codec.</li>\n</ul>\n<p>对于最后一种类型，我们可以在使用AudioQueue同时自己将自己需要的format转化成LPCM。AudioQueue是对mic和speaker的高度抽象，同时可以非常简单的时间音频codecs。与此同时，它也有一些高级功能，例如多个音频的同步播放，回放等等。</p>\n<hr>\n<h2 id=\"About-Audio-Queues\"><a href=\"#About-Audio-Queues\" class=\"headerlink\" title=\"About Audio Queues\"></a>About Audio Queues</h2><p>这章会了解到audio queue的功能，结构体，以及内部运行的机理。具体的内容包括audio queues，audio queue buffers，audio queue会使用到的callback等。还有就是audio queue的状态以及参数。</p>\n<h3 id=\"What-Is-an-Audio-Queue\"><a href=\"#What-Is-an-Audio-Queue\" class=\"headerlink\" title=\"What Is an Audio Queue?\"></a>What Is an Audio Queue?</h3><p>An audio queue 是iOS中play和record audio的对象.底层是<code>AudioQueueRef</code>。Audio queue可以完成以下工作：</p>\n<ul>\n<li>Connecting to audio hardware</li>\n<li>Managing memory</li>\n<li>Employing codecs, as needed, for compressed audio formats</li>\n<li>Mediating recording or playback</li>\n</ul>\n<h4 id=\"Audio-Queue-Architecture\"><a href=\"#Audio-Queue-Architecture\" class=\"headerlink\" title=\"Audio Queue Architecture\"></a>Audio Queue Architecture</h4><p>Audio queue的具体结构有以下几个部分构成：</p>\n<ul>\n<li>A set of <strong>audio queue buffers</strong>, each of which is a temporary repository for some audio data</li>\n<li>A <strong>buffer queue</strong>, an ordered list for the audio queue buffers</li>\n<li>An <strong>audio queue callback</strong> function, that you write</li>\n</ul>\n<p>根据我们使用audio queue的用途（record or play），具体的结构略有不同，仅仅只是callback函数函数的内容不同。</p>\n<h4 id=\"Audio-Queues-for-Recording\"><a href=\"#Audio-Queues-for-Recording\" class=\"headerlink\" title=\"Audio Queues for Recording\"></a>Audio Queues for Recording</h4><p>一个用于record 的audio queue，需要使用<code>AudioQueueNewInput</code>方法创建，它的具体结构如图：</p>\n<p><img src=\"https://developer.apple.com/library/prerelease/content/documentation/MusicAudio/Conceptual/AudioQueueProgrammingGuide/Art/recording_architecture_2x.png\" alt=\"A recording audio queue\"></p>\n<h4 id=\"Audio-Queues-for-Playback\"><a href=\"#Audio-Queues-for-Playback\" class=\"headerlink\" title=\"Audio Queues for Playback\"></a>Audio Queues for Playback</h4><p>一个用于play的audio queue，需要使用<code>AudioQueueNewOutput</code>函数创建，</p>\n<p><img src=\"https://developer.apple.com/library/prerelease/content/documentation/MusicAudio/Conceptual/AudioQueueProgrammingGuide/Art/playback_architecture_2x.png\" alt=\"A playback audio queue\"></p>\n<h4 id=\"Audio-Queue-Buffers\"><a href=\"#Audio-Queue-Buffers\" class=\"headerlink\" title=\"Audio Queue Buffers\"></a>Audio Queue Buffers</h4><p><strong>audio queue buffer</strong>的数据结构如下：</p>\n<figure class=\"highlight objc\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"keyword\">struct</span> AudioQueueBuffer &#123;</div><div class=\"line\">    <span class=\"keyword\">const</span> <span class=\"built_in\">UInt32</span>   mAudioDataBytesCapacity;</div><div class=\"line\">    <span class=\"keyword\">void</span> *<span class=\"keyword\">const</span>    mAudioData;</div><div class=\"line\">    <span class=\"built_in\">UInt32</span>         mAudioDataByteSize;</div><div class=\"line\">    <span class=\"keyword\">void</span>           *mUserData;</div><div class=\"line\">&#125; AudioQueueBuffer;</div><div class=\"line\"><span class=\"keyword\">typedef</span> AudioQueueBuffer *AudioQueueBufferRef;</div></pre></td></tr></table></figure>\n<p>其中<strong>mAudioData</strong>字段表示这个buffer中的有用数据的地址，其他的字段用来辅助audio queue来管理使用这个buffer。一个audio queue可以使用任何数目的buffers。但是我们一般选择3个，比较好管理。</p>\n<p>Audio queue通过下面的方式管理它们内部的buffers：</p>\n<ul>\n<li>An audio queue allocates a buffer when you call the AudioQueueAllocateBuffer function.</li>\n<li>When you release an audio queue by calling the AudioQueueDispose function, the queue releases its buffers.</li>\n</ul>\n<h3 id=\"The-Buffer-Queue-and-Enqueuing\"><a href=\"#The-Buffer-Queue-and-Enqueuing\" class=\"headerlink\" title=\"The Buffer Queue and Enqueuing\"></a>The Buffer Queue and Enqueuing</h3><p>buffer queue是由audio buffers组成的，是audio queue中的buffers。我们前面介绍了audio queue是如何使用callback管理内部的buffers。不论当前是用于record或者是pleyback，将buffer放到audio queue都是需要我们在callback函数中去手动调用的。</p>\n<h4 id=\"The-Recording-Process\"><a href=\"#The-Recording-Process\" class=\"headerlink\" title=\"The Recording Process\"></a>The Recording Process</h4><p><img src=\"https://developer.apple.com/library/prerelease/content/documentation/MusicAudio/Conceptual/AudioQueueProgrammingGuide/Art/recording_callback_function_2x.png\" alt=\"The recording process\"></p>\n<ol>\n<li>In step 1 , recording begins. The audio queue fills a buffer with acquired data.</li>\n<li>In step 2, the first buffer has been filled. The audio queue invokes the callback, handing it the full buffer (buffer 1). The callback (step 3) writes the contents of the buffer to an audio file. At the same time, the audio queue fills another buffer (buffer 2) with freshly acquired data.</li>\n<li>In step 4, the callback enqueues the buffer (buffer 1) that it has just written to disk, putting it in line to be filled again. The audio queue again invokes the callback (step 5), handing it the next full buffer (buffer 2). The callback (step 6) writes the contents of this buffer to the audio file. This looping steady state continues until the user stops the recording.</li>\n</ol>\n<h4 id=\"The-Playback-Process\"><a href=\"#The-Playback-Process\" class=\"headerlink\" title=\"The Playback Process\"></a>The Playback Process</h4><p><img src=\"https://developer.apple.com/library/prerelease/content/documentation/MusicAudio/Conceptual/AudioQueueProgrammingGuide/Art/playback_callback_function_2x.png\" alt=\"The playback process\"></p>\n<h4 id=\"Controlling-the-Playback-Process\"><a href=\"#Controlling-the-Playback-Process\" class=\"headerlink\" title=\"Controlling the Playback Process\"></a>Controlling the Playback Process</h4><p>Audio queue buffers在queue是顺序播放的，我们可以通<code>theAudioQueueEnqueueBufferWithParameters</code>方法来进行控制</p>\n<h3 id=\"The-Audio-Queue-Callback-Function\"><a href=\"#The-Audio-Queue-Callback-Function\" class=\"headerlink\" title=\"The Audio Queue Callback Function\"></a>The Audio Queue Callback Function</h3><p>Audio queue在运行过程中会不断的调用callback函数，通常间隔时间和audio queue buffer的大小相关，一般是几秒一次。</p>\n<p>audio queue callback主要任务是将audio queue buffer归还给audio queue。callback中通过<code>AudioQueueEnqueueBuffer</code>方法将buffer加载到audio queue的最后。在playback中，可以使用<code>AudioQueueEnqueueBufferWithParameters</code>在enqueue的过程中进行更多的控制。</p>\n<h4 id=\"The-Recording-Audio-Queue-Callback-Function\"><a href=\"#The-Recording-Audio-Queue-Callback-Function\" class=\"headerlink\" title=\"The Recording Audio Queue Callback Function\"></a>The Recording Audio Queue Callback Function</h4><p>如果你仅仅使用audio queue去将record的audio data写入file system，callback的方法实现的原型如下：</p>\n<figure class=\"highlight objc\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">AudioQueueInputCallback (</div><div class=\"line\">    <span class=\"keyword\">void</span>                               *inUserData,</div><div class=\"line\">    AudioQueueRef                      inAQ,</div><div class=\"line\">    AudioQueueBufferRef                inBuffer,</div><div class=\"line\">    <span class=\"keyword\">const</span> AudioTimeStamp               *inStartTime,</div><div class=\"line\">    <span class=\"built_in\">UInt32</span>                             inNumberPacketDescriptions,</div><div class=\"line\">    <span class=\"keyword\">const</span> AudioStreamPacketDescription *inPacketDescs</div><div class=\"line\">);</div></pre></td></tr></table></figure>\n<p>一个recording audio queue会触发我们注册的callback，会在callback的参数中传入所有需要的关于audio data的相关信息：</p>\n<ul>\n<li><strong>inUserData</strong> 是一个自定义的结构体，用来存储audio queueu以及audio queue buffer的状态信息，也包括AudioFileID，audio data format等。</li>\n<li><strong>inAQ</strong> 表示哪个audio queue触发这个callback。</li>\n<li><strong>inBuffer</strong> 是一个audio queue buffer，它的内容是由audio queue填充的，内部包括最新的audio data。并且这些audio data已经根据初始化时候传递的格式参数格式化好的数据。</li>\n<li><strong>inStartTime</strong> 表示这个buffer中的第一个采样的采样时间点，一般app中不太需要这个参数。</li>\n<li><strong>inNumberPacketDescriptions</strong> 表示<strong>inPacketDescs</strong>参数中的packet descriptions的个数。如果你是录入VBR format，audio queue就会在callback中提供这个参数，如果是CBR，audio queue就不会使用packet descriptions参数，这个参数会是NULL。</li>\n<li><strong>inPacketDescs</strong> 表示buffer中samples相关的一系列的packet descriptions。是否设置同上一个参数。</li>\n</ul>\n<h4 id=\"The-Playback-Audio-Queue-Callback-Function\"><a href=\"#The-Playback-Audio-Queue-Callback-Function\" class=\"headerlink\" title=\"The Playback Audio Queue Callback Function\"></a>The Playback Audio Queue Callback Function</h4><p>这个片段会介绍如果使用playing audio queue，那么callback应该的信息：</p>\n<figure class=\"highlight objc\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">AudioQueueOutputCallback (</div><div class=\"line\">    <span class=\"keyword\">void</span>                  *inUserData,</div><div class=\"line\">    AudioQueueRef         inAQ,</div><div class=\"line\">    AudioQueueBufferRef   inBuffer</div><div class=\"line\">);</div></pre></td></tr></table></figure>\n<p>一个playback audio queue会触发这个callback，提供一些关于audio data的有用信息：</p>\n<ul>\n<li><strong>inUserData</strong> 见上</li>\n<li><strong>inAQ</strong> 表示哪个audio queue触发这个callback。</li>\n<li><strong>inBuffer</strong> 表示被audio queue设置为空的audio queue buffer，你需要在callback中将其内部信息填满，填充内容是你从AudioFile中读取的audio data。</li>\n</ul>\n<hr>\n<h3 id=\"Using-Codecs-and-Audio-Data-Formats\"><a href=\"#Using-Codecs-and-Audio-Data-Formats\" class=\"headerlink\" title=\"Using Codecs and Audio Data Formats\"></a>Using Codecs and Audio Data Formats</h3><p>我们日常使用Audio Queue Services时，都会使用codecs（audio data coding/decoding componets）用来在不同audio format之间进行转化。</p>\n<p>每个audio queue都有一个audio data format，可以在<code>AudioStreamBasicDescription</code>结构体中得到。当我们在ASBD中指定了<code>mFormatID</code>以后，audio queue在向buffer中填充数据时候就会使用相应的codec。同样如果指定sample rate和channel count，audio queue也会同样。具体的过程见下图：</p>\n<p><img src=\"https://developer.apple.com/library/ios/documentation/MusicAudio/Conceptual/AudioQueueProgrammingGuide/Art/recording_codec_2x.png\" alt=\"Audio format conversion during recording\"></p>\n<ul>\n<li>第一步中，app会告知audio queue开始record，同时告诉它使用的的data format。</li>\n<li>第二步中，audio queue将获取到的new data使用codec转化成目标format。然后audio queue会调用callback函数，传入格式化以后的audio data。</li>\n<li>第三步中，callback函数会将格式化以后的audio data写入file中。</li>\n</ul>\n<p>整个过程中，callback函数压根就不需要知道data fromat是什么。</p>\n<p><img src=\"https://developer.apple.com/library/ios/documentation/MusicAudio/Conceptual/AudioQueueProgrammingGuide/Art/playback_codec_2x.png\" alt=\"Audio format conversion during playback\"></p>\n<p>在播放过程中，正好和录音过程相反，只需要在创建audio queue时候将data format告知即可。</p>\n<hr>\n<h3 id=\"Audio-Queue-Control-and-State\"><a href=\"#Audio-Queue-Control-and-State\" class=\"headerlink\" title=\"Audio Queue Control and State\"></a>Audio Queue Control and State</h3><p>audio queue在创建和销毁的过程有一个声明周期。app需要管理它的声明周期，控制它的状态，具体控制状态的方法如下：</p>\n<ul>\n<li>Start (<code>AudioQueueStart</code>).初始化audio queue用来record或者playback。</li>\n<li>Prime (<code>AudioQueuePrime</code>).对于playback，在调用<code>AudioQueueStart</code>挚爱去哪调用确保数据可用，这个方法和record没有关系。</li>\n<li>Stop (<code>AudioQueueStop</code>). 调用以后会重置audio queue，然后会停止record或者playback。在playback应用中，一般在没有audio data可以播放时候调用。</li>\n<li>Pause (<code>AudioQueuePause</code>). 在record或者playback中调用这个方法不会影响到buffers。如果需要恢复，调用<code>AudioQueueStart</code>。</li>\n<li>Flush (<code>AudioQueueFlush</code>). 在enqueue最后一个audio queue buffer以后调用这个方法，确保所有的数据被record或者play（主要是在midst processing的数据）。</li>\n<li>Reset (<code>AudioQueueReset</code>). 调用以后立即停止audio queue，然后将所有的buffers移除，重置所有的DSP状态等到。</li>\n</ul>\n<p>在调用<code>AudioQueueStop</code>方法时候有两种模式：同步和异步。</p>\n<ul>\n<li>Synchronous stopping happens immediately, without regard for previously buffered audio data.</li>\n<li>Asynchronous stopping happens after all queued buffers have been played or recorded.</li>\n</ul>\n<hr>\n<h2 id=\"Recording-Audio\"><a href=\"#Recording-Audio\" class=\"headerlink\" title=\"Recording Audio\"></a>Recording Audio</h2><p>当我们的record使用Audio Queue Services，存储的路径可以是磁盘上的任何地方，或者网络，或者内存中。这部分内容记录大多数的使用场景，存储在磁盘中。</p>\n<p>具体的步骤如下：</p>\n<ol>\n<li>定义一个结构体去存储状态，format，文件路径等信息。</li>\n<li>完成audio queue callback函数，其中将record以后的数据进行存储。</li>\n<li>为audio queue buffers计算出合适的大小，并且在file中写入magic cookies。</li>\n<li>初始化自定义的结构体</li>\n<li>创建recording audio queue，然后给它创建3个audio queue buffers，然后创建一个file用来存储record以后的audio data。</li>\n<li>启动audio queue</li>\n<li>当audio queue停止以后，dispose它以及buffers</li>\n</ol>\n<p>具体的实现内容可以参考Apple官方文档：<a href=\"https://developer.apple.com/library/prerelease/content/documentation/MusicAudio/Conceptual/AudioQueueProgrammingGuide/AQRecord/RecordingAudio.html\" target=\"_blank\" rel=\"external\">Recording Audio</a>。</p>\n<h2 id=\"Playing-Audio\"><a href=\"#Playing-Audio\" class=\"headerlink\" title=\"Playing Audio\"></a>Playing Audio</h2><p>当我们使用Audio Queue Service去play audio时，音频源文件可以是任何在disk file或者memory中，这部分内容是如何用Audio Queue Service播放存储在disk上的audio file。</p>\n<p>具体的步骤如下：</p>\n<ol>\n<li>定义一个结构体管理Audio queue的状态，format，file path等</li>\n<li>完成audio queue callback函数去进行实际的播放</li>\n<li>创建一个函数用来计算最适合的audio queue buffer的大小</li>\n<li>打开audio file，确定它的audio data format</li>\n<li>创建audio queue，对它进行配置</li>\n<li>为audio queue创建buffers，然后启动audio queue，当播放结束，callback让audio queue停止播放</li>\n<li>销毁audio queue</li>\n</ol>\n<p>具体的实现内容可以参考Apple官方文档：<a href=\"https://developer.apple.com/library/prerelease/content/documentation/MusicAudio/Conceptual/AudioQueueProgrammingGuide/AQPlayback/PlayingAudio.html\" target=\"_blank\" rel=\"external\">Playing Audio</a>。</p>\n<h2 id=\"可运行的Demo\"><a href=\"#可运行的Demo\" class=\"headerlink\" title=\"可运行的Demo\"></a>可运行的Demo</h2><p>请参考我的github: <a href=\"https://github.com/brownfeng/AudioQueueServiceDemo\" target=\"_blank\" rel=\"external\">https://github.com/brownfeng/AudioQueueServiceDemo</a></p>\n","excerpt":"","more":"<p>本篇是AudioQueue的官方文档的笔记。Audio Queue Services可以play和record以下三类任何audio data：</p>\n<ul>\n<li>Linear PCM.</li>\n<li>Any compressed format supported natively on the Apple platform you are developing for.</li>\n<li>Any other format for which a user has an installed codec.</li>\n</ul>\n<p>对于最后一种类型，我们可以在使用AudioQueue同时自己将自己需要的format转化成LPCM。AudioQueue是对mic和speaker的高度抽象，同时可以非常简单的时间音频codecs。与此同时，它也有一些高级功能，例如多个音频的同步播放，回放等等。</p>\n<hr>\n<h2 id=\"About-Audio-Queues\"><a href=\"#About-Audio-Queues\" class=\"headerlink\" title=\"About Audio Queues\"></a>About Audio Queues</h2><p>这章会了解到audio queue的功能，结构体，以及内部运行的机理。具体的内容包括audio queues，audio queue buffers，audio queue会使用到的callback等。还有就是audio queue的状态以及参数。</p>\n<h3 id=\"What-Is-an-Audio-Queue\"><a href=\"#What-Is-an-Audio-Queue\" class=\"headerlink\" title=\"What Is an Audio Queue?\"></a>What Is an Audio Queue?</h3><p>An audio queue 是iOS中play和record audio的对象.底层是<code>AudioQueueRef</code>。Audio queue可以完成以下工作：</p>\n<ul>\n<li>Connecting to audio hardware</li>\n<li>Managing memory</li>\n<li>Employing codecs, as needed, for compressed audio formats</li>\n<li>Mediating recording or playback</li>\n</ul>\n<h4 id=\"Audio-Queue-Architecture\"><a href=\"#Audio-Queue-Architecture\" class=\"headerlink\" title=\"Audio Queue Architecture\"></a>Audio Queue Architecture</h4><p>Audio queue的具体结构有以下几个部分构成：</p>\n<ul>\n<li>A set of <strong>audio queue buffers</strong>, each of which is a temporary repository for some audio data</li>\n<li>A <strong>buffer queue</strong>, an ordered list for the audio queue buffers</li>\n<li>An <strong>audio queue callback</strong> function, that you write</li>\n</ul>\n<p>根据我们使用audio queue的用途（record or play），具体的结构略有不同，仅仅只是callback函数函数的内容不同。</p>\n<h4 id=\"Audio-Queues-for-Recording\"><a href=\"#Audio-Queues-for-Recording\" class=\"headerlink\" title=\"Audio Queues for Recording\"></a>Audio Queues for Recording</h4><p>一个用于record 的audio queue，需要使用<code>AudioQueueNewInput</code>方法创建，它的具体结构如图：</p>\n<p><img src=\"https://developer.apple.com/library/prerelease/content/documentation/MusicAudio/Conceptual/AudioQueueProgrammingGuide/Art/recording_architecture_2x.png\" alt=\"A recording audio queue\"></p>\n<h4 id=\"Audio-Queues-for-Playback\"><a href=\"#Audio-Queues-for-Playback\" class=\"headerlink\" title=\"Audio Queues for Playback\"></a>Audio Queues for Playback</h4><p>一个用于play的audio queue，需要使用<code>AudioQueueNewOutput</code>函数创建，</p>\n<p><img src=\"https://developer.apple.com/library/prerelease/content/documentation/MusicAudio/Conceptual/AudioQueueProgrammingGuide/Art/playback_architecture_2x.png\" alt=\"A playback audio queue\"></p>\n<h4 id=\"Audio-Queue-Buffers\"><a href=\"#Audio-Queue-Buffers\" class=\"headerlink\" title=\"Audio Queue Buffers\"></a>Audio Queue Buffers</h4><p><strong>audio queue buffer</strong>的数据结构如下：</p>\n<figure class=\"highlight objc\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"keyword\">struct</span> AudioQueueBuffer &#123;</div><div class=\"line\">    <span class=\"keyword\">const</span> <span class=\"built_in\">UInt32</span>   mAudioDataBytesCapacity;</div><div class=\"line\">    <span class=\"keyword\">void</span> *<span class=\"keyword\">const</span>    mAudioData;</div><div class=\"line\">    <span class=\"built_in\">UInt32</span>         mAudioDataByteSize;</div><div class=\"line\">    <span class=\"keyword\">void</span>           *mUserData;</div><div class=\"line\">&#125; AudioQueueBuffer;</div><div class=\"line\"><span class=\"keyword\">typedef</span> AudioQueueBuffer *AudioQueueBufferRef;</div></pre></td></tr></table></figure>\n<p>其中<strong>mAudioData</strong>字段表示这个buffer中的有用数据的地址，其他的字段用来辅助audio queue来管理使用这个buffer。一个audio queue可以使用任何数目的buffers。但是我们一般选择3个，比较好管理。</p>\n<p>Audio queue通过下面的方式管理它们内部的buffers：</p>\n<ul>\n<li>An audio queue allocates a buffer when you call the AudioQueueAllocateBuffer function.</li>\n<li>When you release an audio queue by calling the AudioQueueDispose function, the queue releases its buffers.</li>\n</ul>\n<h3 id=\"The-Buffer-Queue-and-Enqueuing\"><a href=\"#The-Buffer-Queue-and-Enqueuing\" class=\"headerlink\" title=\"The Buffer Queue and Enqueuing\"></a>The Buffer Queue and Enqueuing</h3><p>buffer queue是由audio buffers组成的，是audio queue中的buffers。我们前面介绍了audio queue是如何使用callback管理内部的buffers。不论当前是用于record或者是pleyback，将buffer放到audio queue都是需要我们在callback函数中去手动调用的。</p>\n<h4 id=\"The-Recording-Process\"><a href=\"#The-Recording-Process\" class=\"headerlink\" title=\"The Recording Process\"></a>The Recording Process</h4><p><img src=\"https://developer.apple.com/library/prerelease/content/documentation/MusicAudio/Conceptual/AudioQueueProgrammingGuide/Art/recording_callback_function_2x.png\" alt=\"The recording process\"></p>\n<ol>\n<li>In step 1 , recording begins. The audio queue fills a buffer with acquired data.</li>\n<li>In step 2, the first buffer has been filled. The audio queue invokes the callback, handing it the full buffer (buffer 1). The callback (step 3) writes the contents of the buffer to an audio file. At the same time, the audio queue fills another buffer (buffer 2) with freshly acquired data.</li>\n<li>In step 4, the callback enqueues the buffer (buffer 1) that it has just written to disk, putting it in line to be filled again. The audio queue again invokes the callback (step 5), handing it the next full buffer (buffer 2). The callback (step 6) writes the contents of this buffer to the audio file. This looping steady state continues until the user stops the recording.</li>\n</ol>\n<h4 id=\"The-Playback-Process\"><a href=\"#The-Playback-Process\" class=\"headerlink\" title=\"The Playback Process\"></a>The Playback Process</h4><p><img src=\"https://developer.apple.com/library/prerelease/content/documentation/MusicAudio/Conceptual/AudioQueueProgrammingGuide/Art/playback_callback_function_2x.png\" alt=\"The playback process\"></p>\n<h4 id=\"Controlling-the-Playback-Process\"><a href=\"#Controlling-the-Playback-Process\" class=\"headerlink\" title=\"Controlling the Playback Process\"></a>Controlling the Playback Process</h4><p>Audio queue buffers在queue是顺序播放的，我们可以通<code>theAudioQueueEnqueueBufferWithParameters</code>方法来进行控制</p>\n<h3 id=\"The-Audio-Queue-Callback-Function\"><a href=\"#The-Audio-Queue-Callback-Function\" class=\"headerlink\" title=\"The Audio Queue Callback Function\"></a>The Audio Queue Callback Function</h3><p>Audio queue在运行过程中会不断的调用callback函数，通常间隔时间和audio queue buffer的大小相关，一般是几秒一次。</p>\n<p>audio queue callback主要任务是将audio queue buffer归还给audio queue。callback中通过<code>AudioQueueEnqueueBuffer</code>方法将buffer加载到audio queue的最后。在playback中，可以使用<code>AudioQueueEnqueueBufferWithParameters</code>在enqueue的过程中进行更多的控制。</p>\n<h4 id=\"The-Recording-Audio-Queue-Callback-Function\"><a href=\"#The-Recording-Audio-Queue-Callback-Function\" class=\"headerlink\" title=\"The Recording Audio Queue Callback Function\"></a>The Recording Audio Queue Callback Function</h4><p>如果你仅仅使用audio queue去将record的audio data写入file system，callback的方法实现的原型如下：</p>\n<figure class=\"highlight objc\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">AudioQueueInputCallback (</div><div class=\"line\">    <span class=\"keyword\">void</span>                               *inUserData,</div><div class=\"line\">    AudioQueueRef                      inAQ,</div><div class=\"line\">    AudioQueueBufferRef                inBuffer,</div><div class=\"line\">    <span class=\"keyword\">const</span> AudioTimeStamp               *inStartTime,</div><div class=\"line\">    <span class=\"built_in\">UInt32</span>                             inNumberPacketDescriptions,</div><div class=\"line\">    <span class=\"keyword\">const</span> AudioStreamPacketDescription *inPacketDescs</div><div class=\"line\">);</div></pre></td></tr></table></figure>\n<p>一个recording audio queue会触发我们注册的callback，会在callback的参数中传入所有需要的关于audio data的相关信息：</p>\n<ul>\n<li><strong>inUserData</strong> 是一个自定义的结构体，用来存储audio queueu以及audio queue buffer的状态信息，也包括AudioFileID，audio data format等。</li>\n<li><strong>inAQ</strong> 表示哪个audio queue触发这个callback。</li>\n<li><strong>inBuffer</strong> 是一个audio queue buffer，它的内容是由audio queue填充的，内部包括最新的audio data。并且这些audio data已经根据初始化时候传递的格式参数格式化好的数据。</li>\n<li><strong>inStartTime</strong> 表示这个buffer中的第一个采样的采样时间点，一般app中不太需要这个参数。</li>\n<li><strong>inNumberPacketDescriptions</strong> 表示<strong>inPacketDescs</strong>参数中的packet descriptions的个数。如果你是录入VBR format，audio queue就会在callback中提供这个参数，如果是CBR，audio queue就不会使用packet descriptions参数，这个参数会是NULL。</li>\n<li><strong>inPacketDescs</strong> 表示buffer中samples相关的一系列的packet descriptions。是否设置同上一个参数。</li>\n</ul>\n<h4 id=\"The-Playback-Audio-Queue-Callback-Function\"><a href=\"#The-Playback-Audio-Queue-Callback-Function\" class=\"headerlink\" title=\"The Playback Audio Queue Callback Function\"></a>The Playback Audio Queue Callback Function</h4><p>这个片段会介绍如果使用playing audio queue，那么callback应该的信息：</p>\n<figure class=\"highlight objc\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">AudioQueueOutputCallback (</div><div class=\"line\">    <span class=\"keyword\">void</span>                  *inUserData,</div><div class=\"line\">    AudioQueueRef         inAQ,</div><div class=\"line\">    AudioQueueBufferRef   inBuffer</div><div class=\"line\">);</div></pre></td></tr></table></figure>\n<p>一个playback audio queue会触发这个callback，提供一些关于audio data的有用信息：</p>\n<ul>\n<li><strong>inUserData</strong> 见上</li>\n<li><strong>inAQ</strong> 表示哪个audio queue触发这个callback。</li>\n<li><strong>inBuffer</strong> 表示被audio queue设置为空的audio queue buffer，你需要在callback中将其内部信息填满，填充内容是你从AudioFile中读取的audio data。</li>\n</ul>\n<hr>\n<h3 id=\"Using-Codecs-and-Audio-Data-Formats\"><a href=\"#Using-Codecs-and-Audio-Data-Formats\" class=\"headerlink\" title=\"Using Codecs and Audio Data Formats\"></a>Using Codecs and Audio Data Formats</h3><p>我们日常使用Audio Queue Services时，都会使用codecs（audio data coding/decoding componets）用来在不同audio format之间进行转化。</p>\n<p>每个audio queue都有一个audio data format，可以在<code>AudioStreamBasicDescription</code>结构体中得到。当我们在ASBD中指定了<code>mFormatID</code>以后，audio queue在向buffer中填充数据时候就会使用相应的codec。同样如果指定sample rate和channel count，audio queue也会同样。具体的过程见下图：</p>\n<p><img src=\"https://developer.apple.com/library/ios/documentation/MusicAudio/Conceptual/AudioQueueProgrammingGuide/Art/recording_codec_2x.png\" alt=\"Audio format conversion during recording\"></p>\n<ul>\n<li>第一步中，app会告知audio queue开始record，同时告诉它使用的的data format。</li>\n<li>第二步中，audio queue将获取到的new data使用codec转化成目标format。然后audio queue会调用callback函数，传入格式化以后的audio data。</li>\n<li>第三步中，callback函数会将格式化以后的audio data写入file中。</li>\n</ul>\n<p>整个过程中，callback函数压根就不需要知道data fromat是什么。</p>\n<p><img src=\"https://developer.apple.com/library/ios/documentation/MusicAudio/Conceptual/AudioQueueProgrammingGuide/Art/playback_codec_2x.png\" alt=\"Audio format conversion during playback\"></p>\n<p>在播放过程中，正好和录音过程相反，只需要在创建audio queue时候将data format告知即可。</p>\n<hr>\n<h3 id=\"Audio-Queue-Control-and-State\"><a href=\"#Audio-Queue-Control-and-State\" class=\"headerlink\" title=\"Audio Queue Control and State\"></a>Audio Queue Control and State</h3><p>audio queue在创建和销毁的过程有一个声明周期。app需要管理它的声明周期，控制它的状态，具体控制状态的方法如下：</p>\n<ul>\n<li>Start (<code>AudioQueueStart</code>).初始化audio queue用来record或者playback。</li>\n<li>Prime (<code>AudioQueuePrime</code>).对于playback，在调用<code>AudioQueueStart</code>挚爱去哪调用确保数据可用，这个方法和record没有关系。</li>\n<li>Stop (<code>AudioQueueStop</code>). 调用以后会重置audio queue，然后会停止record或者playback。在playback应用中，一般在没有audio data可以播放时候调用。</li>\n<li>Pause (<code>AudioQueuePause</code>). 在record或者playback中调用这个方法不会影响到buffers。如果需要恢复，调用<code>AudioQueueStart</code>。</li>\n<li>Flush (<code>AudioQueueFlush</code>). 在enqueue最后一个audio queue buffer以后调用这个方法，确保所有的数据被record或者play（主要是在midst processing的数据）。</li>\n<li>Reset (<code>AudioQueueReset</code>). 调用以后立即停止audio queue，然后将所有的buffers移除，重置所有的DSP状态等到。</li>\n</ul>\n<p>在调用<code>AudioQueueStop</code>方法时候有两种模式：同步和异步。</p>\n<ul>\n<li>Synchronous stopping happens immediately, without regard for previously buffered audio data.</li>\n<li>Asynchronous stopping happens after all queued buffers have been played or recorded.</li>\n</ul>\n<hr>\n<h2 id=\"Recording-Audio\"><a href=\"#Recording-Audio\" class=\"headerlink\" title=\"Recording Audio\"></a>Recording Audio</h2><p>当我们的record使用Audio Queue Services，存储的路径可以是磁盘上的任何地方，或者网络，或者内存中。这部分内容记录大多数的使用场景，存储在磁盘中。</p>\n<p>具体的步骤如下：</p>\n<ol>\n<li>定义一个结构体去存储状态，format，文件路径等信息。</li>\n<li>完成audio queue callback函数，其中将record以后的数据进行存储。</li>\n<li>为audio queue buffers计算出合适的大小，并且在file中写入magic cookies。</li>\n<li>初始化自定义的结构体</li>\n<li>创建recording audio queue，然后给它创建3个audio queue buffers，然后创建一个file用来存储record以后的audio data。</li>\n<li>启动audio queue</li>\n<li>当audio queue停止以后，dispose它以及buffers</li>\n</ol>\n<p>具体的实现内容可以参考Apple官方文档：<a href=\"https://developer.apple.com/library/prerelease/content/documentation/MusicAudio/Conceptual/AudioQueueProgrammingGuide/AQRecord/RecordingAudio.html\">Recording Audio</a>。</p>\n<h2 id=\"Playing-Audio\"><a href=\"#Playing-Audio\" class=\"headerlink\" title=\"Playing Audio\"></a>Playing Audio</h2><p>当我们使用Audio Queue Service去play audio时，音频源文件可以是任何在disk file或者memory中，这部分内容是如何用Audio Queue Service播放存储在disk上的audio file。</p>\n<p>具体的步骤如下：</p>\n<ol>\n<li>定义一个结构体管理Audio queue的状态，format，file path等</li>\n<li>完成audio queue callback函数去进行实际的播放</li>\n<li>创建一个函数用来计算最适合的audio queue buffer的大小</li>\n<li>打开audio file，确定它的audio data format</li>\n<li>创建audio queue，对它进行配置</li>\n<li>为audio queue创建buffers，然后启动audio queue，当播放结束，callback让audio queue停止播放</li>\n<li>销毁audio queue</li>\n</ol>\n<p>具体的实现内容可以参考Apple官方文档：<a href=\"https://developer.apple.com/library/prerelease/content/documentation/MusicAudio/Conceptual/AudioQueueProgrammingGuide/AQPlayback/PlayingAudio.html\">Playing Audio</a>。</p>\n<h2 id=\"可运行的Demo\"><a href=\"#可运行的Demo\" class=\"headerlink\" title=\"可运行的Demo\"></a>可运行的Demo</h2><p>请参考我的github: <a href=\"https://github.com/brownfeng/AudioQueueServiceDemo\">https://github.com/brownfeng/AudioQueueServiceDemo</a></p>\n"},{"title":"iOS音频系列(四)--音频的文件格式和数据格式","_content":"\n在音频开发中 .WAVs和.MP3有什么不同呢,或者还有的.AAC和.CAF之前又有什么区别呢,这些概念在这篇文章以后应该会有一定的理解.\n\n### 文件格式和数据格式\n\n如果要理解每一个音频文件,就需要了解它的两个部分的内容:文件格式和数据格式.文件格式又称为音频容器,数据格式又可以认为是编码格式.\n\n文件格式(音频容器)描述的是存储在文件系统的文件的本身,而存储在文件中的音频数据是可以被编码成各种各样的格式的.比如,我们常见的CAF文件是一个文件格式(音频容器),它可以用来存储音频编码格式为MP3,LPCM或者其他的音频编码格式.\n\n### 音频数据格式(音频编码)\n\n这里先来理解音频编码,在音频相关的内容中,音频编码是最重要的.\n\niPhone中支持的音频格式如下:\n\nAAC: 被设计用来取代MP3音频编码的.它会压缩原来的声音,因此会减少存储空间.实际中ACC比MP3更好的压缩率.\n\nAMR: AMR是一个编码格式用于压缩语音的音频编码格式.\n\nlinear PCM: 是标准的线性脉冲编码,一般是将模拟声音转化成数字信号,这是一个未压缩的音频格式.由于是未压缩的音频编码格式,因此播放时候用这种格式最是最好的选择,但是会占用过多的空间.\n\nMP3: ...\n\nIMA4: ...未压缩\n\n...\n\n### 该选哪个数据音频格式\n\n首先明确自己有哪些需求:\n\n* 如果用于播放的音频,选用LPCM,IMA4等其他的未压缩或者轻度压缩的音频格式.\n* 如果使用压缩率较高的AAC,MP3等这些iPhone直接硬件支持快速解码(解压缩).但是,硬件解码时候每次只支持一个文件.因此,如果需要同时播放多个需要解码(解压缩)的文件,就需要通过代码进行软件解码,非常慢.\n\n所以如何选择数据音频格式,这里有些建议:\n\n* 如果空间足够,那么最好使用的音频编码格式使用LPCM.不仅播放最快,而且可以同时播放多个音乐而不太占用CPU的资源.\n* 如果对空间有要求,最好使用ACC音频编码来进行音乐的播放,IMA4音频编码进行系统声音的编码.\n\n### 多种LPCM的变体\n\n对于LPCM音频编码是iPhone中使用非压缩音频数据最好的数据格式.同时,根据具体的存储方式,又有多种变种.音频数据可以存储于大端或者小端模式,用float或者integer存储,也可以使用不同的bit-width存储.\n\n而在iPhone中,使用的最平凡的是:little-endian integer 16bit(或者LEI16 short类型)的格式.在Mac中,使用native-endian(和电脑一致) float point 32bit.如果是在Mac上生成音频数据,那么最好生成合适的格式,再转化成iPhone使用的格式.\n\n### 文件格式(音频容器)\n\niPhone支持许多文件格式(音频容器)包括:MPEG-1(.mp3),MPEG-2 ADTS(.aac),AIFF,CAF,WAVE等.但是通常在iPhone中使用的容器格式就CAF,因为它可以用来封装iPhone所支持的所有音频格式.\n\n### Bit Rates比特率\n\n比特率是一个和音频数据格式关系密切的概念.\n\n音频文件的比特率就是只单位时间内传送的bit数,单位是bit/s,kbit/s.更高的比特率会导致更大的文件.我们在使用有些音频数据格式例如AAC或者MP3时,需要我们去设置比特率,这个参数与音频格式在压缩过程的压缩率有关.当我们让比特率变低,那么音频质量就会更差.\n\n>注释: 1kbit/s = 1000bit/s,而不是1024bit/s\n\n我们需要权衡比特率的大小和声音文件的质量,选择合适的比特率.如果我们使用的是语音声音,那么比特率可以适当低一点.\n\n下面是常见的比特率:\n\n* 32kbit/s: AM 无限电广播的质量\n* 48kbit/s: 很长的语音对话\n* 64kbit/s: 正常长度的语音对话的比特率\n* 96kbit/s: FM广播\n* 128kbit/s: MP3音乐\n* 329kbit/s: CD的比特率\n* 500kbit/s~1411kbit/s: 无损音频编码格式,比如LPCM\n\n\n### 采样率\n\n最后一个专业术语:采样率.可以见前面的文章.\n\n\n","source":"_posts/iOS音频系列(四).md","raw":"---\ntitle: iOS音频系列(四)--音频的文件格式和数据格式\ntags:\n- iOS\n- CoreAudio\n---\n\n在音频开发中 .WAVs和.MP3有什么不同呢,或者还有的.AAC和.CAF之前又有什么区别呢,这些概念在这篇文章以后应该会有一定的理解.\n\n### 文件格式和数据格式\n\n如果要理解每一个音频文件,就需要了解它的两个部分的内容:文件格式和数据格式.文件格式又称为音频容器,数据格式又可以认为是编码格式.\n\n文件格式(音频容器)描述的是存储在文件系统的文件的本身,而存储在文件中的音频数据是可以被编码成各种各样的格式的.比如,我们常见的CAF文件是一个文件格式(音频容器),它可以用来存储音频编码格式为MP3,LPCM或者其他的音频编码格式.\n\n### 音频数据格式(音频编码)\n\n这里先来理解音频编码,在音频相关的内容中,音频编码是最重要的.\n\niPhone中支持的音频格式如下:\n\nAAC: 被设计用来取代MP3音频编码的.它会压缩原来的声音,因此会减少存储空间.实际中ACC比MP3更好的压缩率.\n\nAMR: AMR是一个编码格式用于压缩语音的音频编码格式.\n\nlinear PCM: 是标准的线性脉冲编码,一般是将模拟声音转化成数字信号,这是一个未压缩的音频格式.由于是未压缩的音频编码格式,因此播放时候用这种格式最是最好的选择,但是会占用过多的空间.\n\nMP3: ...\n\nIMA4: ...未压缩\n\n...\n\n### 该选哪个数据音频格式\n\n首先明确自己有哪些需求:\n\n* 如果用于播放的音频,选用LPCM,IMA4等其他的未压缩或者轻度压缩的音频格式.\n* 如果使用压缩率较高的AAC,MP3等这些iPhone直接硬件支持快速解码(解压缩).但是,硬件解码时候每次只支持一个文件.因此,如果需要同时播放多个需要解码(解压缩)的文件,就需要通过代码进行软件解码,非常慢.\n\n所以如何选择数据音频格式,这里有些建议:\n\n* 如果空间足够,那么最好使用的音频编码格式使用LPCM.不仅播放最快,而且可以同时播放多个音乐而不太占用CPU的资源.\n* 如果对空间有要求,最好使用ACC音频编码来进行音乐的播放,IMA4音频编码进行系统声音的编码.\n\n### 多种LPCM的变体\n\n对于LPCM音频编码是iPhone中使用非压缩音频数据最好的数据格式.同时,根据具体的存储方式,又有多种变种.音频数据可以存储于大端或者小端模式,用float或者integer存储,也可以使用不同的bit-width存储.\n\n而在iPhone中,使用的最平凡的是:little-endian integer 16bit(或者LEI16 short类型)的格式.在Mac中,使用native-endian(和电脑一致) float point 32bit.如果是在Mac上生成音频数据,那么最好生成合适的格式,再转化成iPhone使用的格式.\n\n### 文件格式(音频容器)\n\niPhone支持许多文件格式(音频容器)包括:MPEG-1(.mp3),MPEG-2 ADTS(.aac),AIFF,CAF,WAVE等.但是通常在iPhone中使用的容器格式就CAF,因为它可以用来封装iPhone所支持的所有音频格式.\n\n### Bit Rates比特率\n\n比特率是一个和音频数据格式关系密切的概念.\n\n音频文件的比特率就是只单位时间内传送的bit数,单位是bit/s,kbit/s.更高的比特率会导致更大的文件.我们在使用有些音频数据格式例如AAC或者MP3时,需要我们去设置比特率,这个参数与音频格式在压缩过程的压缩率有关.当我们让比特率变低,那么音频质量就会更差.\n\n>注释: 1kbit/s = 1000bit/s,而不是1024bit/s\n\n我们需要权衡比特率的大小和声音文件的质量,选择合适的比特率.如果我们使用的是语音声音,那么比特率可以适当低一点.\n\n下面是常见的比特率:\n\n* 32kbit/s: AM 无限电广播的质量\n* 48kbit/s: 很长的语音对话\n* 64kbit/s: 正常长度的语音对话的比特率\n* 96kbit/s: FM广播\n* 128kbit/s: MP3音乐\n* 329kbit/s: CD的比特率\n* 500kbit/s~1411kbit/s: 无损音频编码格式,比如LPCM\n\n\n### 采样率\n\n最后一个专业术语:采样率.可以见前面的文章.\n\n\n","slug":"iOS音频系列(四)","published":1,"date":"2016-08-26T16:19:12.000Z","updated":"2016-08-26T16:19:12.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cisbyxx8b000b6squ4cr4uss4","content":"<p>在音频开发中 .WAVs和.MP3有什么不同呢,或者还有的.AAC和.CAF之前又有什么区别呢,这些概念在这篇文章以后应该会有一定的理解.</p>\n<h3 id=\"文件格式和数据格式\"><a href=\"#文件格式和数据格式\" class=\"headerlink\" title=\"文件格式和数据格式\"></a>文件格式和数据格式</h3><p>如果要理解每一个音频文件,就需要了解它的两个部分的内容:文件格式和数据格式.文件格式又称为音频容器,数据格式又可以认为是编码格式.</p>\n<p>文件格式(音频容器)描述的是存储在文件系统的文件的本身,而存储在文件中的音频数据是可以被编码成各种各样的格式的.比如,我们常见的CAF文件是一个文件格式(音频容器),它可以用来存储音频编码格式为MP3,LPCM或者其他的音频编码格式.</p>\n<h3 id=\"音频数据格式-音频编码\"><a href=\"#音频数据格式-音频编码\" class=\"headerlink\" title=\"音频数据格式(音频编码)\"></a>音频数据格式(音频编码)</h3><p>这里先来理解音频编码,在音频相关的内容中,音频编码是最重要的.</p>\n<p>iPhone中支持的音频格式如下:</p>\n<p>AAC: 被设计用来取代MP3音频编码的.它会压缩原来的声音,因此会减少存储空间.实际中ACC比MP3更好的压缩率.</p>\n<p>AMR: AMR是一个编码格式用于压缩语音的音频编码格式.</p>\n<p>linear PCM: 是标准的线性脉冲编码,一般是将模拟声音转化成数字信号,这是一个未压缩的音频格式.由于是未压缩的音频编码格式,因此播放时候用这种格式最是最好的选择,但是会占用过多的空间.</p>\n<p>MP3: …</p>\n<p>IMA4: …未压缩</p>\n<p>…</p>\n<h3 id=\"该选哪个数据音频格式\"><a href=\"#该选哪个数据音频格式\" class=\"headerlink\" title=\"该选哪个数据音频格式\"></a>该选哪个数据音频格式</h3><p>首先明确自己有哪些需求:</p>\n<ul>\n<li>如果用于播放的音频,选用LPCM,IMA4等其他的未压缩或者轻度压缩的音频格式.</li>\n<li>如果使用压缩率较高的AAC,MP3等这些iPhone直接硬件支持快速解码(解压缩).但是,硬件解码时候每次只支持一个文件.因此,如果需要同时播放多个需要解码(解压缩)的文件,就需要通过代码进行软件解码,非常慢.</li>\n</ul>\n<p>所以如何选择数据音频格式,这里有些建议:</p>\n<ul>\n<li>如果空间足够,那么最好使用的音频编码格式使用LPCM.不仅播放最快,而且可以同时播放多个音乐而不太占用CPU的资源.</li>\n<li>如果对空间有要求,最好使用ACC音频编码来进行音乐的播放,IMA4音频编码进行系统声音的编码.</li>\n</ul>\n<h3 id=\"多种LPCM的变体\"><a href=\"#多种LPCM的变体\" class=\"headerlink\" title=\"多种LPCM的变体\"></a>多种LPCM的变体</h3><p>对于LPCM音频编码是iPhone中使用非压缩音频数据最好的数据格式.同时,根据具体的存储方式,又有多种变种.音频数据可以存储于大端或者小端模式,用float或者integer存储,也可以使用不同的bit-width存储.</p>\n<p>而在iPhone中,使用的最平凡的是:little-endian integer 16bit(或者LEI16 short类型)的格式.在Mac中,使用native-endian(和电脑一致) float point 32bit.如果是在Mac上生成音频数据,那么最好生成合适的格式,再转化成iPhone使用的格式.</p>\n<h3 id=\"文件格式-音频容器\"><a href=\"#文件格式-音频容器\" class=\"headerlink\" title=\"文件格式(音频容器)\"></a>文件格式(音频容器)</h3><p>iPhone支持许多文件格式(音频容器)包括:MPEG-1(.mp3),MPEG-2 ADTS(.aac),AIFF,CAF,WAVE等.但是通常在iPhone中使用的容器格式就CAF,因为它可以用来封装iPhone所支持的所有音频格式.</p>\n<h3 id=\"Bit-Rates比特率\"><a href=\"#Bit-Rates比特率\" class=\"headerlink\" title=\"Bit Rates比特率\"></a>Bit Rates比特率</h3><p>比特率是一个和音频数据格式关系密切的概念.</p>\n<p>音频文件的比特率就是只单位时间内传送的bit数,单位是bit/s,kbit/s.更高的比特率会导致更大的文件.我们在使用有些音频数据格式例如AAC或者MP3时,需要我们去设置比特率,这个参数与音频格式在压缩过程的压缩率有关.当我们让比特率变低,那么音频质量就会更差.</p>\n<blockquote>\n<p>注释: 1kbit/s = 1000bit/s,而不是1024bit/s</p>\n</blockquote>\n<p>我们需要权衡比特率的大小和声音文件的质量,选择合适的比特率.如果我们使用的是语音声音,那么比特率可以适当低一点.</p>\n<p>下面是常见的比特率:</p>\n<ul>\n<li>32kbit/s: AM 无限电广播的质量</li>\n<li>48kbit/s: 很长的语音对话</li>\n<li>64kbit/s: 正常长度的语音对话的比特率</li>\n<li>96kbit/s: FM广播</li>\n<li>128kbit/s: MP3音乐</li>\n<li>329kbit/s: CD的比特率</li>\n<li>500kbit/s~1411kbit/s: 无损音频编码格式,比如LPCM</li>\n</ul>\n<h3 id=\"采样率\"><a href=\"#采样率\" class=\"headerlink\" title=\"采样率\"></a>采样率</h3><p>最后一个专业术语:采样率.可以见前面的文章.</p>\n","excerpt":"","more":"<p>在音频开发中 .WAVs和.MP3有什么不同呢,或者还有的.AAC和.CAF之前又有什么区别呢,这些概念在这篇文章以后应该会有一定的理解.</p>\n<h3 id=\"文件格式和数据格式\"><a href=\"#文件格式和数据格式\" class=\"headerlink\" title=\"文件格式和数据格式\"></a>文件格式和数据格式</h3><p>如果要理解每一个音频文件,就需要了解它的两个部分的内容:文件格式和数据格式.文件格式又称为音频容器,数据格式又可以认为是编码格式.</p>\n<p>文件格式(音频容器)描述的是存储在文件系统的文件的本身,而存储在文件中的音频数据是可以被编码成各种各样的格式的.比如,我们常见的CAF文件是一个文件格式(音频容器),它可以用来存储音频编码格式为MP3,LPCM或者其他的音频编码格式.</p>\n<h3 id=\"音频数据格式-音频编码\"><a href=\"#音频数据格式-音频编码\" class=\"headerlink\" title=\"音频数据格式(音频编码)\"></a>音频数据格式(音频编码)</h3><p>这里先来理解音频编码,在音频相关的内容中,音频编码是最重要的.</p>\n<p>iPhone中支持的音频格式如下:</p>\n<p>AAC: 被设计用来取代MP3音频编码的.它会压缩原来的声音,因此会减少存储空间.实际中ACC比MP3更好的压缩率.</p>\n<p>AMR: AMR是一个编码格式用于压缩语音的音频编码格式.</p>\n<p>linear PCM: 是标准的线性脉冲编码,一般是将模拟声音转化成数字信号,这是一个未压缩的音频格式.由于是未压缩的音频编码格式,因此播放时候用这种格式最是最好的选择,但是会占用过多的空间.</p>\n<p>MP3: …</p>\n<p>IMA4: …未压缩</p>\n<p>…</p>\n<h3 id=\"该选哪个数据音频格式\"><a href=\"#该选哪个数据音频格式\" class=\"headerlink\" title=\"该选哪个数据音频格式\"></a>该选哪个数据音频格式</h3><p>首先明确自己有哪些需求:</p>\n<ul>\n<li>如果用于播放的音频,选用LPCM,IMA4等其他的未压缩或者轻度压缩的音频格式.</li>\n<li>如果使用压缩率较高的AAC,MP3等这些iPhone直接硬件支持快速解码(解压缩).但是,硬件解码时候每次只支持一个文件.因此,如果需要同时播放多个需要解码(解压缩)的文件,就需要通过代码进行软件解码,非常慢.</li>\n</ul>\n<p>所以如何选择数据音频格式,这里有些建议:</p>\n<ul>\n<li>如果空间足够,那么最好使用的音频编码格式使用LPCM.不仅播放最快,而且可以同时播放多个音乐而不太占用CPU的资源.</li>\n<li>如果对空间有要求,最好使用ACC音频编码来进行音乐的播放,IMA4音频编码进行系统声音的编码.</li>\n</ul>\n<h3 id=\"多种LPCM的变体\"><a href=\"#多种LPCM的变体\" class=\"headerlink\" title=\"多种LPCM的变体\"></a>多种LPCM的变体</h3><p>对于LPCM音频编码是iPhone中使用非压缩音频数据最好的数据格式.同时,根据具体的存储方式,又有多种变种.音频数据可以存储于大端或者小端模式,用float或者integer存储,也可以使用不同的bit-width存储.</p>\n<p>而在iPhone中,使用的最平凡的是:little-endian integer 16bit(或者LEI16 short类型)的格式.在Mac中,使用native-endian(和电脑一致) float point 32bit.如果是在Mac上生成音频数据,那么最好生成合适的格式,再转化成iPhone使用的格式.</p>\n<h3 id=\"文件格式-音频容器\"><a href=\"#文件格式-音频容器\" class=\"headerlink\" title=\"文件格式(音频容器)\"></a>文件格式(音频容器)</h3><p>iPhone支持许多文件格式(音频容器)包括:MPEG-1(.mp3),MPEG-2 ADTS(.aac),AIFF,CAF,WAVE等.但是通常在iPhone中使用的容器格式就CAF,因为它可以用来封装iPhone所支持的所有音频格式.</p>\n<h3 id=\"Bit-Rates比特率\"><a href=\"#Bit-Rates比特率\" class=\"headerlink\" title=\"Bit Rates比特率\"></a>Bit Rates比特率</h3><p>比特率是一个和音频数据格式关系密切的概念.</p>\n<p>音频文件的比特率就是只单位时间内传送的bit数,单位是bit/s,kbit/s.更高的比特率会导致更大的文件.我们在使用有些音频数据格式例如AAC或者MP3时,需要我们去设置比特率,这个参数与音频格式在压缩过程的压缩率有关.当我们让比特率变低,那么音频质量就会更差.</p>\n<blockquote>\n<p>注释: 1kbit/s = 1000bit/s,而不是1024bit/s</p>\n</blockquote>\n<p>我们需要权衡比特率的大小和声音文件的质量,选择合适的比特率.如果我们使用的是语音声音,那么比特率可以适当低一点.</p>\n<p>下面是常见的比特率:</p>\n<ul>\n<li>32kbit/s: AM 无限电广播的质量</li>\n<li>48kbit/s: 很长的语音对话</li>\n<li>64kbit/s: 正常长度的语音对话的比特率</li>\n<li>96kbit/s: FM广播</li>\n<li>128kbit/s: MP3音乐</li>\n<li>329kbit/s: CD的比特率</li>\n<li>500kbit/s~1411kbit/s: 无损音频编码格式,比如LPCM</li>\n</ul>\n<h3 id=\"采样率\"><a href=\"#采样率\" class=\"headerlink\" title=\"采样率\"></a>采样率</h3><p>最后一个专业术语:采样率.可以见前面的文章.</p>\n"},{"title":"iOS音频系列(二)--CoreAudio","_content":"\n这一篇主要是CoreAudio官方文档的重点内容的笔记。\n\n### 通过回调函数与CoreAudio交互\n\niOS的CoreAudio是通过callback函数与App交互的。其中需要设置回调函数有以下几种情况：\n\n* CoreAudio会向回调函数给App传入PCM音频数据，然后App需要在回调函数中将音频数据写入文件文件系统。（录音时候）\n* CoreAudio会需要向App请求一些音频数据，App通过从文件系统中读取音频数据，然后通过callback函数传递给CoreAudio。（播放时候）\n* 通过注册属性观察者，监听CoreAudio的属性，注册回调函数\n\n下面是一个使用Audio Queue Services的属性监听器的callback函数的调用模板。\n\n```\ntypedef void (*AudioQueuePropertyListenerProc) (\n                void *                  inUserData,\n                AudioQueueRef           inAQ,\n                AudioQueuePropertyID    inID\n            );\n\n```\n\n在实现和使用这个callback函数时候，你需要完成两件事：\n\n* 实现这个函数。例如，你可以实现property listener callback，根据audio是否在running或者stop状态，去改变更新UI。\n* 注册callback函数时候带上userData数据，在callback函数触发时候使用。\n\n下面是一个property listener callback函数的实现：\n\n```\nstatic void propertyListenerCallback (\n    void                    *inUserData,\n    AudioQueueRef           queueObject,\n    AudioQueuePropertyID    propertyID\n) {\n    AudioPlayer *player = (AudioPlayer *) inUserData;\n        // gets a reference to the playback object\n    [player.notificationDelegate updateUserInterfaceOnAudioQueueStateChange: player];\n        // your notificationDelegate class implements the UI update method\n}\n```\n\n下面是注册一个callback函数的例子：\n\n```\nAudioQueueAddPropertyListener (\n    self.queueObject,                // the object that will invoke your callback\n    kAudioQueueProperty_IsRunning,   // the ID of the property you want to listen for\n    propertyListenerCallback,        // a reference to your callback function\n    self\n);\n```\n\n********\n\n### Audio Data Formats\n\n这部分内容是iOS中支持的音频格式，理解以后对后面的音频相关的编程能够理解更加深刻。\n\n#### iOS中通用的音频数据类型\n\n在CoreAudio中，使用`AudioStreamBasicDescription`和`AudioStreamPacketDescription`这两个类型描述了通用的音频数据类型,包括压缩音频数据，非压缩的音频数据。他们的数据结构如下：\n\n``` objc\nstruct AudioStreamBasicDescription {\n    Float64 mSampleRate;\n    UInt32  mFormatID;\n    UInt32  mFormatFlags;\n    UInt32  mBytesPerPacket;\n    UInt32  mFramesPerPacket;\n    UInt32  mBytesPerFrame;\n    UInt32  mChannelsPerFrame;\n    UInt32  mBitsPerChannel;\n    UInt32  mReserved;\t\t\t\t//0\n};\ntypedef struct AudioStreamBasicDescription  AudioStreamBasicDescription;\n\nstruct  AudioStreamPacketDescription {\n    SInt64  mStartOffset;\n    UInt32  mVariableFramesInPacket;\n    UInt32  mDataByteSize;\n};\ntypedef struct AudioStreamPacketDescription AudioStreamPacketDescription;\n```\n\n>compressed audio formats use a varying number of bits per sample. For these formats, the value of the mBitsPerChannel member is 0.\n\n#### Audio Data Packets\n\n前面定义过，将一个或者多个frames称为一个packet。或者说packet是最有意义的一组frames，它在audio file中代表一个有意义的时间单元。使用Core Audio中一般是对packets进行处理的。\n\n每个audio data格式在packets被装配完成以后，它的fromat就被确定了。ASBD数据结构通过`mBytesPerPacket`和`mFramesPerPacket`描述音频格式的packet信息，其中页包含其他的信息。\n\n在整个Core Audio中可能会用到三种不同的packets：\n\n* CBR (constant bit rate) formats：例如 linear PCM and IMA/ADPCM，所有的packet使用相同的大小。\n* VBR (variable bit rate) formats：例如 AAC，Apple Lossless，MP3，所有的packets拥有相同的frames，但是每个sample中的bits数目不同。\n* VFR (variable frame rate) formats：packets拥有数目不同的的frames。\n\n在Core Audio中使用VBR或者VFR格式，使用ASPD结构体只能用来描述单个packet。如果是record或者play VBR或者VFR音频文件，需要涉及到多个ASPD结构。\n\n在AudioFileService等接口中，都是通过pakcets工作的。例如`AudioFileReadPackets`会得到一系列的packets，同时会得到一个数组的`AudioStreamPacketDescription`。\n\n下面是通过packets计算audio data buffer的大小：\n\n\t\t\t\n``` objc\n- (void) calculateSizesFor: (Float64) seconds {\n \n    UInt32 maxPacketSize;\n    UInt32 propertySize = sizeof (maxPacketSize);\n \n    AudioFileGetProperty (\n        audioFileID,\n        kAudioFilePropertyPacketSizeUpperBound,\n        &propertySize,\n        &maxPacketSize\n    );\n \n    static const int maxBufferSize = 0x10000;   // limit maximum size to 64K\n    static const int minBufferSize = 0x4000;    // limit minimum size to 16K\n \n    if (audioFormat.mFramesPerPacket) {\n        Float64 numPacketsForTime =\n            audioFormat.mSampleRate / audioFormat.mFramesPerPacket * seconds;\n        [self setBufferByteSize: numPacketsForTime * maxPacketSize];\n    } else {\n        // if frames per packet is zero, then the codec doesn't know the\n        // relationship between packets and time. Return a default buffer size\n        [self setBufferByteSize:\n            maxBufferSize > maxPacketSize ? maxBufferSize : maxPacketSize];\n    }\n \n    // clamp buffer size to our specified range\n    if (bufferByteSize > maxBufferSize && bufferByteSize > maxPacketSize) {\n        [self setBufferByteSize: maxBufferSize];\n    } else {\n        if (bufferByteSize < minBufferSize) {\n            [self setBufferByteSize: minBufferSize];\n        }\n    }\n \n    [self setNumPacketsToRead: self.bufferByteSize / maxPacketSize];\n}\n```\n\n#### Data Format Conversion\n\n将音频数据从一种audio data转换成另外一种audio data。常见的有三中音频格式转换：\n\n* Decoding an audio format (such as AAC (Advanced Audio Coding)) to linear PCM format.\n* Converting linear PCM data into a different audio format.\n* Converting between different variants of linear PCM (for example, converting 16-bit signed integer linear PCM to 8.24 fixed-point linear PCM).\n\n*******\n\n### Sound Files\n\n如果要使用声音文件，你需要使用Audio File Services的接口。一般而且，在iOS中需要与音频文件的创建，操作都离不开Audio File Services。\n\n使用`AudioFileGetGlobalInfoSize`和`AudioFileGetGlobalInfo`分别分配info的内存和获取info的内容。你可以获取以下的内容：\n\n* Readable file types\n* Writable file types\n* For each writable type, the audio data formats you can put into the file\n\n#### Creating a New Sound File\n\n为了创建一个能够存储音频数据的audio file，你需要进行以下三步：\n\n* 使用CFURL或者NSURL表示的系统文件的路径\n* 你需要创建的文件的类型的标识identifier，这些identifier定义在`Audio File Types`枚举中。例如，为了创建一个CAF文件，你需要使用`kAudioFileCAFType`的identifier。\n* 创建过程中你需要提供音频数据的ASBD结构体。为了获取ASBD，你可以先提供ASBD结构体的部分成员的值，然后通过函数让`Audio File Services`将剩余的信息填满。\n\n下面是创建一个AudioFile的方法：\n\n``` objc\nAudioFileCreateWithURL (\n    audioFileURL,\n    kAudioFileCAFType,\n    &audioFormat,\n    kAudioFileFlags_EraseFile,\n    &audioFileID   // the function provides the new file object here\n);\n```\n\n#### Opening a Sound File\n\n为了打开sound file，需要使用`AudioFileOpenURL`函数，该函数会返回一个唯一ID，供后面使用。\n\n为了获取sound file的一些属性，通常使用`AudioFileGetPropertyInfo`和`AudioFileGetProperty`，日常使用的属性以下：\n\n* kAudioFilePropertyFileFormat\n* kAudioFilePropertyDataFormat\n* kAudioFilePropertyMagicCookieData\n* kAudioFilePropertyChannelLayout\n\n#### Reading From and Writing To a Sound File\n\niOS中，我们经常需要使用`Audio File Services`去读写audio data到sound file中。读和写是一对相反的内容，操作的对象都可以是bytes或者packets，但是一般而言都是直接使用的packets。\n\n>* 读写VBR数据，只能使用packet\n* 直接使用packet，更加容易计算时间\n\n******\n\n### iPhone Audio File Formats\n\niOS支持的sound file格式如下：\n\n|Format name|Format filename extensions|\n|:---------:|:------------------------:|\n|AIFF|.aif, .aiff|\n|CAF|.caf|\n|MPEG-1, layer 3|.mp3|\n|MPEG-2 or MPEG-4 ADTS|.aac|\n|MPEG-4|.m4a, .mp4|\n|WAV|.wav|\n\niOS中的native format是CAF file format。\n\n****\n\n### Audio Sessions: Cooperating with Core Audio\n\n在iOS中，app在运行过程中有可能接到电话，如果此时正在播放sound，系统会做一定的处理。\n\nAudioSession就是在这种情况的中间人，每个app都会有一个audio session。在播放或者录音时候需要session在做正确的事情，需要我们自己弄清楚以下的情况：\n\n* app收到系统的中断时候应该如何响应，比如收到phone call？\n* 你是否需要app的sound和其他后台运行的app的sounds混合播放，或者需要独占播放？\n* 你需要app如何响应远音频路径的响应，比如拔插耳机时候\n\nAudioSession提供了三种类型的接口：\n\n* **Categories**： A category is a key that identifies a set of audio behaviors for your application. By setting a category, you indicate your audio intentions to iOS, such as whether your audio should continue when the screen locks.\n* **Interruptions and route changes** ：Your audio session posts notifications when your audio is interrupted, when an interruption ends, and when the hardware audio route changes. These notifications let you respond to changes in the larger audio environment—such as an interruption due to in an incoming phone call—gracefully.\n* **Hardware characteristics**：You can query the audio session to discover characteristics of the device your application is running on, such as hardware sample rate, number of hardware channels, and whether audio input is available.\n\n#### Audio Session Default Behavior\nAudio Session拥有一些默认的行为策略：\n\n* 当用户将静音开关静音时，audio就会静音。\n* 当用户锁屏（手动，自动）时候，audio就会静音。\n* 当你app的audio启动时，其他app正在使用的audio就会静音。\n\naudio session的这个特定的默认的行为策略被称为`kAudioSessionCategory_SoloAmbientSound`。同时，它还包括其他的多种策略选择。\n\n#### Interruptions: Deactivation and Activation\n\n默认的audio session的一个典型的特征是，audio会在中断以后自动恢复活动。Audio session有两个重要的状态：`active`和`inactive`。只有当Audio session处于`active`状态时候，app才能使用audio。\n\n在app启动以后，你的默认的audio session就会是`active`状态。然而，如果一个电话被打进来，你的session就会立刻被置为`inactive`，然后app中的audio就会停止。这个电话就被称为一个中断，如果用户选择忽略电话，app就会继续运行。但是此时你的audio session依然会是`inactive`状态，audio也就不会工作。\n\n如果你使用 Audio Queue Services操作audio，我们就需要给中断注册listener回调函数，手动去重启audio session。具体内容可以见`Audio Session Programming Guide`。\n\n#### Determining if Audio Input is Available\n\n一个录音的app只有在设备的音频硬件可用的时候才能录音。为了检查这个属性，需要使用audio session的`kAudioSessionProperty_AudioInputAvailable`属性。\n\n``` objc\nUInt32 audioInputIsAvailable;\nUInt32 propertySize = sizeof (audioInputIsAvailable);\n \nAudioSessionGetProperty (\n    kAudioSessionProperty_AudioInputAvailable,\n    &propertySize,\n    &audioInputIsAvailable // A nonzero value on output means that\n                           // audio input is available\n);\n```\n#### Using Your Audio Session\n\n你的app同时只能有一个audio session策略，你的所有的audio都需要遵循这个`active`策略的特点。如何响应中断，在Audio Session Programming Guide`中有更加详细的内容。\n\n> 如果要测试Audio session，需要使用真机\n\n****\n\n### Playback using the AVAudioPlayer Class\n\n`AVAudioPlayer`提供了简单的OC接口用于audio播放。如果非网络stream，或者需要精确控制，apple推荐使用这个类，它可以用于播放iOS支持的任何audio format，同时这个类并不需要去设置audio session，因为它会在中断发生以后自动恢复播放，除非你需要指定特地的行为。\n\n它可以完成以下工作：\n\n* Play sounds of any duration\n* Play sounds from files or memory buffers\n* Loop sounds\n* Play multiple sounds simultaneously\n* Control relative playback level for each sound you are playing\n* Seek to a particular point in a sound file, which supports such application features as fast forward and rewind\n* Obtain data that you can use for audio level metering\n\n下面就是使用`AVAudioPlayer`的具体流程：\n\n1. Configuring an AVAudioPlayer object\n\n``` objc\nNSString *soundFilePath =\n                [[NSBundle mainBundle] pathForResource: @\"sound\"\n                                                ofType: @\"wav\"];\n \nNSURL *fileURL = [[NSURL alloc] initFileURLWithPath: soundFilePath];\n \nAVAudioPlayer *newPlayer =\n                [[AVAudioPlayer alloc] initWithContentsOfURL: fileURL\n                                                       error: nil];\n[fileURL release];\n \nself.player = newPlayer;\n[newPlayer release];\n \n[self.player prepareToPlay];\n[self.player setDelegate: self];\n```\n\n你的delegate对象用于处理`interruptions`或者音频播放停止以后的操作。\n\n2. Implementing an AVAudioPlayer delegate method \n\n``` objc\n- (void) audioPlayerDidFinishPlaying: (AVAudioPlayer *) player\n                        successfully: (BOOL) flag {\n    if (flag == YES) {\n        [self.button setTitle: @\"Play\" forState: UIControlStateNormal];\n    }\n}\n```\n\n3. Controlling an AVAudioPlayer object\n\n``` objc\n- (IBAction) playOrPause: (id) sender {\n \n    // if already playing, then pause\n    if (self.player.playing) {\n        [self.button setTitle: @\"Play\" forState: UIControlStateHighlighted];\n        [self.button setTitle: @\"Play\" forState: UIControlStateNormal];\n        [self.player pause];\n \n    // if stopped or paused, start playing\n    } else {\n        [self.button setTitle: @\"Pause\" forState: UIControlStateHighlighted];\n        [self.button setTitle: @\"Pause\" forState: UIControlStateNormal];\n        [self.player play];\n    }\n}\n```\n\n****\n\n### Recording and Playback using Audio Queue Services\n\nAudio Queue Services是一个更加直观的record和play audio的方式。同时它还有更多个高级功能，可以使用这个服务完成更多的工作，比如对LPCM数据进行压缩等等。它和`AVAudioPlayer`是iOS中唯二可以播放压缩后音频格式的接口。使用Audio Queue Service播放和录音都是通过回调方法完成的。\n\n#### Creating an Audio Queue Object\n\n为了创建Audio Queue对象，它分成两类：\n\n* AudioQueueNewInput用于录音\n* AudioQueueNewOutput用于播放\n\n使用audio queue object播放audio file，需要一些几个步骤：\n\n1. 创建数据结构用于管理audio queue需要的信息，例如audio format，audio fileID等。\n2. 定义callback函数，用于管理audio queue buffers。callback会使用Audio File Service去读取audio file用来播放\n3. 使用AudioQueueNewOutput用来播放audio file。\n\nCreating an audio queue object具体的代码如下：\n\n``` objc\nstatic const int kNumberBuffers = 3;\n// Create a data structure to manage information needed by the audio queue\nstruct myAQStruct {\n    AudioFileID                     mAudioFile;\n    CAStreamBasicDescription        mDataFormat;\n    AudioQueueRef                   mQueue;\n    AudioQueueBufferRef             mBuffers[kNumberBuffers];\n    SInt64                          mCurrentPacket;\n    UInt32                          mNumPacketsToRead;\n    AudioStreamPacketDescription    *mPacketDescs;\n    bool                            mDone;\n};\n// Define a playback audio queue callback function\nstatic void AQTestBufferCallback(\n    void                   *inUserData,\n    AudioQueueRef          inAQ,\n    AudioQueueBufferRef    inCompleteAQBuffer\n) {\n    myAQStruct *myInfo = (myAQStruct *)inUserData;\n    if (myInfo->mDone) return;\n    UInt32 numBytes;\n    UInt32 nPackets = myInfo->mNumPacketsToRead;\n \n    AudioFileReadPackets (\n        myInfo->mAudioFile,\n        false,\n        &numBytes,\n        myInfo->mPacketDescs,\n        myInfo->mCurrentPacket,\n        &nPackets,\n        inCompleteAQBuffer->mAudioData\n    );\n    if (nPackets > 0) {\n        inCompleteAQBuffer->mAudioDataByteSize = numBytes;\n        AudioQueueEnqueueBuffer (\n            inAQ,\n            inCompleteAQBuffer,\n            (myInfo->mPacketDescs ? nPackets : 0),\n            myInfo->mPacketDescs\n        );\n        myInfo->mCurrentPacket += nPackets;\n    } else {\n        AudioQueueStop (\n            myInfo->mQueue,\n            false\n        );\n        myInfo->mDone = true;\n    }\n}\n// Instantiate an audio queue object\nAudioQueueNewOutput (\n    &myInfo.mDataFormat,\n    AQTestBufferCallback,\n    &myInfo,\n    CFRunLoopGetCurrent(),\n    kCFRunLoopCommonModes,\n    0,\n    &myInfo.mQueue\n);\n```\n\n#### Controlling Audio Queue Playback Level\n\nAudio Queue对象提供了两个方式控制音频level。第一种是直接使用`AudioQueueSetParameter`以及`kAudioQueueParam_Volume`参数，就可以设置，设置完成以后会立即生效。\n\n``` objc\nFloat32 volume = 1;\nAudioQueueSetParameter (\n    myAQstruct.audioQueueObject,\n    kAudioQueueParam_Volume,\n    volume\n);\n\n```\n\n也可以通过`AudioQueueEnqueueBufferWithParameters`给audio queue buffer设置。这种方式只有在audio queue buffer 开始播放时候才起作用。\n\n#### Indicating Audio Queue Playback Level\n\n你也可以直接查询audio queue的`kAudioQueueProperty_CurrentLevelMeterDB`属性，得到的值是一组`AudioQueueLevelMeterState`结构体（一个channel一个数组），具体的结构体是`AudioQueueLevelMeterState`，显示如下：\n\n```\ntypedef struct AudioQueueLevelMeterState {\n    Float32     mAveragePower;\n    Float32     mPeakPower;\n};  AudioQueueLevelMeterState;\n```\n\n*****\n\n### System Sounds: Alerts and Sound Effects\n\n如果你需要播放的音频时间少于30s，那么可以使用System Sound Services。调用`AudioServicesPlaySystemSound`函数可以立即播放一个sound file。你也可以调用`AudioServicesPlayAlertSound`播放alert声音。这两个方法都会在手机静音的情况下振动。\n\n当然，你也在调用`AudioServicesPlaySystemSound`方法使用`kSystemSoundID_Vibrate`属性，显示的触发振动。\n\n为了使用`AudioServicesPlaySystemSound`方法播放sound，首先需要将sound file注册到系统中，得到一个sound ID，然后才能播放。\n\n下面一段代码显示了使用System Sound Services去play sound：\n\n``` objc\n#include <AudioToolbox/AudioToolbox.h>\n#include <CoreFoundation/CoreFoundation.h>\n \n// Define a callback to be called when the sound is finished\n// playing. Useful when you need to free memory after playing.\nstatic void MyCompletionCallback (\n    SystemSoundID  mySSID,\n    void * myURLRef\n) {\n        AudioServicesDisposeSystemSoundID (mySSID);\n        CFRelease (myURLRef);\n        CFRunLoopStop (CFRunLoopGetCurrent());\n}\n \nint main (int argc, const char * argv[]) {\n    // Set up the pieces needed to play a sound.\n    SystemSoundID    mySSID;\n    CFURLRef        myURLRef;\n    myURLRef = CFURLCreateWithFileSystemPath (\n        kCFAllocatorDefault,\n        CFSTR (\"../../ComedyHorns.aif\"),\n        kCFURLPOSIXPathStyle,\n        FALSE\n    );\n \n    // create a system sound ID to represent the sound file\n    OSStatus error = AudioServicesCreateSystemSoundID (myURLRef, &mySSID);\n \n    // Register the sound completion callback.\n    // Again, useful when you need to free memory after playing.\n    AudioServicesAddSystemSoundCompletion (\n        mySSID,\n        NULL,\n        NULL,\n        MyCompletionCallback,\n        (void *) myURLRef\n    );\n \n    // Play the sound file.\n    AudioServicesPlaySystemSound (mySSID);\n \n    // Invoke a run loop on the current thread to keep the application\n    // running long enough for the sound to play; the sound completion\n    // callback later stops this run loop.\n    CFRunLoopRun ();\n    return 0;\n}\n```","source":"_posts/iOS音频系列(二).md","raw":"---\ntitle: iOS音频系列(二)--CoreAudio\ntags:\n- iOS\n- CoreAudio\n---\n\n这一篇主要是CoreAudio官方文档的重点内容的笔记。\n\n### 通过回调函数与CoreAudio交互\n\niOS的CoreAudio是通过callback函数与App交互的。其中需要设置回调函数有以下几种情况：\n\n* CoreAudio会向回调函数给App传入PCM音频数据，然后App需要在回调函数中将音频数据写入文件文件系统。（录音时候）\n* CoreAudio会需要向App请求一些音频数据，App通过从文件系统中读取音频数据，然后通过callback函数传递给CoreAudio。（播放时候）\n* 通过注册属性观察者，监听CoreAudio的属性，注册回调函数\n\n下面是一个使用Audio Queue Services的属性监听器的callback函数的调用模板。\n\n```\ntypedef void (*AudioQueuePropertyListenerProc) (\n                void *                  inUserData,\n                AudioQueueRef           inAQ,\n                AudioQueuePropertyID    inID\n            );\n\n```\n\n在实现和使用这个callback函数时候，你需要完成两件事：\n\n* 实现这个函数。例如，你可以实现property listener callback，根据audio是否在running或者stop状态，去改变更新UI。\n* 注册callback函数时候带上userData数据，在callback函数触发时候使用。\n\n下面是一个property listener callback函数的实现：\n\n```\nstatic void propertyListenerCallback (\n    void                    *inUserData,\n    AudioQueueRef           queueObject,\n    AudioQueuePropertyID    propertyID\n) {\n    AudioPlayer *player = (AudioPlayer *) inUserData;\n        // gets a reference to the playback object\n    [player.notificationDelegate updateUserInterfaceOnAudioQueueStateChange: player];\n        // your notificationDelegate class implements the UI update method\n}\n```\n\n下面是注册一个callback函数的例子：\n\n```\nAudioQueueAddPropertyListener (\n    self.queueObject,                // the object that will invoke your callback\n    kAudioQueueProperty_IsRunning,   // the ID of the property you want to listen for\n    propertyListenerCallback,        // a reference to your callback function\n    self\n);\n```\n\n********\n\n### Audio Data Formats\n\n这部分内容是iOS中支持的音频格式，理解以后对后面的音频相关的编程能够理解更加深刻。\n\n#### iOS中通用的音频数据类型\n\n在CoreAudio中，使用`AudioStreamBasicDescription`和`AudioStreamPacketDescription`这两个类型描述了通用的音频数据类型,包括压缩音频数据，非压缩的音频数据。他们的数据结构如下：\n\n``` objc\nstruct AudioStreamBasicDescription {\n    Float64 mSampleRate;\n    UInt32  mFormatID;\n    UInt32  mFormatFlags;\n    UInt32  mBytesPerPacket;\n    UInt32  mFramesPerPacket;\n    UInt32  mBytesPerFrame;\n    UInt32  mChannelsPerFrame;\n    UInt32  mBitsPerChannel;\n    UInt32  mReserved;\t\t\t\t//0\n};\ntypedef struct AudioStreamBasicDescription  AudioStreamBasicDescription;\n\nstruct  AudioStreamPacketDescription {\n    SInt64  mStartOffset;\n    UInt32  mVariableFramesInPacket;\n    UInt32  mDataByteSize;\n};\ntypedef struct AudioStreamPacketDescription AudioStreamPacketDescription;\n```\n\n>compressed audio formats use a varying number of bits per sample. For these formats, the value of the mBitsPerChannel member is 0.\n\n#### Audio Data Packets\n\n前面定义过，将一个或者多个frames称为一个packet。或者说packet是最有意义的一组frames，它在audio file中代表一个有意义的时间单元。使用Core Audio中一般是对packets进行处理的。\n\n每个audio data格式在packets被装配完成以后，它的fromat就被确定了。ASBD数据结构通过`mBytesPerPacket`和`mFramesPerPacket`描述音频格式的packet信息，其中页包含其他的信息。\n\n在整个Core Audio中可能会用到三种不同的packets：\n\n* CBR (constant bit rate) formats：例如 linear PCM and IMA/ADPCM，所有的packet使用相同的大小。\n* VBR (variable bit rate) formats：例如 AAC，Apple Lossless，MP3，所有的packets拥有相同的frames，但是每个sample中的bits数目不同。\n* VFR (variable frame rate) formats：packets拥有数目不同的的frames。\n\n在Core Audio中使用VBR或者VFR格式，使用ASPD结构体只能用来描述单个packet。如果是record或者play VBR或者VFR音频文件，需要涉及到多个ASPD结构。\n\n在AudioFileService等接口中，都是通过pakcets工作的。例如`AudioFileReadPackets`会得到一系列的packets，同时会得到一个数组的`AudioStreamPacketDescription`。\n\n下面是通过packets计算audio data buffer的大小：\n\n\t\t\t\n``` objc\n- (void) calculateSizesFor: (Float64) seconds {\n \n    UInt32 maxPacketSize;\n    UInt32 propertySize = sizeof (maxPacketSize);\n \n    AudioFileGetProperty (\n        audioFileID,\n        kAudioFilePropertyPacketSizeUpperBound,\n        &propertySize,\n        &maxPacketSize\n    );\n \n    static const int maxBufferSize = 0x10000;   // limit maximum size to 64K\n    static const int minBufferSize = 0x4000;    // limit minimum size to 16K\n \n    if (audioFormat.mFramesPerPacket) {\n        Float64 numPacketsForTime =\n            audioFormat.mSampleRate / audioFormat.mFramesPerPacket * seconds;\n        [self setBufferByteSize: numPacketsForTime * maxPacketSize];\n    } else {\n        // if frames per packet is zero, then the codec doesn't know the\n        // relationship between packets and time. Return a default buffer size\n        [self setBufferByteSize:\n            maxBufferSize > maxPacketSize ? maxBufferSize : maxPacketSize];\n    }\n \n    // clamp buffer size to our specified range\n    if (bufferByteSize > maxBufferSize && bufferByteSize > maxPacketSize) {\n        [self setBufferByteSize: maxBufferSize];\n    } else {\n        if (bufferByteSize < minBufferSize) {\n            [self setBufferByteSize: minBufferSize];\n        }\n    }\n \n    [self setNumPacketsToRead: self.bufferByteSize / maxPacketSize];\n}\n```\n\n#### Data Format Conversion\n\n将音频数据从一种audio data转换成另外一种audio data。常见的有三中音频格式转换：\n\n* Decoding an audio format (such as AAC (Advanced Audio Coding)) to linear PCM format.\n* Converting linear PCM data into a different audio format.\n* Converting between different variants of linear PCM (for example, converting 16-bit signed integer linear PCM to 8.24 fixed-point linear PCM).\n\n*******\n\n### Sound Files\n\n如果要使用声音文件，你需要使用Audio File Services的接口。一般而且，在iOS中需要与音频文件的创建，操作都离不开Audio File Services。\n\n使用`AudioFileGetGlobalInfoSize`和`AudioFileGetGlobalInfo`分别分配info的内存和获取info的内容。你可以获取以下的内容：\n\n* Readable file types\n* Writable file types\n* For each writable type, the audio data formats you can put into the file\n\n#### Creating a New Sound File\n\n为了创建一个能够存储音频数据的audio file，你需要进行以下三步：\n\n* 使用CFURL或者NSURL表示的系统文件的路径\n* 你需要创建的文件的类型的标识identifier，这些identifier定义在`Audio File Types`枚举中。例如，为了创建一个CAF文件，你需要使用`kAudioFileCAFType`的identifier。\n* 创建过程中你需要提供音频数据的ASBD结构体。为了获取ASBD，你可以先提供ASBD结构体的部分成员的值，然后通过函数让`Audio File Services`将剩余的信息填满。\n\n下面是创建一个AudioFile的方法：\n\n``` objc\nAudioFileCreateWithURL (\n    audioFileURL,\n    kAudioFileCAFType,\n    &audioFormat,\n    kAudioFileFlags_EraseFile,\n    &audioFileID   // the function provides the new file object here\n);\n```\n\n#### Opening a Sound File\n\n为了打开sound file，需要使用`AudioFileOpenURL`函数，该函数会返回一个唯一ID，供后面使用。\n\n为了获取sound file的一些属性，通常使用`AudioFileGetPropertyInfo`和`AudioFileGetProperty`，日常使用的属性以下：\n\n* kAudioFilePropertyFileFormat\n* kAudioFilePropertyDataFormat\n* kAudioFilePropertyMagicCookieData\n* kAudioFilePropertyChannelLayout\n\n#### Reading From and Writing To a Sound File\n\niOS中，我们经常需要使用`Audio File Services`去读写audio data到sound file中。读和写是一对相反的内容，操作的对象都可以是bytes或者packets，但是一般而言都是直接使用的packets。\n\n>* 读写VBR数据，只能使用packet\n* 直接使用packet，更加容易计算时间\n\n******\n\n### iPhone Audio File Formats\n\niOS支持的sound file格式如下：\n\n|Format name|Format filename extensions|\n|:---------:|:------------------------:|\n|AIFF|.aif, .aiff|\n|CAF|.caf|\n|MPEG-1, layer 3|.mp3|\n|MPEG-2 or MPEG-4 ADTS|.aac|\n|MPEG-4|.m4a, .mp4|\n|WAV|.wav|\n\niOS中的native format是CAF file format。\n\n****\n\n### Audio Sessions: Cooperating with Core Audio\n\n在iOS中，app在运行过程中有可能接到电话，如果此时正在播放sound，系统会做一定的处理。\n\nAudioSession就是在这种情况的中间人，每个app都会有一个audio session。在播放或者录音时候需要session在做正确的事情，需要我们自己弄清楚以下的情况：\n\n* app收到系统的中断时候应该如何响应，比如收到phone call？\n* 你是否需要app的sound和其他后台运行的app的sounds混合播放，或者需要独占播放？\n* 你需要app如何响应远音频路径的响应，比如拔插耳机时候\n\nAudioSession提供了三种类型的接口：\n\n* **Categories**： A category is a key that identifies a set of audio behaviors for your application. By setting a category, you indicate your audio intentions to iOS, such as whether your audio should continue when the screen locks.\n* **Interruptions and route changes** ：Your audio session posts notifications when your audio is interrupted, when an interruption ends, and when the hardware audio route changes. These notifications let you respond to changes in the larger audio environment—such as an interruption due to in an incoming phone call—gracefully.\n* **Hardware characteristics**：You can query the audio session to discover characteristics of the device your application is running on, such as hardware sample rate, number of hardware channels, and whether audio input is available.\n\n#### Audio Session Default Behavior\nAudio Session拥有一些默认的行为策略：\n\n* 当用户将静音开关静音时，audio就会静音。\n* 当用户锁屏（手动，自动）时候，audio就会静音。\n* 当你app的audio启动时，其他app正在使用的audio就会静音。\n\naudio session的这个特定的默认的行为策略被称为`kAudioSessionCategory_SoloAmbientSound`。同时，它还包括其他的多种策略选择。\n\n#### Interruptions: Deactivation and Activation\n\n默认的audio session的一个典型的特征是，audio会在中断以后自动恢复活动。Audio session有两个重要的状态：`active`和`inactive`。只有当Audio session处于`active`状态时候，app才能使用audio。\n\n在app启动以后，你的默认的audio session就会是`active`状态。然而，如果一个电话被打进来，你的session就会立刻被置为`inactive`，然后app中的audio就会停止。这个电话就被称为一个中断，如果用户选择忽略电话，app就会继续运行。但是此时你的audio session依然会是`inactive`状态，audio也就不会工作。\n\n如果你使用 Audio Queue Services操作audio，我们就需要给中断注册listener回调函数，手动去重启audio session。具体内容可以见`Audio Session Programming Guide`。\n\n#### Determining if Audio Input is Available\n\n一个录音的app只有在设备的音频硬件可用的时候才能录音。为了检查这个属性，需要使用audio session的`kAudioSessionProperty_AudioInputAvailable`属性。\n\n``` objc\nUInt32 audioInputIsAvailable;\nUInt32 propertySize = sizeof (audioInputIsAvailable);\n \nAudioSessionGetProperty (\n    kAudioSessionProperty_AudioInputAvailable,\n    &propertySize,\n    &audioInputIsAvailable // A nonzero value on output means that\n                           // audio input is available\n);\n```\n#### Using Your Audio Session\n\n你的app同时只能有一个audio session策略，你的所有的audio都需要遵循这个`active`策略的特点。如何响应中断，在Audio Session Programming Guide`中有更加详细的内容。\n\n> 如果要测试Audio session，需要使用真机\n\n****\n\n### Playback using the AVAudioPlayer Class\n\n`AVAudioPlayer`提供了简单的OC接口用于audio播放。如果非网络stream，或者需要精确控制，apple推荐使用这个类，它可以用于播放iOS支持的任何audio format，同时这个类并不需要去设置audio session，因为它会在中断发生以后自动恢复播放，除非你需要指定特地的行为。\n\n它可以完成以下工作：\n\n* Play sounds of any duration\n* Play sounds from files or memory buffers\n* Loop sounds\n* Play multiple sounds simultaneously\n* Control relative playback level for each sound you are playing\n* Seek to a particular point in a sound file, which supports such application features as fast forward and rewind\n* Obtain data that you can use for audio level metering\n\n下面就是使用`AVAudioPlayer`的具体流程：\n\n1. Configuring an AVAudioPlayer object\n\n``` objc\nNSString *soundFilePath =\n                [[NSBundle mainBundle] pathForResource: @\"sound\"\n                                                ofType: @\"wav\"];\n \nNSURL *fileURL = [[NSURL alloc] initFileURLWithPath: soundFilePath];\n \nAVAudioPlayer *newPlayer =\n                [[AVAudioPlayer alloc] initWithContentsOfURL: fileURL\n                                                       error: nil];\n[fileURL release];\n \nself.player = newPlayer;\n[newPlayer release];\n \n[self.player prepareToPlay];\n[self.player setDelegate: self];\n```\n\n你的delegate对象用于处理`interruptions`或者音频播放停止以后的操作。\n\n2. Implementing an AVAudioPlayer delegate method \n\n``` objc\n- (void) audioPlayerDidFinishPlaying: (AVAudioPlayer *) player\n                        successfully: (BOOL) flag {\n    if (flag == YES) {\n        [self.button setTitle: @\"Play\" forState: UIControlStateNormal];\n    }\n}\n```\n\n3. Controlling an AVAudioPlayer object\n\n``` objc\n- (IBAction) playOrPause: (id) sender {\n \n    // if already playing, then pause\n    if (self.player.playing) {\n        [self.button setTitle: @\"Play\" forState: UIControlStateHighlighted];\n        [self.button setTitle: @\"Play\" forState: UIControlStateNormal];\n        [self.player pause];\n \n    // if stopped or paused, start playing\n    } else {\n        [self.button setTitle: @\"Pause\" forState: UIControlStateHighlighted];\n        [self.button setTitle: @\"Pause\" forState: UIControlStateNormal];\n        [self.player play];\n    }\n}\n```\n\n****\n\n### Recording and Playback using Audio Queue Services\n\nAudio Queue Services是一个更加直观的record和play audio的方式。同时它还有更多个高级功能，可以使用这个服务完成更多的工作，比如对LPCM数据进行压缩等等。它和`AVAudioPlayer`是iOS中唯二可以播放压缩后音频格式的接口。使用Audio Queue Service播放和录音都是通过回调方法完成的。\n\n#### Creating an Audio Queue Object\n\n为了创建Audio Queue对象，它分成两类：\n\n* AudioQueueNewInput用于录音\n* AudioQueueNewOutput用于播放\n\n使用audio queue object播放audio file，需要一些几个步骤：\n\n1. 创建数据结构用于管理audio queue需要的信息，例如audio format，audio fileID等。\n2. 定义callback函数，用于管理audio queue buffers。callback会使用Audio File Service去读取audio file用来播放\n3. 使用AudioQueueNewOutput用来播放audio file。\n\nCreating an audio queue object具体的代码如下：\n\n``` objc\nstatic const int kNumberBuffers = 3;\n// Create a data structure to manage information needed by the audio queue\nstruct myAQStruct {\n    AudioFileID                     mAudioFile;\n    CAStreamBasicDescription        mDataFormat;\n    AudioQueueRef                   mQueue;\n    AudioQueueBufferRef             mBuffers[kNumberBuffers];\n    SInt64                          mCurrentPacket;\n    UInt32                          mNumPacketsToRead;\n    AudioStreamPacketDescription    *mPacketDescs;\n    bool                            mDone;\n};\n// Define a playback audio queue callback function\nstatic void AQTestBufferCallback(\n    void                   *inUserData,\n    AudioQueueRef          inAQ,\n    AudioQueueBufferRef    inCompleteAQBuffer\n) {\n    myAQStruct *myInfo = (myAQStruct *)inUserData;\n    if (myInfo->mDone) return;\n    UInt32 numBytes;\n    UInt32 nPackets = myInfo->mNumPacketsToRead;\n \n    AudioFileReadPackets (\n        myInfo->mAudioFile,\n        false,\n        &numBytes,\n        myInfo->mPacketDescs,\n        myInfo->mCurrentPacket,\n        &nPackets,\n        inCompleteAQBuffer->mAudioData\n    );\n    if (nPackets > 0) {\n        inCompleteAQBuffer->mAudioDataByteSize = numBytes;\n        AudioQueueEnqueueBuffer (\n            inAQ,\n            inCompleteAQBuffer,\n            (myInfo->mPacketDescs ? nPackets : 0),\n            myInfo->mPacketDescs\n        );\n        myInfo->mCurrentPacket += nPackets;\n    } else {\n        AudioQueueStop (\n            myInfo->mQueue,\n            false\n        );\n        myInfo->mDone = true;\n    }\n}\n// Instantiate an audio queue object\nAudioQueueNewOutput (\n    &myInfo.mDataFormat,\n    AQTestBufferCallback,\n    &myInfo,\n    CFRunLoopGetCurrent(),\n    kCFRunLoopCommonModes,\n    0,\n    &myInfo.mQueue\n);\n```\n\n#### Controlling Audio Queue Playback Level\n\nAudio Queue对象提供了两个方式控制音频level。第一种是直接使用`AudioQueueSetParameter`以及`kAudioQueueParam_Volume`参数，就可以设置，设置完成以后会立即生效。\n\n``` objc\nFloat32 volume = 1;\nAudioQueueSetParameter (\n    myAQstruct.audioQueueObject,\n    kAudioQueueParam_Volume,\n    volume\n);\n\n```\n\n也可以通过`AudioQueueEnqueueBufferWithParameters`给audio queue buffer设置。这种方式只有在audio queue buffer 开始播放时候才起作用。\n\n#### Indicating Audio Queue Playback Level\n\n你也可以直接查询audio queue的`kAudioQueueProperty_CurrentLevelMeterDB`属性，得到的值是一组`AudioQueueLevelMeterState`结构体（一个channel一个数组），具体的结构体是`AudioQueueLevelMeterState`，显示如下：\n\n```\ntypedef struct AudioQueueLevelMeterState {\n    Float32     mAveragePower;\n    Float32     mPeakPower;\n};  AudioQueueLevelMeterState;\n```\n\n*****\n\n### System Sounds: Alerts and Sound Effects\n\n如果你需要播放的音频时间少于30s，那么可以使用System Sound Services。调用`AudioServicesPlaySystemSound`函数可以立即播放一个sound file。你也可以调用`AudioServicesPlayAlertSound`播放alert声音。这两个方法都会在手机静音的情况下振动。\n\n当然，你也在调用`AudioServicesPlaySystemSound`方法使用`kSystemSoundID_Vibrate`属性，显示的触发振动。\n\n为了使用`AudioServicesPlaySystemSound`方法播放sound，首先需要将sound file注册到系统中，得到一个sound ID，然后才能播放。\n\n下面一段代码显示了使用System Sound Services去play sound：\n\n``` objc\n#include <AudioToolbox/AudioToolbox.h>\n#include <CoreFoundation/CoreFoundation.h>\n \n// Define a callback to be called when the sound is finished\n// playing. Useful when you need to free memory after playing.\nstatic void MyCompletionCallback (\n    SystemSoundID  mySSID,\n    void * myURLRef\n) {\n        AudioServicesDisposeSystemSoundID (mySSID);\n        CFRelease (myURLRef);\n        CFRunLoopStop (CFRunLoopGetCurrent());\n}\n \nint main (int argc, const char * argv[]) {\n    // Set up the pieces needed to play a sound.\n    SystemSoundID    mySSID;\n    CFURLRef        myURLRef;\n    myURLRef = CFURLCreateWithFileSystemPath (\n        kCFAllocatorDefault,\n        CFSTR (\"../../ComedyHorns.aif\"),\n        kCFURLPOSIXPathStyle,\n        FALSE\n    );\n \n    // create a system sound ID to represent the sound file\n    OSStatus error = AudioServicesCreateSystemSoundID (myURLRef, &mySSID);\n \n    // Register the sound completion callback.\n    // Again, useful when you need to free memory after playing.\n    AudioServicesAddSystemSoundCompletion (\n        mySSID,\n        NULL,\n        NULL,\n        MyCompletionCallback,\n        (void *) myURLRef\n    );\n \n    // Play the sound file.\n    AudioServicesPlaySystemSound (mySSID);\n \n    // Invoke a run loop on the current thread to keep the application\n    // running long enough for the sound to play; the sound completion\n    // callback later stops this run loop.\n    CFRunLoopRun ();\n    return 0;\n}\n```","slug":"iOS音频系列(二)","published":1,"date":"2016-08-26T16:19:12.000Z","updated":"2016-08-26T16:19:12.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cisbyxx8d000d6squgzz9e5gg","content":"<p>这一篇主要是CoreAudio官方文档的重点内容的笔记。</p>\n<h3 id=\"通过回调函数与CoreAudio交互\"><a href=\"#通过回调函数与CoreAudio交互\" class=\"headerlink\" title=\"通过回调函数与CoreAudio交互\"></a>通过回调函数与CoreAudio交互</h3><p>iOS的CoreAudio是通过callback函数与App交互的。其中需要设置回调函数有以下几种情况：</p>\n<ul>\n<li>CoreAudio会向回调函数给App传入PCM音频数据，然后App需要在回调函数中将音频数据写入文件文件系统。（录音时候）</li>\n<li>CoreAudio会需要向App请求一些音频数据，App通过从文件系统中读取音频数据，然后通过callback函数传递给CoreAudio。（播放时候）</li>\n<li>通过注册属性观察者，监听CoreAudio的属性，注册回调函数</li>\n</ul>\n<p>下面是一个使用Audio Queue Services的属性监听器的callback函数的调用模板。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">typedef void (*AudioQueuePropertyListenerProc) (</div><div class=\"line\">                void *                  inUserData,</div><div class=\"line\">                AudioQueueRef           inAQ,</div><div class=\"line\">                AudioQueuePropertyID    inID</div><div class=\"line\">            );</div></pre></td></tr></table></figure>\n<p>在实现和使用这个callback函数时候，你需要完成两件事：</p>\n<ul>\n<li>实现这个函数。例如，你可以实现property listener callback，根据audio是否在running或者stop状态，去改变更新UI。</li>\n<li>注册callback函数时候带上userData数据，在callback函数触发时候使用。</li>\n</ul>\n<p>下面是一个property listener callback函数的实现：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">static void propertyListenerCallback (</div><div class=\"line\">    void                    *inUserData,</div><div class=\"line\">    AudioQueueRef           queueObject,</div><div class=\"line\">    AudioQueuePropertyID    propertyID</div><div class=\"line\">) &#123;</div><div class=\"line\">    AudioPlayer *player = (AudioPlayer *) inUserData;</div><div class=\"line\">        // gets a reference to the playback object</div><div class=\"line\">    [player.notificationDelegate updateUserInterfaceOnAudioQueueStateChange: player];</div><div class=\"line\">        // your notificationDelegate class implements the UI update method</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>下面是注册一个callback函数的例子：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">AudioQueueAddPropertyListener (</div><div class=\"line\">    self.queueObject,                // the object that will invoke your callback</div><div class=\"line\">    kAudioQueueProperty_IsRunning,   // the ID of the property you want to listen for</div><div class=\"line\">    propertyListenerCallback,        // a reference to your callback function</div><div class=\"line\">    self</div><div class=\"line\">);</div></pre></td></tr></table></figure>\n<hr>\n<h3 id=\"Audio-Data-Formats\"><a href=\"#Audio-Data-Formats\" class=\"headerlink\" title=\"Audio Data Formats\"></a>Audio Data Formats</h3><p>这部分内容是iOS中支持的音频格式，理解以后对后面的音频相关的编程能够理解更加深刻。</p>\n<h4 id=\"iOS中通用的音频数据类型\"><a href=\"#iOS中通用的音频数据类型\" class=\"headerlink\" title=\"iOS中通用的音频数据类型\"></a>iOS中通用的音频数据类型</h4><p>在CoreAudio中，使用<code>AudioStreamBasicDescription</code>和<code>AudioStreamPacketDescription</code>这两个类型描述了通用的音频数据类型,包括压缩音频数据，非压缩的音频数据。他们的数据结构如下：</p>\n<figure class=\"highlight objc\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">struct</span> AudioStreamBasicDescription &#123;</div><div class=\"line\">    Float64 mSampleRate;</div><div class=\"line\">    <span class=\"built_in\">UInt32</span>  mFormatID;</div><div class=\"line\">    <span class=\"built_in\">UInt32</span>  mFormatFlags;</div><div class=\"line\">    <span class=\"built_in\">UInt32</span>  mBytesPerPacket;</div><div class=\"line\">    <span class=\"built_in\">UInt32</span>  mFramesPerPacket;</div><div class=\"line\">    <span class=\"built_in\">UInt32</span>  mBytesPerFrame;</div><div class=\"line\">    <span class=\"built_in\">UInt32</span>  mChannelsPerFrame;</div><div class=\"line\">    <span class=\"built_in\">UInt32</span>  mBitsPerChannel;</div><div class=\"line\">    <span class=\"built_in\">UInt32</span>  mReserved;\t\t\t\t<span class=\"comment\">//0</span></div><div class=\"line\">&#125;;</div><div class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"keyword\">struct</span> AudioStreamBasicDescription  AudioStreamBasicDescription;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">struct</span>  AudioStreamPacketDescription &#123;</div><div class=\"line\">    SInt64  mStartOffset;</div><div class=\"line\">    <span class=\"built_in\">UInt32</span>  mVariableFramesInPacket;</div><div class=\"line\">    <span class=\"built_in\">UInt32</span>  mDataByteSize;</div><div class=\"line\">&#125;;</div><div class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"keyword\">struct</span> AudioStreamPacketDescription AudioStreamPacketDescription;</div></pre></td></tr></table></figure>\n<blockquote>\n<p>compressed audio formats use a varying number of bits per sample. For these formats, the value of the mBitsPerChannel member is 0.</p>\n</blockquote>\n<h4 id=\"Audio-Data-Packets\"><a href=\"#Audio-Data-Packets\" class=\"headerlink\" title=\"Audio Data Packets\"></a>Audio Data Packets</h4><p>前面定义过，将一个或者多个frames称为一个packet。或者说packet是最有意义的一组frames，它在audio file中代表一个有意义的时间单元。使用Core Audio中一般是对packets进行处理的。</p>\n<p>每个audio data格式在packets被装配完成以后，它的fromat就被确定了。ASBD数据结构通过<code>mBytesPerPacket</code>和<code>mFramesPerPacket</code>描述音频格式的packet信息，其中页包含其他的信息。</p>\n<p>在整个Core Audio中可能会用到三种不同的packets：</p>\n<ul>\n<li>CBR (constant bit rate) formats：例如 linear PCM and IMA/ADPCM，所有的packet使用相同的大小。</li>\n<li>VBR (variable bit rate) formats：例如 AAC，Apple Lossless，MP3，所有的packets拥有相同的frames，但是每个sample中的bits数目不同。</li>\n<li>VFR (variable frame rate) formats：packets拥有数目不同的的frames。</li>\n</ul>\n<p>在Core Audio中使用VBR或者VFR格式，使用ASPD结构体只能用来描述单个packet。如果是record或者play VBR或者VFR音频文件，需要涉及到多个ASPD结构。</p>\n<p>在AudioFileService等接口中，都是通过pakcets工作的。例如<code>AudioFileReadPackets</code>会得到一系列的packets，同时会得到一个数组的<code>AudioStreamPacketDescription</code>。</p>\n<p>下面是通过packets计算audio data buffer的大小：</p>\n<figure class=\"highlight objc\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div></pre></td><td class=\"code\"><pre><div class=\"line\">- (<span class=\"keyword\">void</span>) calculateSizesFor: (Float64) seconds &#123;</div><div class=\"line\"> </div><div class=\"line\">    <span class=\"built_in\">UInt32</span> maxPacketSize;</div><div class=\"line\">    <span class=\"built_in\">UInt32</span> propertySize = <span class=\"keyword\">sizeof</span> (maxPacketSize);</div><div class=\"line\"> </div><div class=\"line\">    AudioFileGetProperty (</div><div class=\"line\">        audioFileID,</div><div class=\"line\">        kAudioFilePropertyPacketSizeUpperBound,</div><div class=\"line\">        &amp;propertySize,</div><div class=\"line\">        &amp;maxPacketSize</div><div class=\"line\">    );</div><div class=\"line\"> </div><div class=\"line\">    <span class=\"keyword\">static</span> <span class=\"keyword\">const</span> <span class=\"keyword\">int</span> maxBufferSize = <span class=\"number\">0x10000</span>;   <span class=\"comment\">// limit maximum size to 64K</span></div><div class=\"line\">    <span class=\"keyword\">static</span> <span class=\"keyword\">const</span> <span class=\"keyword\">int</span> minBufferSize = <span class=\"number\">0x4000</span>;    <span class=\"comment\">// limit minimum size to 16K</span></div><div class=\"line\"> </div><div class=\"line\">    <span class=\"keyword\">if</span> (audioFormat.mFramesPerPacket) &#123;</div><div class=\"line\">        Float64 numPacketsForTime =</div><div class=\"line\">            audioFormat.mSampleRate / audioFormat.mFramesPerPacket * seconds;</div><div class=\"line\">        [<span class=\"keyword\">self</span> setBufferByteSize: numPacketsForTime * maxPacketSize];</div><div class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</div><div class=\"line\">        <span class=\"comment\">// if frames per packet is zero, then the codec doesn't know the</span></div><div class=\"line\">        <span class=\"comment\">// relationship between packets and time. Return a default buffer size</span></div><div class=\"line\">        [<span class=\"keyword\">self</span> setBufferByteSize:</div><div class=\"line\">            maxBufferSize &gt; maxPacketSize ? maxBufferSize : maxPacketSize];</div><div class=\"line\">    &#125;</div><div class=\"line\"> </div><div class=\"line\">    <span class=\"comment\">// clamp buffer size to our specified range</span></div><div class=\"line\">    <span class=\"keyword\">if</span> (bufferByteSize &gt; maxBufferSize &amp;&amp; bufferByteSize &gt; maxPacketSize) &#123;</div><div class=\"line\">        [<span class=\"keyword\">self</span> setBufferByteSize: maxBufferSize];</div><div class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</div><div class=\"line\">        <span class=\"keyword\">if</span> (bufferByteSize &lt; minBufferSize) &#123;</div><div class=\"line\">            [<span class=\"keyword\">self</span> setBufferByteSize: minBufferSize];</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\"> </div><div class=\"line\">    [<span class=\"keyword\">self</span> setNumPacketsToRead: <span class=\"keyword\">self</span>.bufferByteSize / maxPacketSize];</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h4 id=\"Data-Format-Conversion\"><a href=\"#Data-Format-Conversion\" class=\"headerlink\" title=\"Data Format Conversion\"></a>Data Format Conversion</h4><p>将音频数据从一种audio data转换成另外一种audio data。常见的有三中音频格式转换：</p>\n<ul>\n<li>Decoding an audio format (such as AAC (Advanced Audio Coding)) to linear PCM format.</li>\n<li>Converting linear PCM data into a different audio format.</li>\n<li>Converting between different variants of linear PCM (for example, converting 16-bit signed integer linear PCM to 8.24 fixed-point linear PCM).</li>\n</ul>\n<hr>\n<h3 id=\"Sound-Files\"><a href=\"#Sound-Files\" class=\"headerlink\" title=\"Sound Files\"></a>Sound Files</h3><p>如果要使用声音文件，你需要使用Audio File Services的接口。一般而且，在iOS中需要与音频文件的创建，操作都离不开Audio File Services。</p>\n<p>使用<code>AudioFileGetGlobalInfoSize</code>和<code>AudioFileGetGlobalInfo</code>分别分配info的内存和获取info的内容。你可以获取以下的内容：</p>\n<ul>\n<li>Readable file types</li>\n<li>Writable file types</li>\n<li>For each writable type, the audio data formats you can put into the file</li>\n</ul>\n<h4 id=\"Creating-a-New-Sound-File\"><a href=\"#Creating-a-New-Sound-File\" class=\"headerlink\" title=\"Creating a New Sound File\"></a>Creating a New Sound File</h4><p>为了创建一个能够存储音频数据的audio file，你需要进行以下三步：</p>\n<ul>\n<li>使用CFURL或者NSURL表示的系统文件的路径</li>\n<li>你需要创建的文件的类型的标识identifier，这些identifier定义在<code>Audio File Types</code>枚举中。例如，为了创建一个CAF文件，你需要使用<code>kAudioFileCAFType</code>的identifier。</li>\n<li>创建过程中你需要提供音频数据的ASBD结构体。为了获取ASBD，你可以先提供ASBD结构体的部分成员的值，然后通过函数让<code>Audio File Services</code>将剩余的信息填满。</li>\n</ul>\n<p>下面是创建一个AudioFile的方法：</p>\n<figure class=\"highlight objc\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">AudioFileCreateWithURL (</div><div class=\"line\">    audioFileURL,</div><div class=\"line\">    kAudioFileCAFType,</div><div class=\"line\">    &amp;audioFormat,</div><div class=\"line\">    kAudioFileFlags_EraseFile,</div><div class=\"line\">    &amp;audioFileID   <span class=\"comment\">// the function provides the new file object here</span></div><div class=\"line\">);</div></pre></td></tr></table></figure>\n<h4 id=\"Opening-a-Sound-File\"><a href=\"#Opening-a-Sound-File\" class=\"headerlink\" title=\"Opening a Sound File\"></a>Opening a Sound File</h4><p>为了打开sound file，需要使用<code>AudioFileOpenURL</code>函数，该函数会返回一个唯一ID，供后面使用。</p>\n<p>为了获取sound file的一些属性，通常使用<code>AudioFileGetPropertyInfo</code>和<code>AudioFileGetProperty</code>，日常使用的属性以下：</p>\n<ul>\n<li>kAudioFilePropertyFileFormat</li>\n<li>kAudioFilePropertyDataFormat</li>\n<li>kAudioFilePropertyMagicCookieData</li>\n<li>kAudioFilePropertyChannelLayout</li>\n</ul>\n<h4 id=\"Reading-From-and-Writing-To-a-Sound-File\"><a href=\"#Reading-From-and-Writing-To-a-Sound-File\" class=\"headerlink\" title=\"Reading From and Writing To a Sound File\"></a>Reading From and Writing To a Sound File</h4><p>iOS中，我们经常需要使用<code>Audio File Services</code>去读写audio data到sound file中。读和写是一对相反的内容，操作的对象都可以是bytes或者packets，但是一般而言都是直接使用的packets。</p>\n<blockquote>\n<ul>\n<li>读写VBR数据，只能使用packet</li>\n<li>直接使用packet，更加容易计算时间</li>\n</ul>\n</blockquote>\n<hr>\n<h3 id=\"iPhone-Audio-File-Formats\"><a href=\"#iPhone-Audio-File-Formats\" class=\"headerlink\" title=\"iPhone Audio File Formats\"></a>iPhone Audio File Formats</h3><p>iOS支持的sound file格式如下：</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">Format name</th>\n<th style=\"text-align:center\">Format filename extensions</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">AIFF</td>\n<td style=\"text-align:center\">.aif, .aiff</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">CAF</td>\n<td style=\"text-align:center\">.caf</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">MPEG-1, layer 3</td>\n<td style=\"text-align:center\">.mp3</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">MPEG-2 or MPEG-4 ADTS</td>\n<td style=\"text-align:center\">.aac</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">MPEG-4</td>\n<td style=\"text-align:center\">.m4a, .mp4</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">WAV</td>\n<td style=\"text-align:center\">.wav</td>\n</tr>\n</tbody>\n</table>\n<p>iOS中的native format是CAF file format。</p>\n<hr>\n<h3 id=\"Audio-Sessions-Cooperating-with-Core-Audio\"><a href=\"#Audio-Sessions-Cooperating-with-Core-Audio\" class=\"headerlink\" title=\"Audio Sessions: Cooperating with Core Audio\"></a>Audio Sessions: Cooperating with Core Audio</h3><p>在iOS中，app在运行过程中有可能接到电话，如果此时正在播放sound，系统会做一定的处理。</p>\n<p>AudioSession就是在这种情况的中间人，每个app都会有一个audio session。在播放或者录音时候需要session在做正确的事情，需要我们自己弄清楚以下的情况：</p>\n<ul>\n<li>app收到系统的中断时候应该如何响应，比如收到phone call？</li>\n<li>你是否需要app的sound和其他后台运行的app的sounds混合播放，或者需要独占播放？</li>\n<li>你需要app如何响应远音频路径的响应，比如拔插耳机时候</li>\n</ul>\n<p>AudioSession提供了三种类型的接口：</p>\n<ul>\n<li><strong>Categories</strong>： A category is a key that identifies a set of audio behaviors for your application. By setting a category, you indicate your audio intentions to iOS, such as whether your audio should continue when the screen locks.</li>\n<li><strong>Interruptions and route changes</strong> ：Your audio session posts notifications when your audio is interrupted, when an interruption ends, and when the hardware audio route changes. These notifications let you respond to changes in the larger audio environment—such as an interruption due to in an incoming phone call—gracefully.</li>\n<li><strong>Hardware characteristics</strong>：You can query the audio session to discover characteristics of the device your application is running on, such as hardware sample rate, number of hardware channels, and whether audio input is available.</li>\n</ul>\n<h4 id=\"Audio-Session-Default-Behavior\"><a href=\"#Audio-Session-Default-Behavior\" class=\"headerlink\" title=\"Audio Session Default Behavior\"></a>Audio Session Default Behavior</h4><p>Audio Session拥有一些默认的行为策略：</p>\n<ul>\n<li>当用户将静音开关静音时，audio就会静音。</li>\n<li>当用户锁屏（手动，自动）时候，audio就会静音。</li>\n<li>当你app的audio启动时，其他app正在使用的audio就会静音。</li>\n</ul>\n<p>audio session的这个特定的默认的行为策略被称为<code>kAudioSessionCategory_SoloAmbientSound</code>。同时，它还包括其他的多种策略选择。</p>\n<h4 id=\"Interruptions-Deactivation-and-Activation\"><a href=\"#Interruptions-Deactivation-and-Activation\" class=\"headerlink\" title=\"Interruptions: Deactivation and Activation\"></a>Interruptions: Deactivation and Activation</h4><p>默认的audio session的一个典型的特征是，audio会在中断以后自动恢复活动。Audio session有两个重要的状态：<code>active</code>和<code>inactive</code>。只有当Audio session处于<code>active</code>状态时候，app才能使用audio。</p>\n<p>在app启动以后，你的默认的audio session就会是<code>active</code>状态。然而，如果一个电话被打进来，你的session就会立刻被置为<code>inactive</code>，然后app中的audio就会停止。这个电话就被称为一个中断，如果用户选择忽略电话，app就会继续运行。但是此时你的audio session依然会是<code>inactive</code>状态，audio也就不会工作。</p>\n<p>如果你使用 Audio Queue Services操作audio，我们就需要给中断注册listener回调函数，手动去重启audio session。具体内容可以见<code>Audio Session Programming Guide</code>。</p>\n<h4 id=\"Determining-if-Audio-Input-is-Available\"><a href=\"#Determining-if-Audio-Input-is-Available\" class=\"headerlink\" title=\"Determining if Audio Input is Available\"></a>Determining if Audio Input is Available</h4><p>一个录音的app只有在设备的音频硬件可用的时候才能录音。为了检查这个属性，需要使用audio session的<code>kAudioSessionProperty_AudioInputAvailable</code>属性。</p>\n<figure class=\"highlight objc\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"built_in\">UInt32</span> audioInputIsAvailable;</div><div class=\"line\"><span class=\"built_in\">UInt32</span> propertySize = <span class=\"keyword\">sizeof</span> (audioInputIsAvailable);</div><div class=\"line\"> </div><div class=\"line\">AudioSessionGetProperty (</div><div class=\"line\">    kAudioSessionProperty_AudioInputAvailable,</div><div class=\"line\">    &amp;propertySize,</div><div class=\"line\">    &amp;audioInputIsAvailable <span class=\"comment\">// A nonzero value on output means that</span></div><div class=\"line\">                           <span class=\"comment\">// audio input is available</span></div><div class=\"line\">);</div></pre></td></tr></table></figure>\n<h4 id=\"Using-Your-Audio-Session\"><a href=\"#Using-Your-Audio-Session\" class=\"headerlink\" title=\"Using Your Audio Session\"></a>Using Your Audio Session</h4><p>你的app同时只能有一个audio session策略，你的所有的audio都需要遵循这个<code>active</code>策略的特点。如何响应中断，在Audio Session Programming Guide`中有更加详细的内容。</p>\n<blockquote>\n<p>如果要测试Audio session，需要使用真机</p>\n</blockquote>\n<hr>\n<h3 id=\"Playback-using-the-AVAudioPlayer-Class\"><a href=\"#Playback-using-the-AVAudioPlayer-Class\" class=\"headerlink\" title=\"Playback using the AVAudioPlayer Class\"></a>Playback using the AVAudioPlayer Class</h3><p><code>AVAudioPlayer</code>提供了简单的OC接口用于audio播放。如果非网络stream，或者需要精确控制，apple推荐使用这个类，它可以用于播放iOS支持的任何audio format，同时这个类并不需要去设置audio session，因为它会在中断发生以后自动恢复播放，除非你需要指定特地的行为。</p>\n<p>它可以完成以下工作：</p>\n<ul>\n<li>Play sounds of any duration</li>\n<li>Play sounds from files or memory buffers</li>\n<li>Loop sounds</li>\n<li>Play multiple sounds simultaneously</li>\n<li>Control relative playback level for each sound you are playing</li>\n<li>Seek to a particular point in a sound file, which supports such application features as fast forward and rewind</li>\n<li>Obtain data that you can use for audio level metering</li>\n</ul>\n<p>下面就是使用<code>AVAudioPlayer</code>的具体流程：</p>\n<ol>\n<li>Configuring an AVAudioPlayer object</li>\n</ol>\n<figure class=\"highlight objc\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"built_in\">NSString</span> *soundFilePath =</div><div class=\"line\">                [[<span class=\"built_in\">NSBundle</span> mainBundle] pathForResource: <span class=\"string\">@\"sound\"</span></div><div class=\"line\">                                                ofType: <span class=\"string\">@\"wav\"</span>];</div><div class=\"line\"> </div><div class=\"line\"><span class=\"built_in\">NSURL</span> *fileURL = [[<span class=\"built_in\">NSURL</span> alloc] initFileURLWithPath: soundFilePath];</div><div class=\"line\"> </div><div class=\"line\"><span class=\"built_in\">AVAudioPlayer</span> *newPlayer =</div><div class=\"line\">                [[<span class=\"built_in\">AVAudioPlayer</span> alloc] initWithContentsOfURL: fileURL</div><div class=\"line\">                                                       error: <span class=\"literal\">nil</span>];</div><div class=\"line\">[fileURL release];</div><div class=\"line\"> </div><div class=\"line\"><span class=\"keyword\">self</span>.player = newPlayer;</div><div class=\"line\">[newPlayer release];</div><div class=\"line\"> </div><div class=\"line\">[<span class=\"keyword\">self</span>.player prepareToPlay];</div><div class=\"line\">[<span class=\"keyword\">self</span>.player setDelegate: <span class=\"keyword\">self</span>];</div></pre></td></tr></table></figure>\n<p>你的delegate对象用于处理<code>interruptions</code>或者音频播放停止以后的操作。</p>\n<ol>\n<li>Implementing an AVAudioPlayer delegate method </li>\n</ol>\n<figure class=\"highlight objc\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">- (<span class=\"keyword\">void</span>) audioPlayerDidFinishPlaying: (<span class=\"built_in\">AVAudioPlayer</span> *) player</div><div class=\"line\">                        successfully: (<span class=\"built_in\">BOOL</span>) flag &#123;</div><div class=\"line\">    <span class=\"keyword\">if</span> (flag == <span class=\"literal\">YES</span>) &#123;</div><div class=\"line\">        [<span class=\"keyword\">self</span>.button setTitle: <span class=\"string\">@\"Play\"</span> forState: <span class=\"built_in\">UIControlStateNormal</span>];</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<ol>\n<li>Controlling an AVAudioPlayer object</li>\n</ol>\n<figure class=\"highlight objc\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div></pre></td><td class=\"code\"><pre><div class=\"line\">- (<span class=\"keyword\">IBAction</span>) playOrPause: (<span class=\"keyword\">id</span>) sender &#123;</div><div class=\"line\"> </div><div class=\"line\">    <span class=\"comment\">// if already playing, then pause</span></div><div class=\"line\">    <span class=\"keyword\">if</span> (<span class=\"keyword\">self</span>.player.playing) &#123;</div><div class=\"line\">        [<span class=\"keyword\">self</span>.button setTitle: <span class=\"string\">@\"Play\"</span> forState: <span class=\"built_in\">UIControlStateHighlighted</span>];</div><div class=\"line\">        [<span class=\"keyword\">self</span>.button setTitle: <span class=\"string\">@\"Play\"</span> forState: <span class=\"built_in\">UIControlStateNormal</span>];</div><div class=\"line\">        [<span class=\"keyword\">self</span>.player pause];</div><div class=\"line\"> </div><div class=\"line\">    <span class=\"comment\">// if stopped or paused, start playing</span></div><div class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</div><div class=\"line\">        [<span class=\"keyword\">self</span>.button setTitle: <span class=\"string\">@\"Pause\"</span> forState: <span class=\"built_in\">UIControlStateHighlighted</span>];</div><div class=\"line\">        [<span class=\"keyword\">self</span>.button setTitle: <span class=\"string\">@\"Pause\"</span> forState: <span class=\"built_in\">UIControlStateNormal</span>];</div><div class=\"line\">        [<span class=\"keyword\">self</span>.player play];</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<hr>\n<h3 id=\"Recording-and-Playback-using-Audio-Queue-Services\"><a href=\"#Recording-and-Playback-using-Audio-Queue-Services\" class=\"headerlink\" title=\"Recording and Playback using Audio Queue Services\"></a>Recording and Playback using Audio Queue Services</h3><p>Audio Queue Services是一个更加直观的record和play audio的方式。同时它还有更多个高级功能，可以使用这个服务完成更多的工作，比如对LPCM数据进行压缩等等。它和<code>AVAudioPlayer</code>是iOS中唯二可以播放压缩后音频格式的接口。使用Audio Queue Service播放和录音都是通过回调方法完成的。</p>\n<h4 id=\"Creating-an-Audio-Queue-Object\"><a href=\"#Creating-an-Audio-Queue-Object\" class=\"headerlink\" title=\"Creating an Audio Queue Object\"></a>Creating an Audio Queue Object</h4><p>为了创建Audio Queue对象，它分成两类：</p>\n<ul>\n<li>AudioQueueNewInput用于录音</li>\n<li>AudioQueueNewOutput用于播放</li>\n</ul>\n<p>使用audio queue object播放audio file，需要一些几个步骤：</p>\n<ol>\n<li>创建数据结构用于管理audio queue需要的信息，例如audio format，audio fileID等。</li>\n<li>定义callback函数，用于管理audio queue buffers。callback会使用Audio File Service去读取audio file用来播放</li>\n<li>使用AudioQueueNewOutput用来播放audio file。</li>\n</ol>\n<p>Creating an audio queue object具体的代码如下：</p>\n<figure class=\"highlight objc\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">static</span> <span class=\"keyword\">const</span> <span class=\"keyword\">int</span> kNumberBuffers = <span class=\"number\">3</span>;</div><div class=\"line\"><span class=\"comment\">// Create a data structure to manage information needed by the audio queue</span></div><div class=\"line\"><span class=\"keyword\">struct</span> myAQStruct &#123;</div><div class=\"line\">    AudioFileID                     mAudioFile;</div><div class=\"line\">    <span class=\"built_in\">CAStreamBasicDescription</span>        mDataFormat;</div><div class=\"line\">    AudioQueueRef                   mQueue;</div><div class=\"line\">    AudioQueueBufferRef             mBuffers[kNumberBuffers];</div><div class=\"line\">    SInt64                          mCurrentPacket;</div><div class=\"line\">    <span class=\"built_in\">UInt32</span>                          mNumPacketsToRead;</div><div class=\"line\">    AudioStreamPacketDescription    *mPacketDescs;</div><div class=\"line\">    <span class=\"keyword\">bool</span>                            mDone;</div><div class=\"line\">&#125;;</div><div class=\"line\"><span class=\"comment\">// Define a playback audio queue callback function</span></div><div class=\"line\"><span class=\"keyword\">static</span> <span class=\"keyword\">void</span> AQTestBufferCallback(</div><div class=\"line\">    <span class=\"keyword\">void</span>                   *inUserData,</div><div class=\"line\">    AudioQueueRef          inAQ,</div><div class=\"line\">    AudioQueueBufferRef    inCompleteAQBuffer</div><div class=\"line\">) &#123;</div><div class=\"line\">    myAQStruct *myInfo = (myAQStruct *)inUserData;</div><div class=\"line\">    <span class=\"keyword\">if</span> (myInfo-&gt;mDone) <span class=\"keyword\">return</span>;</div><div class=\"line\">    <span class=\"built_in\">UInt32</span> numBytes;</div><div class=\"line\">    <span class=\"built_in\">UInt32</span> nPackets = myInfo-&gt;mNumPacketsToRead;</div><div class=\"line\"> </div><div class=\"line\">    AudioFileReadPackets (</div><div class=\"line\">        myInfo-&gt;mAudioFile,</div><div class=\"line\">        <span class=\"literal\">false</span>,</div><div class=\"line\">        &amp;numBytes,</div><div class=\"line\">        myInfo-&gt;mPacketDescs,</div><div class=\"line\">        myInfo-&gt;mCurrentPacket,</div><div class=\"line\">        &amp;nPackets,</div><div class=\"line\">        inCompleteAQBuffer-&gt;mAudioData</div><div class=\"line\">    );</div><div class=\"line\">    <span class=\"keyword\">if</span> (nPackets &gt; <span class=\"number\">0</span>) &#123;</div><div class=\"line\">        inCompleteAQBuffer-&gt;mAudioDataByteSize = numBytes;</div><div class=\"line\">        AudioQueueEnqueueBuffer (</div><div class=\"line\">            inAQ,</div><div class=\"line\">            inCompleteAQBuffer,</div><div class=\"line\">            (myInfo-&gt;mPacketDescs ? nPackets : <span class=\"number\">0</span>),</div><div class=\"line\">            myInfo-&gt;mPacketDescs</div><div class=\"line\">        );</div><div class=\"line\">        myInfo-&gt;mCurrentPacket += nPackets;</div><div class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</div><div class=\"line\">        AudioQueueStop (</div><div class=\"line\">            myInfo-&gt;mQueue,</div><div class=\"line\">            <span class=\"literal\">false</span></div><div class=\"line\">        );</div><div class=\"line\">        myInfo-&gt;mDone = <span class=\"literal\">true</span>;</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;</div><div class=\"line\"><span class=\"comment\">// Instantiate an audio queue object</span></div><div class=\"line\">AudioQueueNewOutput (</div><div class=\"line\">    &amp;myInfo.mDataFormat,</div><div class=\"line\">    AQTestBufferCallback,</div><div class=\"line\">    &amp;myInfo,</div><div class=\"line\">    <span class=\"built_in\">CFRunLoopGetCurrent</span>(),</div><div class=\"line\">    kCFRunLoopCommonModes,</div><div class=\"line\">    <span class=\"number\">0</span>,</div><div class=\"line\">    &amp;myInfo.mQueue</div><div class=\"line\">);</div></pre></td></tr></table></figure>\n<h4 id=\"Controlling-Audio-Queue-Playback-Level\"><a href=\"#Controlling-Audio-Queue-Playback-Level\" class=\"headerlink\" title=\"Controlling Audio Queue Playback Level\"></a>Controlling Audio Queue Playback Level</h4><p>Audio Queue对象提供了两个方式控制音频level。第一种是直接使用<code>AudioQueueSetParameter</code>以及<code>kAudioQueueParam_Volume</code>参数，就可以设置，设置完成以后会立即生效。</p>\n<figure class=\"highlight objc\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">Float32 volume = <span class=\"number\">1</span>;</div><div class=\"line\">AudioQueueSetParameter (</div><div class=\"line\">    myAQstruct.audioQueueObject,</div><div class=\"line\">    kAudioQueueParam_Volume,</div><div class=\"line\">    volume</div><div class=\"line\">);</div></pre></td></tr></table></figure>\n<p>也可以通过<code>AudioQueueEnqueueBufferWithParameters</code>给audio queue buffer设置。这种方式只有在audio queue buffer 开始播放时候才起作用。</p>\n<h4 id=\"Indicating-Audio-Queue-Playback-Level\"><a href=\"#Indicating-Audio-Queue-Playback-Level\" class=\"headerlink\" title=\"Indicating Audio Queue Playback Level\"></a>Indicating Audio Queue Playback Level</h4><p>你也可以直接查询audio queue的<code>kAudioQueueProperty_CurrentLevelMeterDB</code>属性，得到的值是一组<code>AudioQueueLevelMeterState</code>结构体（一个channel一个数组），具体的结构体是<code>AudioQueueLevelMeterState</code>，显示如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">typedef struct AudioQueueLevelMeterState &#123;</div><div class=\"line\">    Float32     mAveragePower;</div><div class=\"line\">    Float32     mPeakPower;</div><div class=\"line\">&#125;;  AudioQueueLevelMeterState;</div></pre></td></tr></table></figure>\n<hr>\n<h3 id=\"System-Sounds-Alerts-and-Sound-Effects\"><a href=\"#System-Sounds-Alerts-and-Sound-Effects\" class=\"headerlink\" title=\"System Sounds: Alerts and Sound Effects\"></a>System Sounds: Alerts and Sound Effects</h3><p>如果你需要播放的音频时间少于30s，那么可以使用System Sound Services。调用<code>AudioServicesPlaySystemSound</code>函数可以立即播放一个sound file。你也可以调用<code>AudioServicesPlayAlertSound</code>播放alert声音。这两个方法都会在手机静音的情况下振动。</p>\n<p>当然，你也在调用<code>AudioServicesPlaySystemSound</code>方法使用<code>kSystemSoundID_Vibrate</code>属性，显示的触发振动。</p>\n<p>为了使用<code>AudioServicesPlaySystemSound</code>方法播放sound，首先需要将sound file注册到系统中，得到一个sound ID，然后才能播放。</p>\n<p>下面一段代码显示了使用System Sound Services去play sound：</p>\n<figure class=\"highlight objc\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">#include <span class=\"meta-string\">&lt;AudioToolbox/AudioToolbox.h&gt;</span></span></div><div class=\"line\"><span class=\"meta\">#include <span class=\"meta-string\">&lt;CoreFoundation/CoreFoundation.h&gt;</span></span></div><div class=\"line\"> </div><div class=\"line\"><span class=\"comment\">// Define a callback to be called when the sound is finished</span></div><div class=\"line\"><span class=\"comment\">// playing. Useful when you need to free memory after playing.</span></div><div class=\"line\"><span class=\"keyword\">static</span> <span class=\"keyword\">void</span> MyCompletionCallback (</div><div class=\"line\">    SystemSoundID  mySSID,</div><div class=\"line\">    <span class=\"keyword\">void</span> * myURLRef</div><div class=\"line\">) &#123;</div><div class=\"line\">        AudioServicesDisposeSystemSoundID (mySSID);</div><div class=\"line\">        <span class=\"built_in\">CFRelease</span> (myURLRef);</div><div class=\"line\">        <span class=\"built_in\">CFRunLoopStop</span> (<span class=\"built_in\">CFRunLoopGetCurrent</span>());</div><div class=\"line\">&#125;</div><div class=\"line\"> </div><div class=\"line\"><span class=\"keyword\">int</span> main (<span class=\"keyword\">int</span> argc, <span class=\"keyword\">const</span> <span class=\"keyword\">char</span> * argv[]) &#123;</div><div class=\"line\">    <span class=\"comment\">// Set up the pieces needed to play a sound.</span></div><div class=\"line\">    SystemSoundID    mySSID;</div><div class=\"line\">    <span class=\"built_in\">CFURLRef</span>        myURLRef;</div><div class=\"line\">    myURLRef = <span class=\"built_in\">CFURLCreateWithFileSystemPath</span> (</div><div class=\"line\">        kCFAllocatorDefault,</div><div class=\"line\">        <span class=\"built_in\">CFSTR</span> (<span class=\"string\">\"../../ComedyHorns.aif\"</span>),</div><div class=\"line\">        kCFURLPOSIXPathStyle,</div><div class=\"line\">        <span class=\"literal\">FALSE</span></div><div class=\"line\">    );</div><div class=\"line\"> </div><div class=\"line\">    <span class=\"comment\">// create a system sound ID to represent the sound file</span></div><div class=\"line\">    OSStatus error = AudioServicesCreateSystemSoundID (myURLRef, &amp;mySSID);</div><div class=\"line\"> </div><div class=\"line\">    <span class=\"comment\">// Register the sound completion callback.</span></div><div class=\"line\">    <span class=\"comment\">// Again, useful when you need to free memory after playing.</span></div><div class=\"line\">    AudioServicesAddSystemSoundCompletion (</div><div class=\"line\">        mySSID,</div><div class=\"line\">        <span class=\"literal\">NULL</span>,</div><div class=\"line\">        <span class=\"literal\">NULL</span>,</div><div class=\"line\">        MyCompletionCallback,</div><div class=\"line\">        (<span class=\"keyword\">void</span> *) myURLRef</div><div class=\"line\">    );</div><div class=\"line\"> </div><div class=\"line\">    <span class=\"comment\">// Play the sound file.</span></div><div class=\"line\">    AudioServicesPlaySystemSound (mySSID);</div><div class=\"line\"> </div><div class=\"line\">    <span class=\"comment\">// Invoke a run loop on the current thread to keep the application</span></div><div class=\"line\">    <span class=\"comment\">// running long enough for the sound to play; the sound completion</span></div><div class=\"line\">    <span class=\"comment\">// callback later stops this run loop.</span></div><div class=\"line\">    <span class=\"built_in\">CFRunLoopRun</span> ();</div><div class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>","excerpt":"","more":"<p>这一篇主要是CoreAudio官方文档的重点内容的笔记。</p>\n<h3 id=\"通过回调函数与CoreAudio交互\"><a href=\"#通过回调函数与CoreAudio交互\" class=\"headerlink\" title=\"通过回调函数与CoreAudio交互\"></a>通过回调函数与CoreAudio交互</h3><p>iOS的CoreAudio是通过callback函数与App交互的。其中需要设置回调函数有以下几种情况：</p>\n<ul>\n<li>CoreAudio会向回调函数给App传入PCM音频数据，然后App需要在回调函数中将音频数据写入文件文件系统。（录音时候）</li>\n<li>CoreAudio会需要向App请求一些音频数据，App通过从文件系统中读取音频数据，然后通过callback函数传递给CoreAudio。（播放时候）</li>\n<li>通过注册属性观察者，监听CoreAudio的属性，注册回调函数</li>\n</ul>\n<p>下面是一个使用Audio Queue Services的属性监听器的callback函数的调用模板。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">typedef void (*AudioQueuePropertyListenerProc) (</div><div class=\"line\">                void *                  inUserData,</div><div class=\"line\">                AudioQueueRef           inAQ,</div><div class=\"line\">                AudioQueuePropertyID    inID</div><div class=\"line\">            );</div></pre></td></tr></table></figure>\n<p>在实现和使用这个callback函数时候，你需要完成两件事：</p>\n<ul>\n<li>实现这个函数。例如，你可以实现property listener callback，根据audio是否在running或者stop状态，去改变更新UI。</li>\n<li>注册callback函数时候带上userData数据，在callback函数触发时候使用。</li>\n</ul>\n<p>下面是一个property listener callback函数的实现：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">static void propertyListenerCallback (</div><div class=\"line\">    void                    *inUserData,</div><div class=\"line\">    AudioQueueRef           queueObject,</div><div class=\"line\">    AudioQueuePropertyID    propertyID</div><div class=\"line\">) &#123;</div><div class=\"line\">    AudioPlayer *player = (AudioPlayer *) inUserData;</div><div class=\"line\">        // gets a reference to the playback object</div><div class=\"line\">    [player.notificationDelegate updateUserInterfaceOnAudioQueueStateChange: player];</div><div class=\"line\">        // your notificationDelegate class implements the UI update method</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>下面是注册一个callback函数的例子：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">AudioQueueAddPropertyListener (</div><div class=\"line\">    self.queueObject,                // the object that will invoke your callback</div><div class=\"line\">    kAudioQueueProperty_IsRunning,   // the ID of the property you want to listen for</div><div class=\"line\">    propertyListenerCallback,        // a reference to your callback function</div><div class=\"line\">    self</div><div class=\"line\">);</div></pre></td></tr></table></figure>\n<hr>\n<h3 id=\"Audio-Data-Formats\"><a href=\"#Audio-Data-Formats\" class=\"headerlink\" title=\"Audio Data Formats\"></a>Audio Data Formats</h3><p>这部分内容是iOS中支持的音频格式，理解以后对后面的音频相关的编程能够理解更加深刻。</p>\n<h4 id=\"iOS中通用的音频数据类型\"><a href=\"#iOS中通用的音频数据类型\" class=\"headerlink\" title=\"iOS中通用的音频数据类型\"></a>iOS中通用的音频数据类型</h4><p>在CoreAudio中，使用<code>AudioStreamBasicDescription</code>和<code>AudioStreamPacketDescription</code>这两个类型描述了通用的音频数据类型,包括压缩音频数据，非压缩的音频数据。他们的数据结构如下：</p>\n<figure class=\"highlight objc\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">struct</span> AudioStreamBasicDescription &#123;</div><div class=\"line\">    Float64 mSampleRate;</div><div class=\"line\">    <span class=\"built_in\">UInt32</span>  mFormatID;</div><div class=\"line\">    <span class=\"built_in\">UInt32</span>  mFormatFlags;</div><div class=\"line\">    <span class=\"built_in\">UInt32</span>  mBytesPerPacket;</div><div class=\"line\">    <span class=\"built_in\">UInt32</span>  mFramesPerPacket;</div><div class=\"line\">    <span class=\"built_in\">UInt32</span>  mBytesPerFrame;</div><div class=\"line\">    <span class=\"built_in\">UInt32</span>  mChannelsPerFrame;</div><div class=\"line\">    <span class=\"built_in\">UInt32</span>  mBitsPerChannel;</div><div class=\"line\">    <span class=\"built_in\">UInt32</span>  mReserved;\t\t\t\t<span class=\"comment\">//0</span></div><div class=\"line\">&#125;;</div><div class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"keyword\">struct</span> AudioStreamBasicDescription  AudioStreamBasicDescription;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">struct</span>  AudioStreamPacketDescription &#123;</div><div class=\"line\">    SInt64  mStartOffset;</div><div class=\"line\">    <span class=\"built_in\">UInt32</span>  mVariableFramesInPacket;</div><div class=\"line\">    <span class=\"built_in\">UInt32</span>  mDataByteSize;</div><div class=\"line\">&#125;;</div><div class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"keyword\">struct</span> AudioStreamPacketDescription AudioStreamPacketDescription;</div></pre></td></tr></table></figure>\n<blockquote>\n<p>compressed audio formats use a varying number of bits per sample. For these formats, the value of the mBitsPerChannel member is 0.</p>\n</blockquote>\n<h4 id=\"Audio-Data-Packets\"><a href=\"#Audio-Data-Packets\" class=\"headerlink\" title=\"Audio Data Packets\"></a>Audio Data Packets</h4><p>前面定义过，将一个或者多个frames称为一个packet。或者说packet是最有意义的一组frames，它在audio file中代表一个有意义的时间单元。使用Core Audio中一般是对packets进行处理的。</p>\n<p>每个audio data格式在packets被装配完成以后，它的fromat就被确定了。ASBD数据结构通过<code>mBytesPerPacket</code>和<code>mFramesPerPacket</code>描述音频格式的packet信息，其中页包含其他的信息。</p>\n<p>在整个Core Audio中可能会用到三种不同的packets：</p>\n<ul>\n<li>CBR (constant bit rate) formats：例如 linear PCM and IMA/ADPCM，所有的packet使用相同的大小。</li>\n<li>VBR (variable bit rate) formats：例如 AAC，Apple Lossless，MP3，所有的packets拥有相同的frames，但是每个sample中的bits数目不同。</li>\n<li>VFR (variable frame rate) formats：packets拥有数目不同的的frames。</li>\n</ul>\n<p>在Core Audio中使用VBR或者VFR格式，使用ASPD结构体只能用来描述单个packet。如果是record或者play VBR或者VFR音频文件，需要涉及到多个ASPD结构。</p>\n<p>在AudioFileService等接口中，都是通过pakcets工作的。例如<code>AudioFileReadPackets</code>会得到一系列的packets，同时会得到一个数组的<code>AudioStreamPacketDescription</code>。</p>\n<p>下面是通过packets计算audio data buffer的大小：</p>\n<figure class=\"highlight objc\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div></pre></td><td class=\"code\"><pre><div class=\"line\">- (<span class=\"keyword\">void</span>) calculateSizesFor: (Float64) seconds &#123;</div><div class=\"line\"> </div><div class=\"line\">    <span class=\"built_in\">UInt32</span> maxPacketSize;</div><div class=\"line\">    <span class=\"built_in\">UInt32</span> propertySize = <span class=\"keyword\">sizeof</span> (maxPacketSize);</div><div class=\"line\"> </div><div class=\"line\">    AudioFileGetProperty (</div><div class=\"line\">        audioFileID,</div><div class=\"line\">        kAudioFilePropertyPacketSizeUpperBound,</div><div class=\"line\">        &amp;propertySize,</div><div class=\"line\">        &amp;maxPacketSize</div><div class=\"line\">    );</div><div class=\"line\"> </div><div class=\"line\">    <span class=\"keyword\">static</span> <span class=\"keyword\">const</span> <span class=\"keyword\">int</span> maxBufferSize = <span class=\"number\">0x10000</span>;   <span class=\"comment\">// limit maximum size to 64K</span></div><div class=\"line\">    <span class=\"keyword\">static</span> <span class=\"keyword\">const</span> <span class=\"keyword\">int</span> minBufferSize = <span class=\"number\">0x4000</span>;    <span class=\"comment\">// limit minimum size to 16K</span></div><div class=\"line\"> </div><div class=\"line\">    <span class=\"keyword\">if</span> (audioFormat.mFramesPerPacket) &#123;</div><div class=\"line\">        Float64 numPacketsForTime =</div><div class=\"line\">            audioFormat.mSampleRate / audioFormat.mFramesPerPacket * seconds;</div><div class=\"line\">        [<span class=\"keyword\">self</span> setBufferByteSize: numPacketsForTime * maxPacketSize];</div><div class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</div><div class=\"line\">        <span class=\"comment\">// if frames per packet is zero, then the codec doesn't know the</span></div><div class=\"line\">        <span class=\"comment\">// relationship between packets and time. Return a default buffer size</span></div><div class=\"line\">        [<span class=\"keyword\">self</span> setBufferByteSize:</div><div class=\"line\">            maxBufferSize &gt; maxPacketSize ? maxBufferSize : maxPacketSize];</div><div class=\"line\">    &#125;</div><div class=\"line\"> </div><div class=\"line\">    <span class=\"comment\">// clamp buffer size to our specified range</span></div><div class=\"line\">    <span class=\"keyword\">if</span> (bufferByteSize &gt; maxBufferSize &amp;&amp; bufferByteSize &gt; maxPacketSize) &#123;</div><div class=\"line\">        [<span class=\"keyword\">self</span> setBufferByteSize: maxBufferSize];</div><div class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</div><div class=\"line\">        <span class=\"keyword\">if</span> (bufferByteSize &lt; minBufferSize) &#123;</div><div class=\"line\">            [<span class=\"keyword\">self</span> setBufferByteSize: minBufferSize];</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\"> </div><div class=\"line\">    [<span class=\"keyword\">self</span> setNumPacketsToRead: <span class=\"keyword\">self</span>.bufferByteSize / maxPacketSize];</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h4 id=\"Data-Format-Conversion\"><a href=\"#Data-Format-Conversion\" class=\"headerlink\" title=\"Data Format Conversion\"></a>Data Format Conversion</h4><p>将音频数据从一种audio data转换成另外一种audio data。常见的有三中音频格式转换：</p>\n<ul>\n<li>Decoding an audio format (such as AAC (Advanced Audio Coding)) to linear PCM format.</li>\n<li>Converting linear PCM data into a different audio format.</li>\n<li>Converting between different variants of linear PCM (for example, converting 16-bit signed integer linear PCM to 8.24 fixed-point linear PCM).</li>\n</ul>\n<hr>\n<h3 id=\"Sound-Files\"><a href=\"#Sound-Files\" class=\"headerlink\" title=\"Sound Files\"></a>Sound Files</h3><p>如果要使用声音文件，你需要使用Audio File Services的接口。一般而且，在iOS中需要与音频文件的创建，操作都离不开Audio File Services。</p>\n<p>使用<code>AudioFileGetGlobalInfoSize</code>和<code>AudioFileGetGlobalInfo</code>分别分配info的内存和获取info的内容。你可以获取以下的内容：</p>\n<ul>\n<li>Readable file types</li>\n<li>Writable file types</li>\n<li>For each writable type, the audio data formats you can put into the file</li>\n</ul>\n<h4 id=\"Creating-a-New-Sound-File\"><a href=\"#Creating-a-New-Sound-File\" class=\"headerlink\" title=\"Creating a New Sound File\"></a>Creating a New Sound File</h4><p>为了创建一个能够存储音频数据的audio file，你需要进行以下三步：</p>\n<ul>\n<li>使用CFURL或者NSURL表示的系统文件的路径</li>\n<li>你需要创建的文件的类型的标识identifier，这些identifier定义在<code>Audio File Types</code>枚举中。例如，为了创建一个CAF文件，你需要使用<code>kAudioFileCAFType</code>的identifier。</li>\n<li>创建过程中你需要提供音频数据的ASBD结构体。为了获取ASBD，你可以先提供ASBD结构体的部分成员的值，然后通过函数让<code>Audio File Services</code>将剩余的信息填满。</li>\n</ul>\n<p>下面是创建一个AudioFile的方法：</p>\n<figure class=\"highlight objc\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">AudioFileCreateWithURL (</div><div class=\"line\">    audioFileURL,</div><div class=\"line\">    kAudioFileCAFType,</div><div class=\"line\">    &amp;audioFormat,</div><div class=\"line\">    kAudioFileFlags_EraseFile,</div><div class=\"line\">    &amp;audioFileID   <span class=\"comment\">// the function provides the new file object here</span></div><div class=\"line\">);</div></pre></td></tr></table></figure>\n<h4 id=\"Opening-a-Sound-File\"><a href=\"#Opening-a-Sound-File\" class=\"headerlink\" title=\"Opening a Sound File\"></a>Opening a Sound File</h4><p>为了打开sound file，需要使用<code>AudioFileOpenURL</code>函数，该函数会返回一个唯一ID，供后面使用。</p>\n<p>为了获取sound file的一些属性，通常使用<code>AudioFileGetPropertyInfo</code>和<code>AudioFileGetProperty</code>，日常使用的属性以下：</p>\n<ul>\n<li>kAudioFilePropertyFileFormat</li>\n<li>kAudioFilePropertyDataFormat</li>\n<li>kAudioFilePropertyMagicCookieData</li>\n<li>kAudioFilePropertyChannelLayout</li>\n</ul>\n<h4 id=\"Reading-From-and-Writing-To-a-Sound-File\"><a href=\"#Reading-From-and-Writing-To-a-Sound-File\" class=\"headerlink\" title=\"Reading From and Writing To a Sound File\"></a>Reading From and Writing To a Sound File</h4><p>iOS中，我们经常需要使用<code>Audio File Services</code>去读写audio data到sound file中。读和写是一对相反的内容，操作的对象都可以是bytes或者packets，但是一般而言都是直接使用的packets。</p>\n<blockquote>\n<ul>\n<li>读写VBR数据，只能使用packet</li>\n<li>直接使用packet，更加容易计算时间</li>\n</ul>\n</blockquote>\n<hr>\n<h3 id=\"iPhone-Audio-File-Formats\"><a href=\"#iPhone-Audio-File-Formats\" class=\"headerlink\" title=\"iPhone Audio File Formats\"></a>iPhone Audio File Formats</h3><p>iOS支持的sound file格式如下：</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">Format name</th>\n<th style=\"text-align:center\">Format filename extensions</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">AIFF</td>\n<td style=\"text-align:center\">.aif, .aiff</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">CAF</td>\n<td style=\"text-align:center\">.caf</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">MPEG-1, layer 3</td>\n<td style=\"text-align:center\">.mp3</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">MPEG-2 or MPEG-4 ADTS</td>\n<td style=\"text-align:center\">.aac</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">MPEG-4</td>\n<td style=\"text-align:center\">.m4a, .mp4</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">WAV</td>\n<td style=\"text-align:center\">.wav</td>\n</tr>\n</tbody>\n</table>\n<p>iOS中的native format是CAF file format。</p>\n<hr>\n<h3 id=\"Audio-Sessions-Cooperating-with-Core-Audio\"><a href=\"#Audio-Sessions-Cooperating-with-Core-Audio\" class=\"headerlink\" title=\"Audio Sessions: Cooperating with Core Audio\"></a>Audio Sessions: Cooperating with Core Audio</h3><p>在iOS中，app在运行过程中有可能接到电话，如果此时正在播放sound，系统会做一定的处理。</p>\n<p>AudioSession就是在这种情况的中间人，每个app都会有一个audio session。在播放或者录音时候需要session在做正确的事情，需要我们自己弄清楚以下的情况：</p>\n<ul>\n<li>app收到系统的中断时候应该如何响应，比如收到phone call？</li>\n<li>你是否需要app的sound和其他后台运行的app的sounds混合播放，或者需要独占播放？</li>\n<li>你需要app如何响应远音频路径的响应，比如拔插耳机时候</li>\n</ul>\n<p>AudioSession提供了三种类型的接口：</p>\n<ul>\n<li><strong>Categories</strong>： A category is a key that identifies a set of audio behaviors for your application. By setting a category, you indicate your audio intentions to iOS, such as whether your audio should continue when the screen locks.</li>\n<li><strong>Interruptions and route changes</strong> ：Your audio session posts notifications when your audio is interrupted, when an interruption ends, and when the hardware audio route changes. These notifications let you respond to changes in the larger audio environment—such as an interruption due to in an incoming phone call—gracefully.</li>\n<li><strong>Hardware characteristics</strong>：You can query the audio session to discover characteristics of the device your application is running on, such as hardware sample rate, number of hardware channels, and whether audio input is available.</li>\n</ul>\n<h4 id=\"Audio-Session-Default-Behavior\"><a href=\"#Audio-Session-Default-Behavior\" class=\"headerlink\" title=\"Audio Session Default Behavior\"></a>Audio Session Default Behavior</h4><p>Audio Session拥有一些默认的行为策略：</p>\n<ul>\n<li>当用户将静音开关静音时，audio就会静音。</li>\n<li>当用户锁屏（手动，自动）时候，audio就会静音。</li>\n<li>当你app的audio启动时，其他app正在使用的audio就会静音。</li>\n</ul>\n<p>audio session的这个特定的默认的行为策略被称为<code>kAudioSessionCategory_SoloAmbientSound</code>。同时，它还包括其他的多种策略选择。</p>\n<h4 id=\"Interruptions-Deactivation-and-Activation\"><a href=\"#Interruptions-Deactivation-and-Activation\" class=\"headerlink\" title=\"Interruptions: Deactivation and Activation\"></a>Interruptions: Deactivation and Activation</h4><p>默认的audio session的一个典型的特征是，audio会在中断以后自动恢复活动。Audio session有两个重要的状态：<code>active</code>和<code>inactive</code>。只有当Audio session处于<code>active</code>状态时候，app才能使用audio。</p>\n<p>在app启动以后，你的默认的audio session就会是<code>active</code>状态。然而，如果一个电话被打进来，你的session就会立刻被置为<code>inactive</code>，然后app中的audio就会停止。这个电话就被称为一个中断，如果用户选择忽略电话，app就会继续运行。但是此时你的audio session依然会是<code>inactive</code>状态，audio也就不会工作。</p>\n<p>如果你使用 Audio Queue Services操作audio，我们就需要给中断注册listener回调函数，手动去重启audio session。具体内容可以见<code>Audio Session Programming Guide</code>。</p>\n<h4 id=\"Determining-if-Audio-Input-is-Available\"><a href=\"#Determining-if-Audio-Input-is-Available\" class=\"headerlink\" title=\"Determining if Audio Input is Available\"></a>Determining if Audio Input is Available</h4><p>一个录音的app只有在设备的音频硬件可用的时候才能录音。为了检查这个属性，需要使用audio session的<code>kAudioSessionProperty_AudioInputAvailable</code>属性。</p>\n<figure class=\"highlight objc\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"built_in\">UInt32</span> audioInputIsAvailable;</div><div class=\"line\"><span class=\"built_in\">UInt32</span> propertySize = <span class=\"keyword\">sizeof</span> (audioInputIsAvailable);</div><div class=\"line\"> </div><div class=\"line\">AudioSessionGetProperty (</div><div class=\"line\">    kAudioSessionProperty_AudioInputAvailable,</div><div class=\"line\">    &amp;propertySize,</div><div class=\"line\">    &amp;audioInputIsAvailable <span class=\"comment\">// A nonzero value on output means that</span></div><div class=\"line\">                           <span class=\"comment\">// audio input is available</span></div><div class=\"line\">);</div></pre></td></tr></table></figure>\n<h4 id=\"Using-Your-Audio-Session\"><a href=\"#Using-Your-Audio-Session\" class=\"headerlink\" title=\"Using Your Audio Session\"></a>Using Your Audio Session</h4><p>你的app同时只能有一个audio session策略，你的所有的audio都需要遵循这个<code>active</code>策略的特点。如何响应中断，在Audio Session Programming Guide`中有更加详细的内容。</p>\n<blockquote>\n<p>如果要测试Audio session，需要使用真机</p>\n</blockquote>\n<hr>\n<h3 id=\"Playback-using-the-AVAudioPlayer-Class\"><a href=\"#Playback-using-the-AVAudioPlayer-Class\" class=\"headerlink\" title=\"Playback using the AVAudioPlayer Class\"></a>Playback using the AVAudioPlayer Class</h3><p><code>AVAudioPlayer</code>提供了简单的OC接口用于audio播放。如果非网络stream，或者需要精确控制，apple推荐使用这个类，它可以用于播放iOS支持的任何audio format，同时这个类并不需要去设置audio session，因为它会在中断发生以后自动恢复播放，除非你需要指定特地的行为。</p>\n<p>它可以完成以下工作：</p>\n<ul>\n<li>Play sounds of any duration</li>\n<li>Play sounds from files or memory buffers</li>\n<li>Loop sounds</li>\n<li>Play multiple sounds simultaneously</li>\n<li>Control relative playback level for each sound you are playing</li>\n<li>Seek to a particular point in a sound file, which supports such application features as fast forward and rewind</li>\n<li>Obtain data that you can use for audio level metering</li>\n</ul>\n<p>下面就是使用<code>AVAudioPlayer</code>的具体流程：</p>\n<ol>\n<li>Configuring an AVAudioPlayer object</li>\n</ol>\n<figure class=\"highlight objc\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"built_in\">NSString</span> *soundFilePath =</div><div class=\"line\">                [[<span class=\"built_in\">NSBundle</span> mainBundle] pathForResource: <span class=\"string\">@\"sound\"</span></div><div class=\"line\">                                                ofType: <span class=\"string\">@\"wav\"</span>];</div><div class=\"line\"> </div><div class=\"line\"><span class=\"built_in\">NSURL</span> *fileURL = [[<span class=\"built_in\">NSURL</span> alloc] initFileURLWithPath: soundFilePath];</div><div class=\"line\"> </div><div class=\"line\"><span class=\"built_in\">AVAudioPlayer</span> *newPlayer =</div><div class=\"line\">                [[<span class=\"built_in\">AVAudioPlayer</span> alloc] initWithContentsOfURL: fileURL</div><div class=\"line\">                                                       error: <span class=\"literal\">nil</span>];</div><div class=\"line\">[fileURL release];</div><div class=\"line\"> </div><div class=\"line\"><span class=\"keyword\">self</span>.player = newPlayer;</div><div class=\"line\">[newPlayer release];</div><div class=\"line\"> </div><div class=\"line\">[<span class=\"keyword\">self</span>.player prepareToPlay];</div><div class=\"line\">[<span class=\"keyword\">self</span>.player setDelegate: <span class=\"keyword\">self</span>];</div></pre></td></tr></table></figure>\n<p>你的delegate对象用于处理<code>interruptions</code>或者音频播放停止以后的操作。</p>\n<ol>\n<li>Implementing an AVAudioPlayer delegate method </li>\n</ol>\n<figure class=\"highlight objc\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">- (<span class=\"keyword\">void</span>) audioPlayerDidFinishPlaying: (<span class=\"built_in\">AVAudioPlayer</span> *) player</div><div class=\"line\">                        successfully: (<span class=\"built_in\">BOOL</span>) flag &#123;</div><div class=\"line\">    <span class=\"keyword\">if</span> (flag == <span class=\"literal\">YES</span>) &#123;</div><div class=\"line\">        [<span class=\"keyword\">self</span>.button setTitle: <span class=\"string\">@\"Play\"</span> forState: <span class=\"built_in\">UIControlStateNormal</span>];</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<ol>\n<li>Controlling an AVAudioPlayer object</li>\n</ol>\n<figure class=\"highlight objc\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div></pre></td><td class=\"code\"><pre><div class=\"line\">- (<span class=\"keyword\">IBAction</span>) playOrPause: (<span class=\"keyword\">id</span>) sender &#123;</div><div class=\"line\"> </div><div class=\"line\">    <span class=\"comment\">// if already playing, then pause</span></div><div class=\"line\">    <span class=\"keyword\">if</span> (<span class=\"keyword\">self</span>.player.playing) &#123;</div><div class=\"line\">        [<span class=\"keyword\">self</span>.button setTitle: <span class=\"string\">@\"Play\"</span> forState: <span class=\"built_in\">UIControlStateHighlighted</span>];</div><div class=\"line\">        [<span class=\"keyword\">self</span>.button setTitle: <span class=\"string\">@\"Play\"</span> forState: <span class=\"built_in\">UIControlStateNormal</span>];</div><div class=\"line\">        [<span class=\"keyword\">self</span>.player pause];</div><div class=\"line\"> </div><div class=\"line\">    <span class=\"comment\">// if stopped or paused, start playing</span></div><div class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</div><div class=\"line\">        [<span class=\"keyword\">self</span>.button setTitle: <span class=\"string\">@\"Pause\"</span> forState: <span class=\"built_in\">UIControlStateHighlighted</span>];</div><div class=\"line\">        [<span class=\"keyword\">self</span>.button setTitle: <span class=\"string\">@\"Pause\"</span> forState: <span class=\"built_in\">UIControlStateNormal</span>];</div><div class=\"line\">        [<span class=\"keyword\">self</span>.player play];</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<hr>\n<h3 id=\"Recording-and-Playback-using-Audio-Queue-Services\"><a href=\"#Recording-and-Playback-using-Audio-Queue-Services\" class=\"headerlink\" title=\"Recording and Playback using Audio Queue Services\"></a>Recording and Playback using Audio Queue Services</h3><p>Audio Queue Services是一个更加直观的record和play audio的方式。同时它还有更多个高级功能，可以使用这个服务完成更多的工作，比如对LPCM数据进行压缩等等。它和<code>AVAudioPlayer</code>是iOS中唯二可以播放压缩后音频格式的接口。使用Audio Queue Service播放和录音都是通过回调方法完成的。</p>\n<h4 id=\"Creating-an-Audio-Queue-Object\"><a href=\"#Creating-an-Audio-Queue-Object\" class=\"headerlink\" title=\"Creating an Audio Queue Object\"></a>Creating an Audio Queue Object</h4><p>为了创建Audio Queue对象，它分成两类：</p>\n<ul>\n<li>AudioQueueNewInput用于录音</li>\n<li>AudioQueueNewOutput用于播放</li>\n</ul>\n<p>使用audio queue object播放audio file，需要一些几个步骤：</p>\n<ol>\n<li>创建数据结构用于管理audio queue需要的信息，例如audio format，audio fileID等。</li>\n<li>定义callback函数，用于管理audio queue buffers。callback会使用Audio File Service去读取audio file用来播放</li>\n<li>使用AudioQueueNewOutput用来播放audio file。</li>\n</ol>\n<p>Creating an audio queue object具体的代码如下：</p>\n<figure class=\"highlight objc\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">static</span> <span class=\"keyword\">const</span> <span class=\"keyword\">int</span> kNumberBuffers = <span class=\"number\">3</span>;</div><div class=\"line\"><span class=\"comment\">// Create a data structure to manage information needed by the audio queue</span></div><div class=\"line\"><span class=\"keyword\">struct</span> myAQStruct &#123;</div><div class=\"line\">    AudioFileID                     mAudioFile;</div><div class=\"line\">    <span class=\"built_in\">CAStreamBasicDescription</span>        mDataFormat;</div><div class=\"line\">    AudioQueueRef                   mQueue;</div><div class=\"line\">    AudioQueueBufferRef             mBuffers[kNumberBuffers];</div><div class=\"line\">    SInt64                          mCurrentPacket;</div><div class=\"line\">    <span class=\"built_in\">UInt32</span>                          mNumPacketsToRead;</div><div class=\"line\">    AudioStreamPacketDescription    *mPacketDescs;</div><div class=\"line\">    <span class=\"keyword\">bool</span>                            mDone;</div><div class=\"line\">&#125;;</div><div class=\"line\"><span class=\"comment\">// Define a playback audio queue callback function</span></div><div class=\"line\"><span class=\"keyword\">static</span> <span class=\"keyword\">void</span> AQTestBufferCallback(</div><div class=\"line\">    <span class=\"keyword\">void</span>                   *inUserData,</div><div class=\"line\">    AudioQueueRef          inAQ,</div><div class=\"line\">    AudioQueueBufferRef    inCompleteAQBuffer</div><div class=\"line\">) &#123;</div><div class=\"line\">    myAQStruct *myInfo = (myAQStruct *)inUserData;</div><div class=\"line\">    <span class=\"keyword\">if</span> (myInfo-&gt;mDone) <span class=\"keyword\">return</span>;</div><div class=\"line\">    <span class=\"built_in\">UInt32</span> numBytes;</div><div class=\"line\">    <span class=\"built_in\">UInt32</span> nPackets = myInfo-&gt;mNumPacketsToRead;</div><div class=\"line\"> </div><div class=\"line\">    AudioFileReadPackets (</div><div class=\"line\">        myInfo-&gt;mAudioFile,</div><div class=\"line\">        <span class=\"literal\">false</span>,</div><div class=\"line\">        &amp;numBytes,</div><div class=\"line\">        myInfo-&gt;mPacketDescs,</div><div class=\"line\">        myInfo-&gt;mCurrentPacket,</div><div class=\"line\">        &amp;nPackets,</div><div class=\"line\">        inCompleteAQBuffer-&gt;mAudioData</div><div class=\"line\">    );</div><div class=\"line\">    <span class=\"keyword\">if</span> (nPackets &gt; <span class=\"number\">0</span>) &#123;</div><div class=\"line\">        inCompleteAQBuffer-&gt;mAudioDataByteSize = numBytes;</div><div class=\"line\">        AudioQueueEnqueueBuffer (</div><div class=\"line\">            inAQ,</div><div class=\"line\">            inCompleteAQBuffer,</div><div class=\"line\">            (myInfo-&gt;mPacketDescs ? nPackets : <span class=\"number\">0</span>),</div><div class=\"line\">            myInfo-&gt;mPacketDescs</div><div class=\"line\">        );</div><div class=\"line\">        myInfo-&gt;mCurrentPacket += nPackets;</div><div class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</div><div class=\"line\">        AudioQueueStop (</div><div class=\"line\">            myInfo-&gt;mQueue,</div><div class=\"line\">            <span class=\"literal\">false</span></div><div class=\"line\">        );</div><div class=\"line\">        myInfo-&gt;mDone = <span class=\"literal\">true</span>;</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;</div><div class=\"line\"><span class=\"comment\">// Instantiate an audio queue object</span></div><div class=\"line\">AudioQueueNewOutput (</div><div class=\"line\">    &amp;myInfo.mDataFormat,</div><div class=\"line\">    AQTestBufferCallback,</div><div class=\"line\">    &amp;myInfo,</div><div class=\"line\">    <span class=\"built_in\">CFRunLoopGetCurrent</span>(),</div><div class=\"line\">    kCFRunLoopCommonModes,</div><div class=\"line\">    <span class=\"number\">0</span>,</div><div class=\"line\">    &amp;myInfo.mQueue</div><div class=\"line\">);</div></pre></td></tr></table></figure>\n<h4 id=\"Controlling-Audio-Queue-Playback-Level\"><a href=\"#Controlling-Audio-Queue-Playback-Level\" class=\"headerlink\" title=\"Controlling Audio Queue Playback Level\"></a>Controlling Audio Queue Playback Level</h4><p>Audio Queue对象提供了两个方式控制音频level。第一种是直接使用<code>AudioQueueSetParameter</code>以及<code>kAudioQueueParam_Volume</code>参数，就可以设置，设置完成以后会立即生效。</p>\n<figure class=\"highlight objc\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">Float32 volume = <span class=\"number\">1</span>;</div><div class=\"line\">AudioQueueSetParameter (</div><div class=\"line\">    myAQstruct.audioQueueObject,</div><div class=\"line\">    kAudioQueueParam_Volume,</div><div class=\"line\">    volume</div><div class=\"line\">);</div></pre></td></tr></table></figure>\n<p>也可以通过<code>AudioQueueEnqueueBufferWithParameters</code>给audio queue buffer设置。这种方式只有在audio queue buffer 开始播放时候才起作用。</p>\n<h4 id=\"Indicating-Audio-Queue-Playback-Level\"><a href=\"#Indicating-Audio-Queue-Playback-Level\" class=\"headerlink\" title=\"Indicating Audio Queue Playback Level\"></a>Indicating Audio Queue Playback Level</h4><p>你也可以直接查询audio queue的<code>kAudioQueueProperty_CurrentLevelMeterDB</code>属性，得到的值是一组<code>AudioQueueLevelMeterState</code>结构体（一个channel一个数组），具体的结构体是<code>AudioQueueLevelMeterState</code>，显示如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">typedef struct AudioQueueLevelMeterState &#123;</div><div class=\"line\">    Float32     mAveragePower;</div><div class=\"line\">    Float32     mPeakPower;</div><div class=\"line\">&#125;;  AudioQueueLevelMeterState;</div></pre></td></tr></table></figure>\n<hr>\n<h3 id=\"System-Sounds-Alerts-and-Sound-Effects\"><a href=\"#System-Sounds-Alerts-and-Sound-Effects\" class=\"headerlink\" title=\"System Sounds: Alerts and Sound Effects\"></a>System Sounds: Alerts and Sound Effects</h3><p>如果你需要播放的音频时间少于30s，那么可以使用System Sound Services。调用<code>AudioServicesPlaySystemSound</code>函数可以立即播放一个sound file。你也可以调用<code>AudioServicesPlayAlertSound</code>播放alert声音。这两个方法都会在手机静音的情况下振动。</p>\n<p>当然，你也在调用<code>AudioServicesPlaySystemSound</code>方法使用<code>kSystemSoundID_Vibrate</code>属性，显示的触发振动。</p>\n<p>为了使用<code>AudioServicesPlaySystemSound</code>方法播放sound，首先需要将sound file注册到系统中，得到一个sound ID，然后才能播放。</p>\n<p>下面一段代码显示了使用System Sound Services去play sound：</p>\n<figure class=\"highlight objc\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">#include <span class=\"meta-string\">&lt;AudioToolbox/AudioToolbox.h&gt;</span></span></div><div class=\"line\"><span class=\"meta\">#include <span class=\"meta-string\">&lt;CoreFoundation/CoreFoundation.h&gt;</span></span></div><div class=\"line\"> </div><div class=\"line\"><span class=\"comment\">// Define a callback to be called when the sound is finished</span></div><div class=\"line\"><span class=\"comment\">// playing. Useful when you need to free memory after playing.</span></div><div class=\"line\"><span class=\"keyword\">static</span> <span class=\"keyword\">void</span> MyCompletionCallback (</div><div class=\"line\">    SystemSoundID  mySSID,</div><div class=\"line\">    <span class=\"keyword\">void</span> * myURLRef</div><div class=\"line\">) &#123;</div><div class=\"line\">        AudioServicesDisposeSystemSoundID (mySSID);</div><div class=\"line\">        <span class=\"built_in\">CFRelease</span> (myURLRef);</div><div class=\"line\">        <span class=\"built_in\">CFRunLoopStop</span> (<span class=\"built_in\">CFRunLoopGetCurrent</span>());</div><div class=\"line\">&#125;</div><div class=\"line\"> </div><div class=\"line\"><span class=\"keyword\">int</span> main (<span class=\"keyword\">int</span> argc, <span class=\"keyword\">const</span> <span class=\"keyword\">char</span> * argv[]) &#123;</div><div class=\"line\">    <span class=\"comment\">// Set up the pieces needed to play a sound.</span></div><div class=\"line\">    SystemSoundID    mySSID;</div><div class=\"line\">    <span class=\"built_in\">CFURLRef</span>        myURLRef;</div><div class=\"line\">    myURLRef = <span class=\"built_in\">CFURLCreateWithFileSystemPath</span> (</div><div class=\"line\">        kCFAllocatorDefault,</div><div class=\"line\">        <span class=\"built_in\">CFSTR</span> (<span class=\"string\">\"../../ComedyHorns.aif\"</span>),</div><div class=\"line\">        kCFURLPOSIXPathStyle,</div><div class=\"line\">        <span class=\"literal\">FALSE</span></div><div class=\"line\">    );</div><div class=\"line\"> </div><div class=\"line\">    <span class=\"comment\">// create a system sound ID to represent the sound file</span></div><div class=\"line\">    OSStatus error = AudioServicesCreateSystemSoundID (myURLRef, &amp;mySSID);</div><div class=\"line\"> </div><div class=\"line\">    <span class=\"comment\">// Register the sound completion callback.</span></div><div class=\"line\">    <span class=\"comment\">// Again, useful when you need to free memory after playing.</span></div><div class=\"line\">    AudioServicesAddSystemSoundCompletion (</div><div class=\"line\">        mySSID,</div><div class=\"line\">        <span class=\"literal\">NULL</span>,</div><div class=\"line\">        <span class=\"literal\">NULL</span>,</div><div class=\"line\">        MyCompletionCallback,</div><div class=\"line\">        (<span class=\"keyword\">void</span> *) myURLRef</div><div class=\"line\">    );</div><div class=\"line\"> </div><div class=\"line\">    <span class=\"comment\">// Play the sound file.</span></div><div class=\"line\">    AudioServicesPlaySystemSound (mySSID);</div><div class=\"line\"> </div><div class=\"line\">    <span class=\"comment\">// Invoke a run loop on the current thread to keep the application</span></div><div class=\"line\">    <span class=\"comment\">// running long enough for the sound to play; the sound completion</span></div><div class=\"line\">    <span class=\"comment\">// callback later stops this run loop.</span></div><div class=\"line\">    <span class=\"built_in\">CFRunLoopRun</span> ();</div><div class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>"},{"title":"popToViewController的坑","_content":"\n在使用popViewController时候遇到了两个比较隐蔽的问题.因此,在以后的开发中需要自己注意.\n\n\n### tips1\n在调用popViewController时,使用GCD丢到main queue中去执行:\n\n``` objc\ndispatch_after(dispatch_time(DISPATCH_TIME_NOW, (int64_t)(0.1 * NSEC_PER_SEC)), dispatch_get_main_queue(), ^{\n\t...\n   [self.navigationController popViewControllerAnimated:NO];\n\t...\n});\n```\n\n### tips2\n\n在代码中可能连续多次调用`popViewControllerAnimated`地方,最好通过遍历`navigationController.viewControllers`找到具体要遍历到哪个再直接pop到目标`controller`.\n\n``` objc\ndispatch_after(dispatch_time(DISPATCH_TIME_NOW, (int64_t)(0.1 * NSEC_PER_SEC)), dispatch_get_main_queue(), ^{\n    BOOL findIt = NO;\n    UIViewController *targetVC = nil;\n    for (UIViewController *subVC in self.navigationController.viewControllers) {\n        if (findIt) {\n            break;\n        }\n        if (subVC == xxx) {\n            findIt = YES;\n        }else{\n            targetVC = subVC;\n        }\n    }\n\t[self.navigationController popToViewController:targetVC animated:NO];\n\n});\n\n```","source":"_posts/popToViewController的坑.md","raw":"---\ntitle: popToViewController的坑\ntags:\n- iOS\n---\n\n在使用popViewController时候遇到了两个比较隐蔽的问题.因此,在以后的开发中需要自己注意.\n\n\n### tips1\n在调用popViewController时,使用GCD丢到main queue中去执行:\n\n``` objc\ndispatch_after(dispatch_time(DISPATCH_TIME_NOW, (int64_t)(0.1 * NSEC_PER_SEC)), dispatch_get_main_queue(), ^{\n\t...\n   [self.navigationController popViewControllerAnimated:NO];\n\t...\n});\n```\n\n### tips2\n\n在代码中可能连续多次调用`popViewControllerAnimated`地方,最好通过遍历`navigationController.viewControllers`找到具体要遍历到哪个再直接pop到目标`controller`.\n\n``` objc\ndispatch_after(dispatch_time(DISPATCH_TIME_NOW, (int64_t)(0.1 * NSEC_PER_SEC)), dispatch_get_main_queue(), ^{\n    BOOL findIt = NO;\n    UIViewController *targetVC = nil;\n    for (UIViewController *subVC in self.navigationController.viewControllers) {\n        if (findIt) {\n            break;\n        }\n        if (subVC == xxx) {\n            findIt = YES;\n        }else{\n            targetVC = subVC;\n        }\n    }\n\t[self.navigationController popToViewController:targetVC animated:NO];\n\n});\n\n```","slug":"popToViewController的坑","published":1,"date":"2016-08-26T16:19:12.000Z","updated":"2016-08-26T16:19:12.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cisbyxx8e000e6squxgyuu1bt","content":"<p>在使用popViewController时候遇到了两个比较隐蔽的问题.因此,在以后的开发中需要自己注意.</p>\n<h3 id=\"tips1\"><a href=\"#tips1\" class=\"headerlink\" title=\"tips1\"></a>tips1</h3><p>在调用popViewController时,使用GCD丢到main queue中去执行:</p>\n<figure class=\"highlight objc\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">dispatch_after(dispatch_time(DISPATCH_TIME_NOW, (int64_t)(<span class=\"number\">0.1</span> * <span class=\"built_in\">NSEC_PER_SEC</span>)), dispatch_get_main_queue(), ^&#123;</div><div class=\"line\">\t...</div><div class=\"line\">   [<span class=\"keyword\">self</span>.navigationController popViewControllerAnimated:<span class=\"literal\">NO</span>];</div><div class=\"line\">\t...</div><div class=\"line\">&#125;);</div></pre></td></tr></table></figure>\n<h3 id=\"tips2\"><a href=\"#tips2\" class=\"headerlink\" title=\"tips2\"></a>tips2</h3><p>在代码中可能连续多次调用<code>popViewControllerAnimated</code>地方,最好通过遍历<code>navigationController.viewControllers</code>找到具体要遍历到哪个再直接pop到目标<code>controller</code>.</p>\n<figure class=\"highlight objc\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\">dispatch_after(dispatch_time(DISPATCH_TIME_NOW, (int64_t)(<span class=\"number\">0.1</span> * <span class=\"built_in\">NSEC_PER_SEC</span>)), dispatch_get_main_queue(), ^&#123;</div><div class=\"line\">    <span class=\"built_in\">BOOL</span> findIt = <span class=\"literal\">NO</span>;</div><div class=\"line\">    <span class=\"built_in\">UIViewController</span> *targetVC = <span class=\"literal\">nil</span>;</div><div class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"built_in\">UIViewController</span> *subVC <span class=\"keyword\">in</span> <span class=\"keyword\">self</span>.navigationController.viewControllers) &#123;</div><div class=\"line\">        <span class=\"keyword\">if</span> (findIt) &#123;</div><div class=\"line\">            <span class=\"keyword\">break</span>;</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"keyword\">if</span> (subVC == xxx) &#123;</div><div class=\"line\">            findIt = <span class=\"literal\">YES</span>;</div><div class=\"line\">        &#125;<span class=\"keyword\">else</span>&#123;</div><div class=\"line\">            targetVC = subVC;</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">\t[<span class=\"keyword\">self</span>.navigationController popToViewController:targetVC animated:<span class=\"literal\">NO</span>];</div><div class=\"line\"></div><div class=\"line\">&#125;);</div></pre></td></tr></table></figure>","excerpt":"","more":"<p>在使用popViewController时候遇到了两个比较隐蔽的问题.因此,在以后的开发中需要自己注意.</p>\n<h3 id=\"tips1\"><a href=\"#tips1\" class=\"headerlink\" title=\"tips1\"></a>tips1</h3><p>在调用popViewController时,使用GCD丢到main queue中去执行:</p>\n<figure class=\"highlight objc\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">dispatch_after(dispatch_time(DISPATCH_TIME_NOW, (int64_t)(<span class=\"number\">0.1</span> * <span class=\"built_in\">NSEC_PER_SEC</span>)), dispatch_get_main_queue(), ^&#123;</div><div class=\"line\">\t...</div><div class=\"line\">   [<span class=\"keyword\">self</span>.navigationController popViewControllerAnimated:<span class=\"literal\">NO</span>];</div><div class=\"line\">\t...</div><div class=\"line\">&#125;);</div></pre></td></tr></table></figure>\n<h3 id=\"tips2\"><a href=\"#tips2\" class=\"headerlink\" title=\"tips2\"></a>tips2</h3><p>在代码中可能连续多次调用<code>popViewControllerAnimated</code>地方,最好通过遍历<code>navigationController.viewControllers</code>找到具体要遍历到哪个再直接pop到目标<code>controller</code>.</p>\n<figure class=\"highlight objc\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\">dispatch_after(dispatch_time(DISPATCH_TIME_NOW, (int64_t)(<span class=\"number\">0.1</span> * <span class=\"built_in\">NSEC_PER_SEC</span>)), dispatch_get_main_queue(), ^&#123;</div><div class=\"line\">    <span class=\"built_in\">BOOL</span> findIt = <span class=\"literal\">NO</span>;</div><div class=\"line\">    <span class=\"built_in\">UIViewController</span> *targetVC = <span class=\"literal\">nil</span>;</div><div class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"built_in\">UIViewController</span> *subVC <span class=\"keyword\">in</span> <span class=\"keyword\">self</span>.navigationController.viewControllers) &#123;</div><div class=\"line\">        <span class=\"keyword\">if</span> (findIt) &#123;</div><div class=\"line\">            <span class=\"keyword\">break</span>;</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"keyword\">if</span> (subVC == xxx) &#123;</div><div class=\"line\">            findIt = <span class=\"literal\">YES</span>;</div><div class=\"line\">        &#125;<span class=\"keyword\">else</span>&#123;</div><div class=\"line\">            targetVC = subVC;</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">\t[<span class=\"keyword\">self</span>.navigationController popToViewController:targetVC animated:<span class=\"literal\">NO</span>];</div><div class=\"line\"></div><div class=\"line\">&#125;);</div></pre></td></tr></table></figure>"}],"PostAsset":[],"PostCategory":[],"PostTag":[{"post_id":"cisbyxx7p00006squppn3h5k6","tag_id":"cisbyxx7z00046squ5zz3d6bf","_id":"cisbyxx8i000g6squleqjmw0o"},{"post_id":"cisbyxx7p00006squppn3h5k6","tag_id":"cisbyxx8700086squ3q1t3ck6","_id":"cisbyxx8i000h6squuzgja2i1"},{"post_id":"cisbyxx7p00006squppn3h5k6","tag_id":"cisbyxx8c000c6squlv2kd039","_id":"cisbyxx8j000j6squwy0v9kow"},{"post_id":"cisbyxx8e000e6squxgyuu1bt","tag_id":"cisbyxx7z00046squ5zz3d6bf","_id":"cisbyxx8j000k6squ47g782qc"},{"post_id":"cisbyxx8500076squa773be04","tag_id":"cisbyxx8f000f6squ6iopq9u2","_id":"cisbyxx8k000m6squsnjl7juc"},{"post_id":"cisbyxx8500076squa773be04","tag_id":"cisbyxx7z00046squ5zz3d6bf","_id":"cisbyxx8k000n6squcimn6ds5"},{"post_id":"cisbyxx8800096squjp73s8x3","tag_id":"cisbyxx7z00046squ5zz3d6bf","_id":"cisbyxx8n000p6squbwji16ao"},{"post_id":"cisbyxx8800096squjp73s8x3","tag_id":"cisbyxx8i000i6squxjxxnmhx","_id":"cisbyxx8n000q6squ7n55p9yt"},{"post_id":"cisbyxx8a000a6squwqded6b9","tag_id":"cisbyxx7z00046squ5zz3d6bf","_id":"cisbyxx8p000s6squycj8z5cg"},{"post_id":"cisbyxx8a000a6squwqded6b9","tag_id":"cisbyxx8i000i6squxjxxnmhx","_id":"cisbyxx8p000t6squxu7crjnu"},{"post_id":"cisbyxx8a000a6squwqded6b9","tag_id":"cisbyxx8m000o6squ6mb907ge","_id":"cisbyxx8p000v6squsi7y92th"},{"post_id":"cisbyxx8b000b6squ4cr4uss4","tag_id":"cisbyxx7z00046squ5zz3d6bf","_id":"cisbyxx8p000w6squ0ioz5mvv"},{"post_id":"cisbyxx8b000b6squ4cr4uss4","tag_id":"cisbyxx8i000i6squxjxxnmhx","_id":"cisbyxx8q000x6squ1796kx8y"},{"post_id":"cisbyxx8d000d6squgzz9e5gg","tag_id":"cisbyxx7z00046squ5zz3d6bf","_id":"cisbyxx8q000y6squfqqnemk5"},{"post_id":"cisbyxx8d000d6squgzz9e5gg","tag_id":"cisbyxx8i000i6squxjxxnmhx","_id":"cisbyxx8q000z6squbfdde2sd"}],"Tag":[{"name":"iOS","_id":"cisbyxx7z00046squ5zz3d6bf"},{"name":"PhotoKit","_id":"cisbyxx8700086squ3q1t3ck6"},{"name":"AssetsLibrary","_id":"cisbyxx8c000c6squlv2kd039"},{"name":"runloop","_id":"cisbyxx8f000f6squ6iopq9u2"},{"name":"CoreAudio","_id":"cisbyxx8i000i6squxjxxnmhx"},{"name":"AudioQueue","_id":"cisbyxx8m000o6squ6mb907ge"}]}}